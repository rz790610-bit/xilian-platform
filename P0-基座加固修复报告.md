# P0 基座加固修复报告

**项目**：西联智能平台 (xilian-platform)  
**提交**：`61349ff` — `feat(P0): 基座加固`  
**日期**：2026-02-19  
**修改规模**：10 个文件，+785 / -122 行

---

## 一、修复总览

本次 P0 基座加固针对代码审计中发现的 **5 类关键问题** 进行了系统性修复，覆盖并发安全、数据库性能、安全漏洞和访问控制四个维度。

| 序号 | 修复项 | 风险等级 | 影响文件 | 修改行数 |
|------|--------|----------|----------|----------|
| 1 | 认知调度器重写 | **严重** | `cognition-scheduler.ts`, `types/index.ts` | +468/-50 |
| 2 | EventBus 异步批写 | **严重** | `kafkaEventBus.ts` | +193/-30 |
| 3 | SQL 注入修复 | **严重** | `pipeline.engine.ts`, `workbench.service.ts` | +87/-10 |
| 4 | 数据库连接池 | **高** | `db/index.ts` | +71/-12 |
| 5 | 路由鉴权加固 | **高** | `docker.router.ts`, `database.router.ts`, `workbench.router.ts`, `pipeline.router.ts` | +38/-20 |

---

## 二、详细修复说明

### 2.1 认知调度器重写

**文件**：`server/platform/cognition/scheduler/cognition-scheduler.ts`

**原始问题**：调度器使用 `Set.size` 作为并发计数器，在高并发场景下存在竞态条件；无队列容量上限，内存可能无限增长；去重 Set 永不清理导致内存泄漏；无降级机制和重试退避。

**修复方案**：

完全重写调度器，引入以下机制：

- **并发安全信号量**：使用 `Promise` 链实现的信号量替代 `Set.size` 计数，确保 `acquire()` / `release()` 原子性。信号量初始容量通过环境变量 `COGNITION_MAX_CONCURRENCY` 配置，默认值为 5。

- **队列容量上限**：队列最大容量 500（可通过 `COGNITION_QUEUE_CAPACITY` 配置），超限时直接拒绝并返回错误，防止 OOM。

- **去重自动清理**：任务完成后（无论成功或失败）立即从去重 Set 中移除对应 `taskKey`，避免内存泄漏。

- **降级动态并发**：每 60 秒检查错误率，当错误率超过 30% 时自动将并发数减半（最低为 1）；错误率恢复到 10% 以下时逐步恢复原始并发数。

- **指数退避重试**：重试间隔从 500ms 开始，按 `baseDelay * 2^attempt` 递增，上限 30 秒。最大重试次数通过 `COGNITION_MAX_RETRIES` 配置，默认 3 次。

### 2.2 EventBus 异步批量写入

**文件**：`server/lib/clients/kafkaEventBus.ts`

**原始问题**：`publish()` 方法在每次事件发布时同步执行 `database.insert()`，在 2000 事件/秒的场景下会直接打爆数据库连接。`publishBatch()` 虽然分批 100 条插入，但仍然是同步阻塞调用方。

**修复方案**：

将 DB 写入从同步路径移到异步缓冲路径：

- **内存缓冲区**：`publish()` 和 `publishBatch()` 不再直接写 DB，而是将记录放入内存缓冲区 `dbBuffer[]`。

- **批量刷写**：当缓冲区达到 100 条（`DB_BUFFER_FLUSH_SIZE`）时立即触发刷写；同时每 500ms（`DB_BUFFER_FLUSH_INTERVAL_MS`）定时刷写一次，确保低流量时数据也能及时持久化。

- **并发保护**：通过 `dbFlushing` 标志防止多个刷写操作同时执行。

- **失败恢复**：刷写失败时，该批次记录放回缓冲区头部，下次重试。单批最多重试 3 次（`DB_BUFFER_RETRY_LIMIT`），超过后丢弃该批次并记录错误日志。

- **背压保护**：缓冲区超过 5000 条（`DB_BUFFER_MAX_SIZE`）时，丢弃 `severity=info` 的事件，保留 `warning/error/critical` 级别事件。

- **优雅关闭**：`shutdown()` 方法先停止定时器，然后尝试最多 3 次刷写剩余缓冲区，最后再关闭 Kafka 连接。

- **监控接口**：新增 `getBufferStatus()` 方法，返回缓冲区大小、丢弃计数和刷写状态，供健康检查使用。

### 2.3 SQL 注入修复

**文件**：`server/services/pipeline.engine.ts`, `server/services/database/workbench.service.ts`

**原始问题**：

Pipeline 引擎的 `execMySQL()` 方法（第 182 行）直接将用户配置的 `config.query` 传入 `sql.raw()`，攻击者可通过 Pipeline 配置执行任意 SQL（包括 `DROP TABLE`、`DELETE` 等破坏性操作）。

`execMySQLSink()` 方法（第 915 行）直接拼接表名、列名和值到 SQL 字符串中，虽然对单引号做了转义，但表名和列名未做任何验证，存在标识符注入风险。

SQL 工作台的 `execute()` 方法仅检查了 `DROP DATABASE` 和 `DROP SCHEMA` 两种模式，遗漏了 `GRANT`、`LOAD DATA`、`INTO OUTFILE` 等危险操作。

**修复方案**：

**Pipeline 引擎**：
- `execMySQL()`：添加 `validateReadOnlyQuery()` 验证，仅允许 `SELECT` 开头的查询；使用正则黑名单拦截 12 种 DDL/DML 关键字；禁止分号（多语句执行）。
- `execMySQLSink()`：添加 `validateIdentifier()` 验证表名和列名，仅允许 `[a-zA-Z_][a-zA-Z0-9_]{0,63}` 模式；使用反引号转义标识符；分批插入（每批 100 条）。

**SQL 工作台**：
- 新增统一的 `validateWorkbenchQuery()` 函数，包含 10 种危险 SQL 模式的正则黑名单（`DROP DATABASE/SCHEMA`、`GRANT`、`REVOKE`、`CREATE/ALTER/DROP USER`、`LOAD DATA`、`INTO OUTFILE/DUMPFILE`）。
- 禁止多语句执行（允许末尾分号，但禁止中间分号）。

### 2.4 数据库连接池配置

**文件**：`server/lib/db/index.ts`

**原始问题**：使用 `drizzle(process.env.DATABASE_URL)` 创建数据库实例，Drizzle 默认使用 `mysql2.createConnection()`（单连接），在 100 设备 / 2000 测点的并发场景下，所有请求共享一个连接，导致严重的连接争用和超时。

**修复方案**：

引入 `mysql2/promise` 的 `createPool()` 替代默认单连接：

| 参数 | 值 | 说明 |
|------|-----|------|
| `connectionLimit` | 50 | 最大连接数（MySQL 默认 `max_connections=151`，留余量给其他服务） |
| `waitForConnections` | true | 连接池满时等待而非报错 |
| `queueLimit` | 200 | 等待队列上限，超过则报错（背压保护） |
| `idleTimeout` | 30000ms | 空闲连接 30s 后释放 |
| `maxIdle` | 10 | 最小保持 10 个空闲连接（避免冷启动延迟） |
| `enableKeepAlive` | true | TCP keepalive 防止连接被中间件断开 |
| `keepAliveInitialDelay` | 30000ms | keepalive 初始延迟 |

所有参数均支持通过环境变量覆盖（`DB_POOL_MAX`、`DB_POOL_QUEUE_LIMIT`、`DB_POOL_IDLE_TIMEOUT`、`DB_POOL_MIN_IDLE`）。

新增 `getPoolStatus()` 监控接口，返回当前连接池的总连接数、空闲连接数和等待队列长度。

`resetDb()` 方法改为异步，先调用 `pool.end()` 关闭现有连接池，再清除缓存实例。

### 2.5 路由鉴权加固

**文件**：`docker.router.ts`, `database.router.ts`, `workbench.router.ts`, `pipeline.router.ts`

**原始问题**：代码审计发现全平台 645 个 `publicProcedure`（无需认证）vs 191 个 `protectedProcedure`（需认证），其中 Docker 管理、数据库操作、SQL 工作台、Pipeline 引擎等高危路由全部无需认证即可访问。

**修复方案**：

| 路由文件 | 路由数量 | 原始权限 | 修复后权限 | 说明 |
|----------|----------|----------|------------|------|
| `docker.router.ts` | 12 | `publicProcedure` | `adminProcedure` | 容器管理需管理员权限 |
| `database.router.ts` | 235 | `publicProcedure` | `protectedProcedure` | 数据库操作需登录认证 |
| `workbench.router.ts` | 31 | `publicProcedure` | `protectedProcedure` | SQL 工作台需登录认证 |
| `pipeline.router.ts` | 12 | `publicProcedure` | `protectedProcedure` | Pipeline 管理需登录认证 |

Docker 路由使用 `adminProcedure`（需管理员角色），其余高危路由使用 `protectedProcedure`（需登录认证 + 审计日志）。采用兼容别名方式（`const publicProcedure = protectedProcedure`）最小化代码改动量。

---

## 三、环境变量新增

本次修复新增以下可选环境变量，均有合理默认值：

| 环境变量 | 默认值 | 说明 |
|----------|--------|------|
| `COGNITION_MAX_CONCURRENCY` | 5 | 认知调度器最大并发数 |
| `COGNITION_QUEUE_CAPACITY` | 500 | 认知调度器队列容量上限 |
| `COGNITION_MAX_RETRIES` | 3 | 认知调度器最大重试次数 |
| `DB_POOL_MAX` | 50 | 数据库连接池最大连接数 |
| `DB_POOL_QUEUE_LIMIT` | 200 | 数据库连接池等待队列上限 |
| `DB_POOL_IDLE_TIMEOUT` | 30000 | 数据库空闲连接超时（ms） |
| `DB_POOL_MIN_IDLE` | 10 | 数据库最小空闲连接数 |

---

## 四、后续建议

1. **前端适配**：Docker 管理页面需要在请求头中携带认证 token，否则会收到 401 错误。建议检查前端 tRPC 客户端是否已配置认证拦截器。

2. **负载测试**：建议在 100 设备 / 2000 测点场景下进行压力测试，验证连接池参数和 EventBus 缓冲区大小是否合适。

3. **监控告警**：建议将 `getPoolStatus()` 和 `kafkaEventBus.getBufferStatus()` 接入 Prometheus/Grafana 监控面板，设置以下告警规则：
   - 连接池等待队列 > 100：告警
   - EventBus 缓冲区 > 3000：告警
   - EventBus 丢弃事件数持续增长：告警

4. **剩余路由审计**：本次修复覆盖了 4 个最高危的路由文件（290 个路由），但全平台仍有约 355 个 `publicProcedure` 分布在其他路由文件中，建议后续逐步审计和加固。
