
================================================================================
## ok核心基础设施层代码审查报告.docx
段落数: 165, 表格行数: 41
================================================================================
  P4: 综合评分
  P6: 基础设施层存在多处安全漏洞与架构不一致，需要系统性重构
  P10: 该层代码呈现出明显的双轨并存格局：config.ts / errors.ts / logger.ts / registry.ts 四个文件属于精心设计的高质量基础设施组件；而 env.ts、auth.service.ts、context.ts 三个文件则存在严重的设计缺陷或安全风险，且与前四个文件形成体系割裂。整体看，层内自身的一致性比单文件质量更成问题。
  P11: 二、安全漏洞专项 — P0 必须修复
  P23: 2.2 context.ts — SKIP_AUTH 允许绕过产线认证
  P41: 综合评分  █████████░  9/10
  P46: ⚠ 警告  JWT_SECRET 默认值 "change-me-in-production" 在 validateConfig() 中仅作 error 处理（返回数组），不会阻断进程启动。若调用方未检查 valid 字段，弱密钥仍可被使用。建议在 production 环境下弱密钥直接 process.exit(1)。
  P49: 综合评分  █████████░  9/10
  P57: 综合评分  ████████░░  8/10
  P62: ✗ 缺陷  Logger 实例的 this.level 在构造时由 Logger.globalLevel 固化为数值，之后 Logger.setGlobalLevel() 修改静态变量，但已有实例的 this.level 不会跟着变化，导致动态级别调整无效。
  P71: 综合评分  ██████░░░░  6/10
  P75: ✗ 缺陷  assertApiKey() 的错误提示为 "OPENAI_API_KEY is not configured"，但实际读取的是 ENV.forgeApiKey（对应环境变量 BUILT_IN_FORGE_API_KEY）。错误提示误导开发者排查错误的环境变量。
  P80: ✗ 缺陷  payload.thinking = { "budget_tokens": 128 } 和 payload.max_tokens = 32768 均硬编码，调用方无法覆写。thinking 字段属于模型特定参数，在非 Gemini 模型上会被忽略或报错，但当前抽象层不应泄露特定模型的私有参数。
  P84: 综合评分  ████████░░  8/10
  P89: ✗ 缺陷  emit() 中错误日志调用为 log.error(`[${this.name}] ...`, err)，但 Logger.error() 签名为 error(data: Record<string,unknown> | string, message?: string)，第二参数应为字符串 message，直接传 err 对象会导致 [object Object] 输出。
  P96: 综合评分  █████░░░░░  5/10
  P102: 综合评分  ███░░░░░░░  3/10
  P103: ✗ 缺陷  与 config.ts 功能严重重叠：ENV.databaseUrl 对应 config.mysql.url，ENV.cookieSecret 对应 config.security.jwtSecret，形成双重配置来源，维护时易遗漏同步。
  P104: 🔒 安全  ENV.cookieSecret 默认值为 "local-dev-secret-key-12345678"，不同于 config.ts 的 "change-me-in-production"，两套密钥系统任一被遗漏更新均构成安全风险。
  P105: ✗ 缺陷  ENV.forgeApiUrl 和 ENV.forgeApiKey 仅在 llm.ts 中使用，但这两个配置项未出现在 config.ts 的 externalApis 下，形成配置孤岛。
  P108: 综合评分  ██████░░░░  6/10
  P110: ✓ 亮点  notifyOwner 使用 adminProcedure，权限控制在路由层即可阻断，不需要业务层额外判断。
  P114: 综合评分  █░░░░░░░░░  1/10
  P116: ✗ 缺陷  validateToken 返回类型未明确：{ valid: boolean } | { valid: boolean; userId: string }。调用方对 userId 的存在无法在 TypeScript 中静态保证。
  P127: 综合评分  ███████░░░  7/10
  P131: ✗ 缺陷  loadFromRedis() 和 getTopology() 均使用 client.keys("svc:*")，在 Redis Key 数量较多时会阻塞 Redis 主线程（O(N)）。生产环境应改用 SCAN 迭代。
  P143: context.ts 直接调用 sdk.authenticateRequest()，绕过 auth.service.ts
  P151: 5.1 P0 — 安全漏洞（生产部署前必须修复）
  P152: 5.2 P1 — 功能 Bug（近期迭代修复）
  P153: 5.3 P2 — 架构改进（规划迭代处理）
  P156: 该层代码的最大问题不是单个文件的质量，而是层内一致性：config.ts 和 errors.ts 代表了高质量的工业级设计，而 auth.service.ts 和 env.ts 则处于未完成或遗留状态，与其他文件形成明显落差。安全漏洞集中在认证链（context + env + auth），三处相互呼应，需要系统性修复而非打补丁。
  P159: 本周：修复 S0-1/2/3（认证安全漏洞），这是唯一阻塞生产部署的风险。
  P160: 下周：修复 P1-1 到 P1-5（功能 Bug），确保日志级别调整和 LLM 错误提示正确。
  --- TABLE ISSUES ---
  T: 文件 | 类型 | 评分 | 核心问题
  T: env.ts | 环境变量 | 3/10 | 与 config.ts 严重重叠；cookieSecret 有弱默认值；职责不清
  T: 编号 | 文件 | 问题 | 修复方向
  T: S0-1 | auth.service.ts | 整文件为桩代码，validateToken 恒 valid，getUserPermissions 恒 admin | 实现真实 JWT 验签，或至少 throw NotImplementedError
  T: S0-2 | context.ts | SKIP_AUTH=true 无生产环境保护，LOCAL_DEV_USER.role=admin | 增加 NODE_ENV=production 时 throw Error 拦截
  T: S0-3 | env.ts | cookieSecret 弱默认值，与 config.ts 双轨密钥系统 | 合并到 config.ts，删除 env.ts 独立密钥配置
  T: 编号 | 文件 | 问题 | 修复方向
  T: P1-1 | logger.ts | this.level 构造时快照，setGlobalLevel 对已有实例无效 | 改为动态读取 Logger.globalLevel
  T: P1-2 | llm.ts | assertApiKey 错误提示变量名错误（OPENAI vs BUILT_IN_FORGE） | 修正提示信息
  T: P1-3 | llm.ts | thinking.budget_tokens 硬编码，调用方无法覆写 | 提取为 InvokeParams 可选字段
  T: P1-4 | registry.ts | emit() 错误日志调用签名不规范（第二参数传 err 对象） | 改为 log.error({ err }, message) 格式
  T: P1-5 | serviceRegistry.ts | client.keys() 全表扫描阻塞 Redis | 改用 SCAN 迭代
  T: 编号 | 文件 | 问题 | 修复方向

================================================================================
## ok平台中间件层代码审查报告.docx
段落数: 204, 表格行数: 68
================================================================================
  P4: 综合评分
  P6: 中间件层整体质量高；连接器层存在体系割裂；auth.service 为桩代码、安全漏洞遗留
  P8: 本次审查覆盖 xilian-platform 平台层全部 25 个 TypeScript 源文件，分布于 routes/、middleware/、connectors/、services/、cognition/dimensions/ 五个目录。文件总体呈现"三档分布"格局：中间件层（circuitBreaker、gracefulShutdown、metricsCollector、auditLog 等）质量优秀，达到工业级水准；服务层（configCenter、dataFlowTracer、resourceDiscovery）设计合理；连接器层（4 个 connector）则呈现明显的"双速开发
  P9: 1.1 文件评分总览
  P10: 二、安全漏洞专项 — P0
  P34: 综合评分  █████████░  9/10
  P41: 综合评分  ████████░░  8/10
  P44: ✗ 缺陷  每个 protect* 函数为方法建立独立的断路器实例（`serviceName.methodName` 格式），导致 Redis 的 get/set/del 等方法各有独立断路器，同一服务故障时各方法分别独立熔断，可能出现"set 已熔断但 get 仍执行"的状态不一致问题。
  P47: 综合评分  ████████░░  8/10
  P54: 综合评分  █████████░  9/10
  P61: 综合评分  ████████░░  8/10
  P65: ✗ 缺陷  flush() 中为关联敏感日志，需要通过 traceId 反查 auditLogs 表获取 auditLogId，每条敏感记录需要一次额外 SELECT 查询。高并发时 N 条敏感记录 = N 次额外查询，性能差。
  P70: 综合评分  █████████░  9/10
  P73: ✗ 缺陷  registerBuiltinShutdownHooks 中"database"和"data-artery"两个钩子均注册为优先级 50，当多个同优先级钩子执行时顺序不确定。data-artery 依赖 Kafka 和 Redis，理应在它们之前关闭（优先级更低的数值，如 28）。
  P78: 综合评分  █████████░  9/10
  P85: 综合评分  ████████░░  8/10
  P88: ✗ 缺陷  getConfig() 中 samplingRatio 从环境变量读取，但 initOpenTelemetry 调用后 sampler 实例被固化，运行时修改 OTEL_SAMPLING_RATIO 无效。这与 configCenter 的热更新理念冲突。
  P89: ⚠ 警告  auto-instrumentation 中 @opentelemetry/instrumentation-redis-4 与代码库使用的 ioredis 客户端（redis.client.ts）不匹配。ioredis v4 对应的是 @opentelemetry/instrumentation-ioredis，建议替换。
  P91: 综合评分  ████████░░  8/10
  P97: 综合评分  ███████░░░  7/10
  P101: ✗ 缺陷  startAutoRenewal 的 setInterval 回调中，startAutoRenewal 函数只检查 renewalInterval 是否已存在，不检查 null 值。若先调用 stopAutoRenewal() 再调用 startAutoRenewal()，新定时器将被正确创建，但已续租中的 secretCache 条目的 expiresAt 会被修改（`cached.expiresAt = now + config.cacheTtlMs`），可能导致未续租成功的密钥被延长有效期。
  P106: 综合评分  █████░░░░░  5/10
  P108: ✗ 缺陷  healthCheck 捕获异常后返回 { status: "unhealthy" } 但吞掉了错误原因，调用方（system.routes.ts 的 healthRouter）无法区分"连接拒绝"和"查询超时"。
  P110: ✗ 缺陷  getConnection() 返回 Drizzle ORM 的 db 实例，但 db 不是标准数据库连接对象，该方法名和返回值语义不一致，调用方可能误解为获取了原生连接。
  P113: 综合评分  ███████░░░  7/10
  P118: 综合评分  ████░░░░░░  4/10
  P119: ✗ 缺陷  query() 直接读取 process.env.CLICKHOUSE_URL，绕过 config.ts 的统一配置，与配置体系不一致。
  P120: ✗ 缺陷  HTTP 响应处理只检查 !res.ok，未区分 400（查询语法错误）和 500（服务器故障），错误处理粒度太粗。ClickHouse 的 HTTP 接口错误信息在响应体中，未被提取。
  P121: ✗ 缺陷  fetch 不设置 timeout，慢查询将导致调用方永久挂起。
  P125: 综合评分  ███░░░░░░░  3/10
  P127: ✗ 缺陷  healthCheck 执行 SHOW SPACES 查询，该命令需要列出所有图空间，在大型集群上开销不小。建议改用 SHOW HOSTS 或轻量级的版本查询。
  P128: ✗ 缺陷  r.json() 被直接返回但未检查 HTTP 状态码，当 Nebula 返回 4xx/5xx 时也会静默返回 JSON 错误对象。
  P131: 综合评分  █████░░░░░  5/10
  P134: ✗ 缺陷  结果从 d.result 提取，Qdrant v1.x 返回格式为 { result: [...] }，但 v2.x 已调整为 { points: [...] }，硬编码字段名导致升级时静默返回空数组。
  P135: ✗ 缺陷  fetch 无 timeout，search 请求对于大向量集合可能耗时较长。
  P138: 综合评分  ████████░░  8/10
  P142: ✗ 缺陷  loadFromRedis 使用 client.keys("config:*")，同 serviceRegistry 的问题——全表扫描，生产环境应改为 SCAN。
  P146: 综合评分  ████████░░  8/10
  P150: ✗ 缺陷  getFlowGraph 中调用 require("../../core/registries/module.registry")，在 ESM 模块中混用 require() 破坏了模块系统一致性，在某些 Node.js 版本下会报错。
  P155: 综合评分  ███████░░░  7/10
  P156: ✓ 亮点  scan() 使用 Promise.allSettled 并行运行所有扫描器，单个扫描器失败不阻断其他扫描器，容错设计正确。
  P158: ✗ 缺陷  ClickHouseTableScanner 硬编码了 4 张业务表（sensor_data、device_events、metrics、audit_logs），而 MySQLTableScanner 通过 schemaRegistry 动态发现表，两者策略不一致。ClickHouse 硬编码的表可能与实际部署不符。
  P162: 综合评分  ██████░░░░  6/10
  P164: ✗ 缺陷  listTables() 查询 information_schema.TABLES 无分页，若数据库包含数百张表，结果集可能导致内存压力。建议添加 LIMIT 并支持分页参数。
  P167: 综合评分  ███████░░░  7/10
  P168: 问题与上批次审查结论一致：loadFromRedis 使用 client.keys() 全扫描，K8s 模式下 cacheRefreshTimer 空转。本批次无新增问题，修复建议参见上批次 P1-5 和 A2-4。
  P170: 综合评分  ████████░░  8/10
  P173: ✓ 亮点  applyConstraintsAndRank 的综合评分 = 0.7 × 熵减少 + 0.3 × 原始优先级，权重合理，熵优先但不完全忽略业务优先级。
  P174: ✗ 缺陷  ActionTemplate 中 type: "deploy" 的 trigger 为 ctx.stimulus.type === "training_completed"，但 CognitionStimulus.type 枚举中不存在 "training_completed"（上批次审查已标注此问题），导致 deploy 动作永远不会被触发。
  P180: system.routes.ts 中大量使用 as any 传递数据库操作结果（db.insert().values(input as any)、db.update().set({...data} as any)），这绕过了 Drizzle ORM 的类型检查，数据库 schema 变更时编译器无法提示调用方更新。
  P188: 本批次新发现的配置读取分散问题（在上批次 config.ts 双轨问题的基础上进一步扩散）：
  P190: 8.1 P0 — 安全漏洞（生产部署前必须修复）
  P191: 8.2 P1 — 功能 Bug（近期修复）
  P192: 8.3 P2 — 架构改进（规划迭代）
  P197: auth.service.ts 桩代码 — 已从上批次蔓延到 auth.routes.ts，形成对外暴露的安全漏洞（最高优先级）
  --- TABLE ISSUES ---
  T: vaultIntegration.ts | middleware | 7/10 | 原生 http/https 替代 SDK；auto-renewal 定时器泄漏风险
  T: redis.connector.ts | connectors | 7/10 | 正确代理 ioredis；silent fail 在生产可能掩盖问题
  T: auth.service.ts | services | 1/10 | 桩代码，已通过 auth.routes 暴露为公开 API（P0 安全漏洞）
  T: serviceRegistry.ts | services | 7/10 | 四策略 LB；client.keys() 全扫描问题同上批次
  T: 文件 | any 出现位置 | 风险
  T: 文件 | 读取方式 | 问题
  T: 编号 | 文件 | 问题 | 修复方向
  T: S0-1 | auth.service.ts + auth.routes.ts | 桩认证已通过 publicProcedure 对外暴露 | 实现真实 JWT 验签；auth.routes 改为 protectedProcedure
  T: S0-2 | system.routes.ts | CRUD 路由无鉴权；sensitiveList 公开 | 改为 adminProcedure/protectedProcedure
  T: S0-3 | 四个 connectors | 无认证（API key/Basic Auth）；无断路器保护 | 添加认证头；注册到 circuitBreakerIntegration
  T: S0-4 | vaultIntegration.ts | config.caCert 定义但 TLS 未实际验证 | 在 vaultRequest 中传入 ca 选项
  T: 编号 | 文件 | 问题 | 修复方向
  T: P1-1 | decision-processor.ts | stimulus.type === 'training_completed' 枚举值不存在 | 对齐实际枚举值
  T: P1-2 | auditLog.ts | sensitiveEntry 关联查询 N+1 问题 | 改用 lastInsertId 或 RETURNING id
  T: P1-3 | circuitBreakerIntegration.ts | 同服务方法各自独立断路器，状态不一致 | 同服务方法共享一个断路器实例
  T: P1-4 | gracefulShutdown.ts | database 和 data-artery 两钩子均 priority=50，顺序不确定 | data-artery 改为 p=28
  T: P1-5 | configCenter.ts | loadFromRedis 使用 client.keys() 全扫描 | 改为 SCAN 迭代
  T: P1-6 | securityHeaders.ts | generateRequestId 使用 Math.random() | 改为 crypto.randomUUID()
  T: 编号 | 涉及文件 | 问题 | 修复方向
  T: 本周 | 立即 | 消除安全漏洞 | S0-1 ~ S0-4：实现真实认证，添加路由鉴权，connector 认证头
  T: 下周 | 1-2周 | 修复功能 Bug | P1-1 ~ P1-6：枚举值、N+1、断路器、钩子优先级

================================================================================
## ok数据库服务层·中间件层·路由层代码审查.docx
段落数: 82, 表格行数: 53
================================================================================
  P4: 第一部分：批欬3 整体质量评分
  P5: 本批欬14个文件覆盖数据库服务层（6个）、平台中间件层（4个）、tRPC路由层（4个）。整体质量高于前两批，平均评分 8.4/10。
  P23: 2.3 diagnosis.db.service.ts — 软删除缺失
  P30: 3.1 circuitBreaker.ts — 生产级实现，存在权限漏洞
  P34: ⚠ 权限漏洞（P1）：
  P39: 3.2 rateLimiter.ts — Docker URL豊免漏洞
  P49: ⚠ 竞态条件（P2）：敏感日志通过 traceId 匹配 auditLogId，批量插入后 SELECT 查找 ID，存在时序窗口。建议使用 insert().returning('id') 直接获取主键。
  P53: 4.1 P0安全修复落地状态不一致
  P54: 4.2 fusionDiagnosis.router.ts — 新增 P0 问题
  P55: 这是本批欬发现的最大安全漏洞。完整暴露了诊断引擎的控制面，无需任何认证：
  P70: 6.1 新增 P0 问题
  P71: 6.2 新增 P1 问题
  P72: 6.3 新增 P2 问题
  --- TABLE ISSUES ---
  T: backpressure.ts | 9/10 | ✓ | 工业级背压控制；Token Bucket+自适并发+Kafka集成；p99计算有边界风险
  T: database.router.ts | 7/10 | ⚠ | P0安全修复已落地（别名技巧）；但别名法掩盖真实鉴权状态，请正确修复
  T: 存储位置 | 实现方式 | 问题
  T: 路由文件 | 修复状态 | 说明
  T: database.router.ts | ⚠ 别名修复 | const publicProcedure = protectedProcedure — 技术上有效，但代码可读性差，未来可能误导维护者
  T: algorithm.router.ts | ✅ 正确修复 | 查询用publicProcedure，mutation用protectedProcedure — 本批欬最佳实践
  T: fusionDiagnosis.router.ts | ✗ 未修复 | 全部12个端点仍用 publicProcedure（含 diagnose/updateWeight/registerExpert 等写操作）
  T: plugin.router.ts | ⚠ 部分修复 | 写操作已用protectedProcedure；但getAuditLog/getSecurityEvents等安全敏感接口仍用publicProcedure
  T: 端点 | 类型 | 风险 | 影响
  T: 中间件 | 集成状态 | 问题
  T: 级别 | 编号 | 文件 | 问题描述 | 修复方案
  T: P0 | P0-6 | fusionDiagnosis.router.ts | diagnose/updateWeight/registerExpert等写操作全用publicProcedure，无需认证 | 所有mutation改为protectedProcedure
  T: 级别 | 编号 | 文件 | 问题描述 | 修复方案
  T: P1 | P1-6 | circuitBreaker.ts | forceOpen/forceClose无鉴权，任意代码可手动燕断关键服务 | 通过admin专用protectedProcedure路由暴露
  T: P1 | P1-7 | rateLimiter.ts | skip()用url.includes()匹配，可被构造URL绕过认证限流 | 改为解析tRPC路径后精确匹配
  T: P1 | P1-8 | 4个中间件文件 | 中间件已实现但未确认挂载状态 | 检查server/index.ts确认实际集成
  T: P1 | P1-9 | fusionDiagnosis.router.ts | diagnosisHistory内存存储，重启丢失，无法水平扩展 | 迁移至Redis ZSET存储
  T: P1 | P1-10 | plugin.router.ts | getAuditLog/getSecurityEvents等安全审计接口用publicProcedure | 改为protectedProcedure，安全日志应受保护
  T: 级别 | 编号 | 文件 | 问题描述 | 修复方案
  T: P2 | P2-8 | knowledge.db.service.ts | 向量数据双写（MySQL TEXT + Qdrant），MySQLkbEmbeddings无实际用途 | 删除vectorData字段，仅存Qdrant point ID
  T: P2 | P2-9 | auditLog.ts | 敏感日志通过traceId关联存在竞态，高并发下可能关联错误 | 使用insert().returning()直接获取主键
  T: P2 | P2-10 | data.service.ts | EventStore无乐观锁，aggregateVersion无版本冲突检测 | 添加 WHERE aggregateVersion = expected 乐观锁
  T: P2 | P2-11 | database.router.ts | publicProcedure别名技巧掩盖了真实鉴权意图 | 移除别名，直接使用protectedProcedure
  T: P2 | P2-12 | edge.db.service.ts | ipAddress存明文；heartbeat()未校验gatewayCode存在性 | 添加IP格式校验；heartbeat改为upsert语义
  T: 问题层 | P0 | P1 | P2 | 核心问题描述
  T: 总计 | 8 | 13 | 12 | 33个问题：P0生产阻塞8个，P1需1-2周修复13个，P2优化项1 2个，覆盖41个文件

================================================================================
## okAPI路由层代码审查报告.docx
段落数: 86, 表格行数: 68
================================================================================
  P3: 综合评分：7.2 / 10
  P5: 发现：P0安全漏洞 3项 | P1功能Bug 5项 | P2架构改进 7项
  P10: 二、P0 安全漏洞（生产部署前必须修复）
  P11: 以下3项安全漏洞属于高危级别，必须在系统上线前修复。核心问题是认证保护缺失，攻击者可在无需任何凭证的情况下访问、修改或破坏核心系统配置。
  P20: fusionDiagnosis.router.ts：专家注册/注销（registerExpert/unregisterExpert）无保护，可注入恶意诊断专家
  P28: 三、P1 功能 Bug（近期迭代修复）
  P30: 四、P2 架构改进建议
  P42: database.router.ts 达1200行，包含13个子路由（asset/config/slice/clean/event/workbench/pluginDb/opsDb/governanceDb等），严重违反单一职责原则，可读性和可维护性极差。建议拆分为独立文件（asset.router.ts、config.router.ts、workbench.router.ts等），每个文件聚焦单一领域。
  P44: docker.router.ts 的 getCoreServices() 返回固定顺序的服务列表（MySQL → Redis → Kafka → ClickHouse → Qdrant → MinIO），无法根据服务间的依赖关系自动调整启动顺序。建议在服务定义中声明依赖关系，由启动流程自动计算拓扑排序后依序启动。
  P46: microservice.router.ts 的 SERVICE_REGISTRY 中硬编码了7个微服务定义（API Gateway/Auth Service/Rule Engine等），新增服务需要修改路由代码。建议将服务定义迁移到配置文件或统一注册中心，支持动态加载。
  P48: observability.router.ts 部分接口使用模拟数据服务（PrometheusService.getInstance()），部分接口使用真实客户端（prometheusClient/jaegerClient），逻辑不一致且难以测试。建议统一使用真实客户端为主，模拟数据服务仅在开发环境（NODE_ENV=development）下启用，并通过环境变量控制。
  P50: pipeline.router.ts 的 resourceDiscovery.scan() 扫描的资源类型（MySQL表/Kafka Topic/Qdrant集合/模型/插件等）硬编码在服务中。建议从注册中心动态加载扫描器配置，新增资源类型时无需修改代码。
  P61: 六、文件评分汇总
  P63: 第一优先级：本周内（安全漏洞修复）
  P64: 修复S0-1：accessLayer.router.ts 所有mutation改为protectedProcedure或adminProcedure
  P65: 修复S0-2：advancedDistillation/conditionNormalizer/fusionDiagnosis/grokDiagnostic/kgOrchestrator/platformHealth 的mutation统一改为protectedProcedure
  P66: 修复S0-3：registry.router.ts 添加速率限制（建议：每IP每分钟100次请求）；评估是否需要改为protectedProcedure
  P69: 修复P1-1：docker.router.ts 迁移失败时阻断bootstrapAll并返回详细错误
  P70: 修复P1-2：kafka.router.ts listTopics添加source字段（'kafka'|'predefined'）
  P71: 修复P1-3：microservice.router.ts 从真实Prometheus查询指标或明确标记isSimulated
  P72: 修复P1-4：observability.router.ts 统一回退逻辑，优先真实数据
  P73: 修复P1-5：pipeline.router.ts legacyConfigToDAG添加默认timezone
  P84: 平台整体已具备完整的工业IoT智能诊断平台功能，从接入层到算法、诊断、可观测性均有覆盖。核心挑战是安全体系的系统性缺失——三轮审查累计发现13项P0安全漏洞，表明平台在快速功能开发阶段对安全认证的投入不足。建议在下一阶段专项进行安全加固，建立系统性的安全开发规范（SDL），确保后续迭代的安全质量。
  --- TABLE ISSUES ---
  T: 综合评分 | 7.2 / 10（较第二轮7.2持平，安全问题更集中）
  T: P0安全漏洞 | 3项（需在生产部署前修复）
  T: P1功能Bug | 5项（近期迭代修复）
  T: P2架构改进 | 7项（规划迭代处理）
  T: 核心安全风险 | 6个核心路由文件全部使用publicProcedure，无认证保护
  T: S0-1 | accessLayer.router.ts | 高危
全部12个接入层管理端点使用publicProcedure，包括Connector/Endpoint/Binding的增删改查操作。任何未登录用户均可创建/删除连接器、修改端点配置、绑定目标系统，可能导致数据泄露或恶意数据注入。
  T: S0-2 | 6个路由文件 | 高危
advancedDistillation、conditionNormalizer、fusionDiagnosis、grokDiagnostic、kgOrchestrator、platformHealth 共6个核心业务路由文件全部使用publicProcedure。攻击者可无需认证即可执行知识蒸馏训练、修改归一化配置、注册融合诊断专家、触发Grok诊断、修改知识图谱、查询平台健康诊断报告。
  T: S0-3 | registry.router.ts | 中危
统一注册中心全部查询接口使用publicProcedure，暴露了平台所有能力的元数据（类型列表/分类/配置Schema/API端点）。虽为只读接口，但攻击者可以枚举平台所有能力、发现潜在攻击面、为后续攻击制定策略。
  T: 编号 | 位置 | 问题描述 | 修复方向
  T: P1-1 | docker.router.ts
bootstrapAll | MySQL迁移失败后仍继续启动后续服务并返回success:true，后续服务因表结构缺失而报错，问题难以定位 | migrate失败时立即返回success:false并阻断后续服务启动，在postInit阶段添加严格错误处理
  T: P1-2 | kafka.router.ts
listTopics | Kafka未连接时返回预定义主题列表但不加以标记，前端无法区分是真实已创建主题还是预定义模板主题 | 返回结构添加source字段（'kafka'或'predefined'），前端根据此字段差异化显示
  T: P1-3 | microservice.router.ts
getPrometheusMetrics | 使用hashString+deterministicRandom生成伪随机时序数据，与真实Prometheus指标无关，但前端无感知 | 优先从真实Prometheus查询历史数据；若Prometheus不可用，明确标记isSimulated:true
  T: P1-4 | observability.router.ts
getNodeMetrics | 优先使用PrometheusService模拟数据服务，与其他接口优先真实客户端的逻辑相反，造成数据源不一致 | 统一回退逻辑：优先真实prometheusClient，失败才使用模拟数据；模拟数据添加来源标记
  T: P1-5 | pipeline.router.ts
legacyConfigToDAG | 旧版PipelineConfig转换为DAG时schedule.timezone字段丢失（旧格式无此字段），造成调度时区错误 | 添加默认timezone（如从ENV.TZ或'Asia/Shanghai'读取）；或在转换时提示用户补全
  T: accessLayer.router.ts | 5/10 | 全用publicProcedure，无认证保护（P0）
  T: advancedDistillation.router.ts | 6/10 | 全用publicProcedure（P0），功能完整
  T: conditionNormalizer.router.ts | 6/10 | 全用publicProcedure（P0），12个端点
  T: fusionDiagnosis.router.ts | 6/10 | 全用publicProcedure（P0），D-S证据理论
  T: grokDiagnostic.router.ts | 6/10 | 全用publicProcedure（P0），三种诊断模式
  T: kgOrchestrator.router.ts | 6/10 | 全用publicProcedure（P0），图谱编排
  T: platformHealth.router.ts | 6/10 | 全用publicProcedure（P0），平台健康
  T: workbench.router.ts | 8/10 | 数据库工作台，5个子路由，P0加固
  T: 目标：消除所有P0安全漏洞，防止未授权访问核心系统
  T: 目标：修复已知功能Bug，提升系统可靠性和前端数据准确性
  T: 第一轮（核心基础设施层） | 综合评分 6.1/10 | 10个文件 | 5项P0漏洞
  T: 第二轮（平台服务与中间件层） | 综合评分 7.2/10 | 25个文件 | 5项P0漏洞
  T: 第三轮（API路由层） | 综合评分 7.2/10 | 33个文件 | 3项P0漏洞
  T: 三轮累计发现P0漏洞 | 13项高危安全漏洞
  T: 三轮累计发现P1 Bug | 17项功能Bug
  T: 三轮累计P2架构改进 | 22项架构优化建议
  T: 总体趋势 | 功能完整性逐轮提升；安全问题仍是最大风险；架构复杂度较高

================================================================================
## 前端核心层审查报告.docx
段落数: 84, 表格行数: 46
================================================================================
  P5: 共发现问题 17 项：P1 级 3 项（含 2 项安全/数据漏洞、1 项功能缺陷）、P2 级 8 项、P3 级 6 项。同时记录了 7 项架构亮点。
  P13: App.tsx 的路由配置完整覆盖了所有功能模块，路由结构清晰，使用 wouter Switch+Route 模式，ErrorBoundary 全局包裹。发现两个值得关注的问题：
  P16: 修复建议：删除 localStorage 写入，若需跨 Tab 共享用户信息，使用 BroadcastChannel API 或 sessionStorage（更短生命周期），同时在 logout 时显式调用 localStorage.removeItem。
  P18: appStore 中大量 Agent、Plugin 的默认数据（6 个 Agent、6 个 Plugin）以明文硬编码嵌入 Store 初始化，这些是 Demo 数据而非配置，建议迁移到单独的 constants/demoData.ts 文件，保持 Store 干净。appStore 使用 zustand persist 将全部状态持久化到 localStorage，包括 pipelineNodes、pipelineConnections 等大型画布状态，可能导致 localStorage 超出 5MB 限制。建议对画布状态使用 sessionStorage 或不持久化。
  P27: navigationConfig 共定义了约 70 个导航条目，覆盖全部功能模块，结构清晰分层（核心业务/资产数据/基础设置/智能引擎/平台管理）。发现 1 个一致性问题：status-microservices 导航条目（label: '微服务监控'）存在于 navigation.ts，但 App.tsx 中无对应路由（已在 P3-E2 中标注）。navigation.ts 末尾被截断（最后一行为 { id: 'status-microservices', lab），但从上下文判断缺失的是 label 字段，不影响已解析的条目功能。
  P75: 第 1 步（立即）：删除 useAuth.ts 中 localStorage.setItem('manus-runtime-user-info', ...)，修复 P1-A1 安全漏洞。
  P76: 第 2 步（立即）：删除 appStore.ts 中 API_BASE 常量并全局搜索其使用位置，修复 P1-A2 配置错误。
  P77: 第 3 步（本迭代）：为 EvolutionBoard 实现 trpc.evolution.* 接口调用，替换全部 Mock 数据（P1-V1）。
  P78: 第 4 步（本迭代）：为 pipelineEditorStore completeConnection 添加 DFS 环路检测（P2-S1）。
  P79: 第 5 步（本迭代）：将 types/index.ts 中 DataSourceConfig.password 移除，改为后端持有（P2-T1）。
  P80: 第 6 步（下迭代）：为 AutoTrain 训练任务 CRUD 实现 tRPC 对接，移除 Mock 数据（P2-Tr1）。
  P81: 第 7 步（下迭代）：为 SystemTopology 节点拖拽添加 mouseup 节流，合并位置更新请求（P2-Tp1）。
  P82: 第 8 步（计划中）：为 Infrastructure 容器 stop/restart 添加 AlertDialog 确认弹窗（P2-I1）。
  --- TABLE ISSUES ---
  T: 25
审查文件数 | 3
P1 高优先级 | 8
P2 中优先级 | 6
P3 低优先级
  T: P2-E1  本地开发认证跳过基于 hostname 检测，存在绕过风险
redirectToLoginIfUnauthorized 中通过 window.location.hostname === 'localhost' || '127.0.0.1' 判断是否跳过重定向。但在 Docker Compose 开发环境中，前端容器可能使用服务名访问（如 portai-nexus.local），使该条件失效，反复重定向。更重要的是，这一判断只是前端视觉行为，实际的 SKIP_AUTH 由服务端控制（见批次 13-14 P1-D1），两套机制不同步。
P3-E1  QueryClient 未配置全局
  T: P2-E2  大量旧路由用 Redirect 兜底，新用户无上下文提示
/device/list、/device/maintenance、/device/alerts、/device/kpi 以及 /docs 均直接重定向到新路径，无任何提示信息。若开发者在集成测试中使用旧路径，会静默成功进入新页面，掩盖了路径映射错误。建议改为带说明的包装组件，或在控制台打印 deprecation warning。
P3-E2  MicroserviceDashboard 路由缺失
navigation.ts 中有 status-microservices 导航条目（label: '微服务监控'），但 Ap
  T: P1-A1  用户信息明文写入 localStorage（敏感数据泄露风险）
useAuth.ts 第 44 行：localStorage.setItem('manus-runtime-user-info', JSON.stringify(meQuery.data))，将完整的用户对象（包含 userId、roles、permissions 等敏感字段）序列化后写入 localStorage。问题有三：(1) localStorage 可被同源的任意 JS 脚本读取，若存在 XSS 漏洞则直接暴露身份信息；(2) 写入操作放在 useMemo 中，每次用户信息变更都会执行，包括 logout 
  T: P1-A2  appStore 硬编码 API_BASE = 'http://localhost:8000'，生产部署失效
appStore.ts 第 11 行：export const API_BASE = 'http://localhost:8000'。这个常量在生产环境中指向错误地址，更严重的是：(1) 该常量被 export 导出但在项目中是否有实际消费者无法通过本次审查确认；(2) 若任何组件或 service 文件直接使用 API_BASE 发起 fetch（绕过 tRPC），将在生产环境完全失效；(3) 带有明确端口 8000 暗示这可能是历史开发用途遗留（tRPC 代理端口为 
  T: P2-S1  completeConnection 缺少环路检测（DAG 完整性）
completeConnection 只检查了 fromNodeId===toNodeId（自连），但未检测是否会形成有向环（如 A→B→C→A）。对于 Pipeline DAG，有向环会导致执行引擎无限循环。应在 completeConnection 中使用 DFS/BFS 从 toNodeId 出发检测是否存在到 fromNodeId 的可达路径。
  T: P2-T1  types/index.ts 中 DataSourceConfig 含明文 password 字段
DataSourceConfig 接口定义了 password?: string 字段，若该类型被序列化为 JSON 存入 zustand persist（appStore 已 persist），用户的数据库密码将以明文形式持久化在 localStorage 中。应将密码字段从前端类型中移除，改由后端持有，前端只传递 connectionId 引用。
P3-T1  types/index.ts 与 tRPC 返回类型存在结构重复
Model、DatabaseConfig、Topol
  T: P2-H1  disconnect 不重置 reconnectAttempts，重连逻辑边界不清晰
reconnect() 中先调用 disconnect() 再 connect()，但 disconnect() 不重置 reconnectAttempts.current。若之前已经自动重连失败 4 次，手动调用 reconnect() 后再次断开，只会再尝试一次就进入 maxReconnectAttempts 上限。修复：disconnect() 中加入 reconnectAttempts.current = 0，或在 reconnect() 开头显式重置。
  T: P2-M1  sendMessage 的乐观更新实现有竞态风险
handleSendMessage 中先用 setMessages(prev=>[...prev,tempUserMessage]) 添加临时消息，再调用 sendMessageMutation.mutate()。若用户在前一条消息还未收到响应时再次发送，setMessages(prev=>prev.slice(0,-1)) 会误删新消息而非旧临时消息，因为 slice(0,-1) 只删除最后一条。应使用 tempId 精准匹配删除，如 setMessages(prev=>prev.filter(m=>m.id!==tempId)
  T: P2-F1  FusionDiagnosis 中 beliefMass 归一化判断缺失（已在批次审查中记录，此处确认前端层复现）
BeliefMassChart 中 maxVal=Math.max(...entries.map(([,v])=>v),0.01)，柱宽基于 value/maxVal 计算，而非固定 value/1.0（单位归一化）。若后端返回的 beliefMass 未归一化（总和 ≠1），柱图的相对高度视觉上是正确的，但用户无法直观看出总概率和不为 1 的异常状态。建议添加总和校验，若 |sum-1| > 0.05 则显示警告。
P3-F1  FAULT_TYPE_LABELS
  T: P1-V1  EvolutionBoard 整页使用 Mock 数据，无 tRPC 对接
文件中 mockModels、mockRules、mockHealthMetrics 全部为静态数据，页面头部未导入 trpc，所有「触发重训」「切换规则」「查看进化报告」按钮均只触发本地 useState 操作。这意味着进化看板展示的模型版本、健康分、演化规则均为虚假数据，无法反映系统真实进化状态。考虑到进化引擎是平台的核心价值主张之一，此问题严重性为 P1。应实现 trpc.evolution.getModels / trpc.evolution.getRules / trpc.evolution.g
  T: P2-Tr1  AutoTrain 使用 Mock 数据，TrainJob CRUD 无后端对接
mockJobs 硬编码了 5 个训练任务，startTrainMutation、terminateTrainMutation、retryTrainMutation 等 mutation 实际上只是 toast 提示 + setTimeout 模拟进度变化，没有调用任何 tRPC 接口。训练进度的实时更新（progress 百分比变化）应通过 WebSocket 或 tRPC 订阅实现。
P3-Tr1  AutoTrain 中 statusConfig 中的 icon 使用 React.React
  T: P2-CN1  ConditionNormalizer tRPC 调用与 Mock 混用，生产环境行为不一致
归一化控制台（normalizeQuery）和基线管理（listBaselines / updateThresholds / learnBaseline）调用了真实 tRPC，但工况定义管理 Tab 使用 useState 维护 mockConditions，工况的增删改操作不持久化到后端。这种混用模式会造成工况定义在页面刷新后重置，与真实基线数据不一致。应统一改为 tRPC 对接。
  T: P2-Tp1  SVG 节点拖拽时频繁调用 updateNodePositionMutation，无节流
mousemove 事件直接触发 updateNodePositionMutation.mutate()，每次鼠标移动（约 16ms 间隔）都会发起一次 tRPC 请求。拖拽一秒约触发 60 次请求，导致服务端高负载和 UI 卡顿。应使用 requestAnimationFrame 或 lodash.throttle（100ms）仅在 mouseup 时提交最终位置，中间过程只更新本地状态。
P3-Tp1  hasFittedView 状态未使用 useEffect 依赖控制，可能多次执行
  T: P2-Pd1  setModuleEnabledMutation 修改模块开关后未 refetch overviewQuery
toggleModule 成功后调用 featureFlagsQuery.refetch()，但 overviewQuery（包含 totalEnabled / totalDisabled 统计）未同步刷新。用户关闭一个模块后，顶部统计卡片数字不会立即更新，需等 30s 自动刷新才能体现变化。应在 onSuccess 中同时调用 overviewQuery.refetch()。
  T: P2-I1  Infrastructure 中容器操作 mutation 缺少确认对话框
containerRestartMutation 和 containerStopMutation 直接绑定在按钮 onClick 上，点击即执行，没有确认弹窗。在生产环境误触「停止 MySQL」会导致服务中断。应使用 AlertDialog 二次确认高风险操作（stop/restart），仅 start 操作可直接执行。
  T: P1-A1 | useAuth.ts | P1 | 安全漏洞 | 用户信息（含 roles/permissions）明文写入 localStorage
  T: P1-A2 | appStore.ts | P1 | 配置错误 | 硬编码 API_BASE='http://localhost:8000'，生产部署失效
  T: P1-V1 | EvolutionBoard.tsx | P1 | 功能缺陷 | 整页 Mock 数据，进化引擎状态全部虚假，无 tRPC 对接
  T: P2-E1 | main.tsx | P2 | 安全风险 | 本地开发 hostname 检测跳过 401 重定向，Docker 环境失效
  T: P2-E2 | App.tsx | P2 | 用户体验 | 旧路由 Redirect 无提示，掩盖路径映射错误
  T: P2-S1 | pipelineEditorStore.ts | P2 | 功能缺陷 | completeConnection 缺少 DAG 环路检测，Pipeline 可形成死循环
  T: P2-T1 | types/index.ts | P2 | 安全漏洞 | DataSourceConfig 含明文 password 字段，持久化到 localStorage
  T: P2-M1 | ModelCenter.tsx | P2 | 竞态问题 | sendMessage 乐观更新 slice(0,-1) 在并发时误删错误消息
  T: P2-F1 | FusionDiagnosis.tsx | P2 | 显示缺陷 | beliefMass 柱图无总和归一化校验，异常状态不可见
  T: P2-Tr1 | AutoTrain.tsx | P2 | 功能缺陷 | 训练任务 CRUD 无后端对接，进度更新为本地模拟
  T: P2-CN1 | ConditionNormalizer.tsx | P2 | 数据一致性 | 工况定义 Tab 使用 Mock，与真实基线数据不一致
  T: P2-Tp1 | SystemTopology.tsx | P2 | 性能问题 | 节点拖拽无节流，每 16ms 触发一次 tRPC 请求
  T: P2-Pd1 | PlatformDiagnostic.tsx | P2 | 数据同步 | 模块开关修改后 overviewQuery 未同步刷新
  T: P2-I1 | Infrastructure.tsx | P2 | 安全风险 | 容器 stop/restart 无确认弹窗，生产误操作风险
  T: 最终评估
经过 15 个批次、130+ 个核心文件的全量审查，PortAI Nexus 项目整体工程质量达到工业级 AI 平台的高标准。架构设计前瞻（React 19 / tRPC 11 / Drizzle ORM / Tailwind v4 / 11 业务域 64 张表），功能覆盖完整（融合诊断 / 进化引擎 / 算法库 / 知识图谱 / 系统拓扑 / 平台诊断 Agent），大量技术细节处理精准（DS 证据融合 / EWMA 工况归一化 / Kafka 流处理 / Outbox 模式 / Saga 补偿）。
核心待修复问题归纳为三类：
① 安全类（最高优先级）：SKIP_AUTH 生产默认

================================================================================
## 基础设施层代码审查.docx
段落数: 94, 表格行数: 78
================================================================================
  P6: 综合评分总览
  P7: 综合评分：6.5 / 10  —  基础扎实，插件引擎与 AI 集成层需大幅加强
  P28: redis.client.ts 实现了完整的 Redis Streams API（xadd/xread/xreadgroup/xack/xpending 等），这是一个超出普通 Redis SDK 的优秀扩展。但存在以下问题：
  P35: tRPC context 引用了 TrpcContext，但上下文构建（context.ts）不在审查文件中，无法确认 ctx.user 的来源与验证方式，需确保 JWT/Session 解析不可绕过。
  P44: 这是审查中评分最低的维度（5/10）。当前平台缺少任何形式的插件/扩展注册机制，所有客户端都是硬编码实例，无法在运行时动态加载、替换或组合。这会在以下场景造成严重阻碍：
  P53: AI 集成层目前是最薄弱的维度（4/10）。Qdrant 向量数据库已经接入，但整个 AI 推理、Embedding 生成、RAG 管道均缺少完整的抽象层，以散装形式分布在 databaseMonitor 的副作用中，无法支持后续的算法执行、模型编排等核心业务。
  P66: P0 — 立即修复（阻塞后续开发）
  P72: P1 — 短期优化（1-2 周）
  P79: P2 — 中期增强（1 个月）
  P92: 然而，当前插件引擎的缺失和 AI 集成层的薄弱是最关键的结构性问题。如果在此基础上直接堆积业务代码，将在 3-6 个月内产生大量技术债务，阻碍平台的扩展能力。建议在进入算法服务、设备管理等核心业务开发之前，优先完成 P0 清单的 5 项修复，以确保地基稳固。
  --- TABLE ISSUES ---
  T: 安全性 Security | 6/10 | ██████░░░░ | 硬编码凭证存在泄露风险
  T: 文件/模块 | 级别 | 问题描述 | 改进建议
  T: databaseMonitor.ts | P1 | MySQL 总磁盘: 100GB; ClickHouse: 10TB; Redis: 16GB — 均硬编码为常量，与实际部署完全脱节 | 从环境变量读取 DB_STORAGE_CAPACITY 或调用实际 OS/DB API
  T: databaseMonitor.ts | P1 | Qdrant 向量维度硬编码为 768，每向量内存估算: points * 768 * 4，维度不匹配则容量计算完全错误 | 从 Collection info 中读取实际 vectors_config.size
  T: airflow.client.ts | P2 | getOverview() 中仅取前 10 个 DAG 的 RunStats (dags.slice(0, 10))，导致 overview 数据严重不完整 | 使用 listDAGRuns 的 state 参数批量查询，或调用 /dagRuns?state=running
  T: redis.client.ts | P2 | acquireLock() 在 client=null 时返回 memory-lock-{ts} 字符串，调用方可能以为获锁成功，实则无任何保护 | client=null 时应返回 null，并在文档中说明降级语义
  T: 共同问题 | 当前状态 | 建议方案
  T: 文件/模块 | 级别 | 问题描述 | 改进建议
  T: 全局 | P0 | 无 PluginRegistry，所有 client 直接 new + 单例导出，无法替换/Mock/热重载 | 设计 PluginRegistry<T> 接口，支持 register(name, factory) + resolve(name)
  T: grpcClients.ts | P1 | SERVICE_CONFIGS 硬编码设备服务和算法服务，无法动态注册新微服务端点 | 将 ServiceConfig 移入 DB/Config Store，支持运行时 CRUD
  T: healthChecker.ts | P1 | SERVICE_CONFIGS 数组硬编码 7 个服务，addServiceConfig 只能在运行时追加内存状态，重启丢失 | 持久化 service config 到 Redis 或 DB，并实现 removeServiceConfig
  T: kafkaEventBus.ts | P2 | matchTopic 通配符匹配是字符串操作，无法支持 NATS/AMQP 风格的复杂路由（如 header-based routing） | 抽象 TopicMatcher 接口，支持插件式路由策略
  T: 全局 | P2 | 无数据转换管道（Transform Pipeline），Kafka 消息体结构固定，无法在不修改核心代码的情况下添加数据预处理步骤 | 实现 TransformPlugin 接口和责任链模式的管道注册
  T: 文件/模块 | 级别 | 问题描述 | 改进建议
  T: databaseMonitor.ts | P1 | Qdrant 客户端在监控文件中初始化，混淆了监控职责与向量 DB 访问职责 | 独立 qdrant.client.ts，实现完整的集合管理、向量检索 API
  T: databaseMonitor.ts | P1 | Qdrant 版本硬编码为 '1.7.0'，实际版本从 API 中获取（/telemetry endpoint） | 调用 GET / 获取实际版本，或 GET /telemetry
  T: grpcClients.ts | P1 | AlgorithmServiceClient 所有参数类型为 any，彻底丢失类型安全，算法输入/输出契约不可验证 | 从 proto 生成强类型 TS 定义，使用 protoc-gen-ts 或 buf CLI
  T: 全局 | P0 | 无 Embedding Service 抽象，无法调用 Ollama/OpenAI/本地模型生成向量，Qdrant 中数据来源未知 | 实现 IEmbeddingProvider 接口，支持 Ollama/OpenAI/BGE
  T: 全局 | P0 | 无 RAG Pipeline，无法将传感器异常事件与知识库结合进行 AI 分析 | 实现 RAGPipeline: retrieve() → augment() → generate()
  T: 全局 | P1 | 无 LLM 调用层，healthChecker 中已配置 Ollama 端点但从未使用，资源浪费 | 实现 LLMClient: chat() / complete() / embed()，统一 Ollama 调用
  T: P0 | qdrant.client.ts | 向量集合 CRUD、upsert、search (cosine/dot/euclid)、payload 过滤 | @qdrant/js-client-rest 已安装
  T: P0 | llm.client.ts | LLMClient 接口 + Ollama 实现：chat / embed / stream | healthChecker 已有 Ollama endpoint
  T: P0 | embedding.service.ts | 文本/传感器数据 → 向量，支持批量 & 缓存 | llm.client + redis.client
  T: P1 | rag.pipeline.ts | retrieve(query) → augment(context) → generate(answer) | qdrant + llm + embedding
  T: P1 | algorithm.executor.ts | 在 Worker Thread 中安全执行用户自定义算法 | grpcClients / 本地执行双模式
  T: P2 | anomaly.detector.ts | 实时流式异常检测，结合 Kafka 消费 + AI 推理 | kafka + qdrant + llm
  T: 文件/模块 | 级别 | 问题描述 | 改进建议
  T: airflow.client.ts | P0 | AIRFLOW_CONFIG.password 默认值 'admin'，若未配置环境变量直接使用明文 admin/admin 连接生产 Airflow | 移除默认值，设 required: true，启动时检查并抛出
  T: clickhouse.client.ts | P0 | CLICKHOUSE_PASSWORD 默认值 'xilian123'，生产环境风险极高 | 同上，必须通过 Secret Manager 注入
  T: grpcClients.ts | P1 | 所有 gRPC 连接使用 createInsecure()，明文传输微服务间数据 | 生产环境应使用 createSsl() 或 mTLS
  T: elasticsearch.client.ts | P1 | Basic Auth 凭证在每次 HTTP 请求中重新 Buffer.from 计算，应缓存编码结果 | 提升为模块级常量
  T: redis.client.ts | P2 | keys(pattern) 方法暴露了对 KEYS 命令的直接访问，在大型生产 Redis 中 KEYS 是阻塞操作 | 限制为内部使用，提供 scan(pattern) 替代
  T: 文件/模块 | 级别 | 问题描述 | 改进建议
  T: systemMonitor.ts | P1 | calculateCpuUsage() 使用 os.cpus() 的累计时间快照计算使用率，这是瞬时快照，实际上是开机到现在的平均值，而非「当前」使用率 | 使用两次采样的差值 (t2-t1) / (total2-total1)，或信任 si.currentLoad()
  T: jaeger.client.ts | P1 | searchTraces limit=1000 拉取全量 trace 到内存做本地百分位计算，数据量大时内存占用高、延迟长 | Jaeger API 支持 lookback + service histogram，或在客户端侧分页
  T: airflow.client.ts | P1 | getOverview() 并发 listDAGs() + listPools() + listVariables() + listConnections() 后又串行遍历 dags.slice(0,10)，总请求数可达 60+，极其缓慢 | 添加 Redis 缓存，TTL 60s；或使用 Airflow 2.9+ 的 summary API
  T: databaseMonitor.ts | P2 | getAllDatabaseStatus() 并行 4 个 DB 的状态查询，任何一个 DB hang 会导致整体超时，无单独 timeout 控制 | 为每个查询设置独立 Promise.race + AbortController，超时返回 degraded 状态
  T: kafka.client.ts | P2 | subscribe() 每次调用都创建新 Consumer 实例，没有 Consumer 复用池，对同一 groupId 多次 subscribe 会创建冗余消费者 | 实现 ConsumerPool，同一 groupId 复用同一 Consumer 实例

================================================================================
## 平台服务与中间件层代码审查报告（第四轮）.docx
段落数: 62, 表格行数: 62
================================================================================
  P3: 综合评分：8.0 / 10
  P5: 发现：P0安全漏洞 2项 | P1功能Bug 4项 | P2架构改进 6项
  P6: 亮点：中间件层工程质量极高，2个文件达 10/10，7个文件达 9/10
  P11: 二、P0 安全漏洞（生产部署前必须修复）
  P12: 以下2项安全漏洞属于高危级别，必须在系统上线前修复。
  P13: 2.1 auth.service.ts — 认证服务形同虚设（全平台最高危）
  P20: 修复优先级：立即处理，与 S0-1 同步
  P21: 三、P1 功能 Bug（近期迭代修复）
  P22: 四、P2 架构改进建议
  P23: 4.1 连接器层质量严重不均
  P24: 5个连接器文件实现质量差异极大。redis.connector.ts（8/10）通过代理 ioredis 实现静默失败降级，而 mysql.connector.ts（仅17行）、clickhouse/nebula/qdrant.connector.ts 均为最简实现，缺乏连接池、认证配置和断路器集成。建议按照 redis.connector.ts 的水准补全其余连接器，并集成已有的 circuitBreakerIntegration.ts 保护。
  P25: 4.2 schema-registry.service.ts SQL 注入风险
  P29: 4.4 resource-discovery.service.ts ClickHouseTableScanner 硬编码
  P37: 5.1 circuitBreaker.ts（10/10）— 完整三态断路器
  P38: 5.2 auditLog.ts（10/10）— 工业级审计日志
  P39: 5.3 backpressure.ts（9/10）— 自适应背压控制
  P40: 5.4 decision-processor.ts（9/10）— Shannon熵驱动决策
  P41: 六、文件评分汇总
  P43: 第一优先级：本周内（安全漏洞修复）
  P44: S0-1修复：重写 auth.service.ts，实现真实JWT验证（jsonwebtoken）+ MySQL用户查询 + RBAC权限控制 + Redis token黑名单
  P45: S0-2修复：system.routes.ts 的 alertRules/scheduledTasks 所有mutation改为protectedProcedure或adminProcedure
  P48: P1-1修复：clickhouse.connector.ts 改用POST请求，SQL放请求体，添加认证支持
  P49: P1-2修复：nebula.connector.ts 添加响应校验和认证配置
  P50: P1-3修复：configCenter.ts rollback() 将历史持久化到MySQL，失败时返回明确错误
  P51: P1-4修复：dataFlowTracer.ts 支持 metadata.targetModule 覆盖，动态构建 topic→module 映射
  P60: 四轮审查核心发现：西联平台的基础设施中间件层（断路器、背压、优雅关闭、可观测性、密钥管理）已达到工业级水准。然而，认证服务（auth.service.ts 的 validateToken 永远返回 true）是全平台安全体系的致命缺陷——在该服务修复前，所有基于 protectedProcedure/adminProcedure 的访问控制均形同虚设。建议将认证服务重构作为最高优先级，修复后再进行生产部署或安全评审。
  --- TABLE ISSUES ---
  T: P0安全漏洞 | 2项（需在生产部署前修复）
  T: P1功能Bug | 4项（近期迭代修复）
  T: P2架构改进 | 6项（规划迭代处理）
  T: 最低分文件（3/10） | auth.service.ts（全平台最高危：永远验证成功）
  T: 核心风险 | auth.service.ts 的 validateToken 永远返回 valid:true，所有认证控制失效
  T: S0-1 | auth.service.ts | 极高危
validateToken() 方法只检查 token 是否非空，只要传入任意非空字符串即返回 { valid: true, userId: 'system' }。getUserPermissions() 对任意 userId 一律返回 ['read','write','admin'] 最高权限。整个认证服务是空壳：无 JWT 验证、无签名校验、无 token 黑名单、无用户查询。任何人用 token='abc' 即可获得系统管理员权限，绕过所有 protectedProcedure 和 adminProcedure 保护。
  T: S0-2 | system.routes.ts | 高危
alertRules 和 scheduledTasks 的所有 mutation 操作（create/update/delete/toggleActive/toggleStatus）均使用 publicProcedure，无认证保护。攻击者可无需登录即可创建/删除任意告警规则、修改告警阈值、创建/删除定时任务。auditLogs 查询接口也完全公开，暴露完整的操作审计记录（含操作人、IP等敏感信息）。
  T: 编号 | 位置 | 问题描述 | 修复方向
  T: P1-1 | clickhouse
.connector.ts | query() 使用 GET 请求将 SQL 附加到 URL，复杂 SQL 超过 URL 长度限制（约2083字符），SQL 语句在服务器日志明文可见，且无 Basic Auth | 改用 POST 请求，SQL 放入请求体（Content-Type: text/plain）；添加 CLICKHOUSE_USER / CLICKHOUSE_PASSWORD 环境变量支持
  T: P1-2 | nebula
.connector.ts | healthCheck() 执行 SHOW SPACES 但不校验返回结果，错误响应也可能返回 healthy。缺少认证配置 | 校验 r.json() 的错误字段；添加 NEBULA_USER / NEBULA_PASSWORD 配置；检查 HTTP 状态码
  T: P1-3 | configCenter.ts
rollback() | rollback() 从内存 history 数组查找 targetVersion（最多1000条），历史版本超出窗口时静默失败，仅 log.error 无返回错误 | 将 history 持久化到 MySQL 配置变更日志表；rollback() 失败时抛出明确错误（TRPCError）
  T: P1-4 | dataFlowTracer
.processEvent() | TOPIC_TARGET_MAP 静态硬编码 topic→target 映射，动态 topic 被归类为 'unknown' 目标，数据流图谱出现大量虚假 unknown 节点 | 优先使用事件 metadata.targetModule；从 ModuleRegistry 动态构建补充映射；unknown 节点按比例过滤
  T: auditLog.ts | 10/10 | 平台最佳实践
异步写入队列（不阻塞主请求）+ 批量入库（每批100条）+ 队列满时丢弃最旧10%防止OOM + 输入脱敏（password/secret/token/apiKey自动替换为***）+ 超10KB数据截断 + 敏感操作自动标记写入敏感日志表 + 高风险操作标记（high/medium/low三级）+ OTel traceId集成 + 优雅关闭（进程退出前刷空队列）+ 敏感日志通过traceId关联主日志。全平台工程质量最高的单文件。
  T: system.routes.ts | 5/10 | 告警规则/定时任务CRUD全部publicProcedure（P0）
  T: clickhouse.connector.ts | 4/10 | GET请求+URL拼接SQL，无认证（P1）
  T: nebula.connector.ts | 4/10 | 无认证，healthCheck校验不完整（P1）
  T: auth.service.ts | 3/10 | 极高危：永远验证成功，全部返回admin权限（P0）
  T: schema-registry.service.ts | 6/10 | 简洁MySQL Schema查询，有SQL注入风险
  T: 目标：消除P0安全漏洞，特别是修复认证服务这个全平台最高危缺陷
  T: 目标：修复功能Bug，提升连接器层可靠性
  T: 目标：提升连接器层一致性，修复潜在安全隐患
  T: 第一轮（核心基础设施层） | 综合评分 6.1/10 | 10个文件 | 5项P0漏洞
  T: 第二轮（平台服务与中间件层） | 综合评分 7.2/10 | 25个文件 | 5项P0漏洞
  T: 第三轮（API路由层） | 综合评分 7.2/10 | 33个文件 | 3项P0漏洞
  T: 第四轮（本轮：平台服务与中间件层） | 综合评分 8.0/10 | 25个文件 | 2项P0漏洞
  T: 四轮累计P0安全漏洞 | 15项高危安全漏洞
  T: 四轮累计P1 Bug | 21项功能Bug
  T: 四轮累计P2架构改进 | 28项架构优化建议
  T: 全平台最高危缺陷 | auth.service.ts（3/10，永远验证成功）
  T: 总体趋势 | 中间件层质量四轮最高；认证体系全平台最大风险；连接器层质量极度不均

================================================================================
## 前端组件数据服务层审查报告.docx
段落数: 61, 表格行数: 76
================================================================================
  P6: 共发现问题 15 项：P1 级 2 项（含 1 项安全漏洞、1 项设计缺陷）、P2 级 7 项、P3 级 6 项。同时记录了 6 项架构亮点。
  P42: qdrant.ts 封装了对 Qdrant 向量数据库的完整操作（集合管理、向量写入、相似搜索、点级操作）。已在批次 12 的 P1-C3 中识别为前端直连向量数据库的安全问题，本次重点关注该文件本身的实现质量。
  P44: ollama.ts 封装了 Ollama API 的全套操作（模型列表/拉取/删除/聊天/嵌入向量生成），实现质量高：流式响应正确使用 ReadableStream + TextDecoder 逐行解析，pullModel 的 onProgress 回调准确传递 completed/total 进度数据供 UI 展示进度条。AbortSignal.timeout(5000) 的状态检测超时是 ES2022 的现代写法。
  P58: 综合批次 16 发现，按以下顺序处理（前 3 步为安全/严重性能问题，应在本迭代完成）：
  P60: 批次 16 完成后，全项目共 15（批次1-15）+ 16（本批次）= 31 批 共约 155+ 文件完成审查。本批次未发现新的跨域性质问题，但以下跨批次 P1 问题仍处于开放状态，提醒团队优先处理：
  --- TABLE ISSUES ---
  T: 25
审查文件数 | 2
P1 高优先级 | 7
P2 中优先级 | 6
P3 低优先级
  T: [P2-PC1] P2 — handleCanvasMouseMove 内节点位置更新无节流，拖拽性能风险
📁 位置: PipelineCanvas.tsx → handleCanvasMouseMove
问题: 拖拽节点时每次 mousemove 事件（约 16ms 间隔）都直接调用 updateNodePosition()，触发 Zustand Store 更新和组件重渲染。多选整体拖动时每次事件触发 O(n) 次 updateNodePosition 调用。高节点数场景下会引发明显卡顿，且 pipelineEditorStore 中 updateNodePosition 没有内置节流。

  T: [P3-PC1] P3 — hasFittedView 仅依赖 nodes.length 变化，新增节点后无法再次 fit-to-view
📁 位置: PipelineCanvas.tsx → useEffect([editor.nodes.length, hasFittedView])
问题: 初始加载后设 hasFittedView=true，之后无论怎样变更节点数，useEffect 不再触发 fitToView。若用户希望通过"重置视图"按钮重新居中，需要手动重置 hasFittedView；但目前画布工具栏没有暴露 resetView 入口，用户无法手动触发 fitToView。
修
  T: [P2-PC2] P2 — password 类型字段在 localConfig 中以明文存储，validateNode 时暴露给 Store
📁 位置: PipelineConfigPanel.tsx → renderField("password")
问题: 密码字段渲染为 <Input type="password"> 保证了界面遮蔽，但 localConfig 字典内部仍是明文字符串。当用户点击"保存"后，updateNodeConfig(selectedNode.id, localConfig) 将包含密码字段的完整配置写入 pipelineEditorStore，而 Store 通
  T: [P2-KG1] P2 — handleMouseMove 内多选拖拽每次事件都枚举所有节点更新位置，无批量更新
📁 位置: KGCanvas.tsx → handleMouseMove → dragging 分支
问题: 多选拖拽时每次 mousemove 事件触发 O(selected) 次 updateNode() Store 操作（kgOrchestratorStore），每次均触发 Zustand 订阅通知，导致整个 KGCanvas 重渲染（含所有边的 SVG 路径重新计算）。知识图谱场景下节点数可能超过 100，多选拖动 10 个节点时每 16ms 有 10 次 Store 更新
  T: [P3-KG1] P3 — relationPicker 位置使用屏幕坐标，在缩放/平移后位置偏移
📁 位置: KGCanvas.tsx → setRelationPicker({ nodeId, x: e.clientX, y: e.clientY })
问题: relationPicker 的显示位置使用 e.clientX/e.clientY（屏幕坐标）计算后以 absolute 定位渲染，没有进行 canvas 坐标到视口坐标的转换。当用户在放大视图后尝试建立连线时，弹出菜单的位置会与目标节点发生视觉偏移。
修复: 使用已有的 screenToCanvas 函数将端口位置转换为 SVG
  T: [P3-KG2] P3 — KGConfigPanel 直接调用 updateNode 更新节点标签，缺少防抖，每次 keystroke 触发 Store 写入
📁 位置: KGConfigPanel.tsx → onChange={e => updateNode(nodeId, { label: e.target.value })}
问题: 节点名称输入框 onChange 直接绑定 updateNode，每次按键触发一次 Zustand 状态更新 + 画布节点重渲染。在中文输入场景下（IME 输入过程中 onChange 频繁触发），会出现拼音中间状态写入 Store、SVG 节点文字跳动
  T: [P3-L1] P3 — Sidebar 宽度（50/200px）在 MainLayout 和 Sidebar 中分别硬编码，改动需同步两处
📁 位置: MainLayout.tsx(ml-[50px]/ml-[200px]) 与 Sidebar.tsx(w-[50px]/w-[200px])
问题: 展开/折叠时的 Sidebar 宽度字面量分别出现在 MainLayout 的 ml-[50px]/ml-[200px] 和 Sidebar 的 w-[50px]/w-[200px]。若将来需要调整边栏宽度，必须同步修改两个文件的四处 Tailwind 类，否则产生视觉错位，属于 DRY 原则违
  T: [P2-S1] P2 — Sidebar 内直接调用 useAppStore.getState()，绕过 React 响应式系统
📁 位置: Sidebar.tsx → !subItem.children && currentPage === item.id && useAppStore.getState().currentSubPage === subItem.id
问题: 在 JSX 渲染表达式中直接使用 useAppStore.getState().currentSubPage 而非通过 useAppStore(s => s.currentSubPage) Hook 订阅。getStat
  T: [P3-L2] P3 — Logo 图片使用外部 CDN URL，部署断网环境下无法加载
📁 位置: Sidebar.tsx → <img src="https://files.manuscdn.com/...">
问题: Sidebar 品牌 Logo 通过外部 manuscdn.com CDN URL 加载，在网络受限的工业内网环境（无公网访问）中 Logo 会显示为空白，影响品牌一致性与页面完整性。
修复: 将 Logo 图片资源放入 client/public/logo.png，引用改为 src="/logo.png" 或 import logoUrl from '@/assets/l
  T: [P3-D1] P3 — generateSQL 函数与 ddl-generator.ts 中的 generateTableDDL 功能重复，两套实现可能产生差异
📁 位置: VisualDesigner.tsx → function generateSQL(...) 与 lib/ddl-generator.ts → generateTableDDL()
问题: VisualDesigner 内部定义了独立的 generateSQL 函数（约 20 行），lib/ddl-generator.ts 中有更完整的 generateTableDDL 函数（支持 DROP IF EXISTS、自动处理
  T: [P2-ER1] P2 — 拖拽节点时 dragStart 不更新导致累积偏差，多选拖拽 dx/dy 基于同一 dragStart 计算错误
📁 位置: ERDiagram.tsx → handleMouseMove → dragging 分支
问题: 拖拽时通过 dx = (e.clientX - dragStart.x) / zoom 计算每帧的位移增量，然后将增量累加到当前位置，同时通过 setDragStart({ x: e.clientX, y: e.clientY }) 更新基准点。这是"增量拖拽"的标准实现，单个节点拖拽逻辑正确；但多选拖拽时，对 selectedTableIds
  T: [P2-ER2] P2 — ER 位置写入 localStorage 未限制数据量，64 表 × JSON 可超 50KB，接近 5MB 配额风险
📁 位置: ERDiagram.tsx → localStorage.setItem(LS_KEY, JSON.stringify(positions))
问题: 每个 TableNodeState 存储 {id, x, y} 三个字段，64 张表约 3~5KB，单独存储没有问题。但 ERDiagram 与 appStore（已 persist 所有状态）、pipelineEditorStore（persist 画布节点）、kgOrchestrat
  T: [P3-D2] P3 — SqlEditor 执行 SQL 为本地模拟（setTimeout + Mock 结果），无真实后端对接
📁 位置: SqlEditor.tsx → handleExecute → setTimeout(..., 1000)
问题: 点击"执行"后触发 setExecuted(true)，通过 1 秒 setTimeout 模拟执行延迟，结果来自 schema.getMockRows("asset_nodes") 的静态数据。"SQL 优化建议"同样是 MOCK_OPTIMIZE 硬编码常量。用户无法区分 SQL 执行是否真实成功，有助于演示但影响生产可用性。
修复:
  T: [P2-SR1] P2 — diagnosis.ts / device-ops.ts 中表定义的 tableComment 字段大量为空字符串
📁 位置: data/fields/diagnosis.ts → tableComment: ""（diagnosis_rules 等 4 张表）
问题: 批次 15 已审查的 data/types.ts 中 TableRegistryEntry.tableComment 标注为必填字符串。diagnosis.ts 中 diagnosis_rules、diagnosis_tasks、anomaly_detections、device_rule_vers
  T: [P3-SR1] P3 — topology.ts 仅覆盖 5 张核心表（asset_nodes 等），64 张表中 59 张无拓扑映射
📁 位置: data/topology.ts → TOPO_MAP（仅含 asset_nodes, device_kpis, diagnosis_rules, event_logs, kb_documents）
问题: SystemTopology 页面通过 schema.getTopoMapping(tableName) 访问 TOPO_MAP，对未注册的表返回 undefined（fallback 到通用拓扑）。59 张表没有具体的数据流描述，对于想了解
  T: [P1-QD1] P1 — (重申 P1-C3) Qdrant 直连 URL 逻辑存在安全绕过风险
📁 位置: services/qdrant.ts → QDRANT_BASE_URL
问题: const QDRANT_BASE_URL = import.meta.env.VITE_QDRANT_URL || (hostname==='localhost' ? 'http://localhost:6333' : '/qdrant')；在生产环境（hostname≠localhost）走 nginx 代理路径 /qdrant，看似安全；但若 VITE_QDRANT_URL 被误配置（如开发者将 
  T: [P2-OL1] P2 — OLLAMA_BASE_URL 在生产环境（非 DEV）为空字符串，依赖相对路径 fetch 的隐式假设
📁 位置: services/ollama.ts → const OLLAMA_BASE_URL = import.meta.env.DEV ? 'http://localhost:11434' : ''
问题: 生产模式下 OLLAMA_BASE_URL 为空字符串，所有 fetch 调用使用相对路径（如 /api/tags）。这隐含假设：nginx 已配置 /api → ollama:11434 的代理。若 nginx 未配置或配置路径不同（如 /ollam
  T: [P3-OL1] P3 — streamGenerate 函数流式生成未处理 AbortError，用户取消时报错
📁 位置: services/ollama.ts → streamGenerate / chat（流式部分）
问题: 聊天流式读取使用 reader.read() 循环，但没有接受 AbortController signal 参数。若 UI 组件在用户取消生成或导航离开时需要终止流，只能等待流自然结束；如果页面快速切换时 reader 未 cancel，可能因 GC 延迟导致后台请求继续消耗带宽。
修复: 为 chat/streamGenerate 函数添加可选 signal?
  T: [P2-DR1] P2 — t-SNE 降维在主线程同步执行，iterations=500 时阻塞 UI 500ms+
📁 位置: services/dimensionReduction.ts → tsneReduce → for(let i=0; i<iterations; i++) tsne.next()
问题: t-SNE 的 500 次迭代是 CPU 密集型循环，在主线程同步执行时会完全阻塞 React 渲染。实测在 200 个 768 维向量（nomic-embed-text 维度）上，500 次 t-SNE 迭代约需 2-4 秒，期间用户界面完全无响应。对于知识库向量可视化这种探索
  T: [P3-DR1] P3 — UMAP 参数通过 (methodParams as any) 类型断言传递，绕过 TypeScript 检查
📁 位置: services/dimensionReduction.ts → reduceVectors → umapReduce(..., { neighbors: (methodParams as any).neighbors })
问题: reduceVectors 函数的 methodParams 类型为 Omit<ReductionParams, "method">，其中没有 neighbors 和 minDist 字段（这两个是 UMAP 专有
  T: [P3-DDL1] P3 — generateForeignKeys 生成的外键名存在哈希冲突风险
📁 位置: lib/ddl-generator.ts → fkName = `fk_${r.from}_${r.fromCol}`
问题: 外键名由 fk_来源表_来源列 构成。当一张表对同一目标表有多个 FK 引用时（如 asset_nodes 同时通过 code 和 parent_node_id 引用自身），所有 FK 名都是 fk_asset_nodes_code / fk_asset_nodes_parent_node_id，名称唯一。但当 fromCol 在不同表中恰好同名时（如 fk
  T: 编号 | 优先级 | 问题描述 | 位置 | 修复方向
  T: P1-QD1 | P1 [严重] | Qdrant 直连 URL 可绕过 nginx 认证层，凭据泄漏风险（重申 P1-C3） | services/qdrant.ts | 删除前端直连，统一通过 tRPC 代理
  T: P2-DR1 | P1 [严重] | t-SNE/UMAP 主线程同步计算 500 次迭代，阻塞 UI 2-4 秒 | services/dimensionReduction.ts | 移入 Web Worker 异步执行
  T: P2-PC1 | P2 [中等] | mousemove 节点位置更新无节流，多选整体拖动 O(n) Store 更新 | PipelineCanvas.tsx | RAF 节流 + batchUpdate
  T: P2-PC2 | P2 [中等] | password 类型字段配置写入 Store 并持久化到 localStorage | PipelineConfigPanel.tsx | 前端不持久化密码，改用 credentialId
  T: P2-KG1 | P2 [中等] | KGCanvas 多选拖拽每帧 O(n) 次 updateNode，高节点数卡顿 | KGCanvas.tsx | 增加 batchUpdateNodePositions Store Action
  T: P2-S1 | P2 [中等] | Sidebar 内 useAppStore.getState() 非响应式调用，子导航高亮不更新 | Sidebar.tsx | 改为 useAppStore 响应式订阅
  T: P2-ER1 | P2 [中等] | 多选拖拽 dx/dy 增量累加逻辑错误，非主节点位置飘移 | ERDiagram.tsx | 记录初始位置快照，改用总偏移量计算
  T: P2-ER2 | P2 [中等] | ER 位置写入 localStorage 未做容量防护，QuotaExceededError 风险 | ERDiagram.tsx | 捕获 QuotaExceededError 并 toast 提示
  T: P2-OL1 | P2 [中等] | Ollama 生产 base URL 为空字符串，隐式依赖 nginx 路径配置 | services/ollama.ts | 明确配置 VITE_OLLAMA_URL 并加文档说明
  T: P2-SR1 | P2 [中等] | diagnosis/device-ops 域所有表的 tableComment 为空字符串 | data/fields/diagnosis.ts, device-ops.ts | 补全表注释，CI 添加空注释检查
  T: 时序 | 问题编号 | 修复内容
  T: 第 1 步（立即） | P1-QD1 | 确认 qdrant.ts 前端直连彻底移除（延续批次 12 P1-C3），所有向量库操作通过 tRPC 代理
  T: 第 2 步（立即） | P2-DR1（重分类为 P1） | 将 tsneReduce/umapReduce 移入 Web Worker，避免主线程阻塞 2-4 秒
  T: 第 3 步（本迭代） | P2-PC2 | 删除 PipelineConfigPanel 中 password 字段写入 Store 的逻辑，改用 credentialId 引用
  T: 第 4 步（本迭代） | P2-S1 | 修复 Sidebar 中 useAppStore.getState() 非响应式调用，改为响应式订阅
  T: 第 5 步（本迭代） | P2-ER1 | 修复 ERDiagram 多选拖拽增量累加错误，改为初始位置快照 + 总偏移量计算
  T: 第 6 步（下迭代） | P2-PC1 + P2-KG1 | 为 PipelineCanvas 和 KGCanvas 的拖拽操作添加 RAF 节流和批量位置更新
  T: 来源批次 | 问题编号 | 问题描述 | 状态
  T: 批次 12 | P1-C3 | Qdrant 前端直连绕过认证层 | 未关闭（本批次 P1-QD1 重申）
  T: 批次 13-14 | P1-D1 | SKIP_AUTH 生产默认值未关闭 | 未关闭
  T: 批次 13-14 | P1-D2 | 明文密码硬编码于配置文件 | 未关闭
  T: 批次 15 | P1-A1 | 用户信息（含 roles）明文写入 localStorage | 未关闭
  T: 批次 15 | P1-V1 | EvolutionBoard 整页 Mock 数据 | 未关闭

================================================================================
## 服务端代码审查报告.docx
段落数: 63, 表格行数: 139
================================================================================
  P5: 总体而言，本轮文件整体工程质量高于前几轮平均水平，尤其是 shared/ 目录的类型定义（pipelineTypes.ts 节点系统 10/10、kgOrchestratorTypes.ts 知识图谱 9.5/10）和 drizzle/schema.ts 的数据库设计（覆盖 30+ 领域实体，9.0/10）体现出工业级设计水准。然而，存在若干高危安全隐患和架构欠债，必须优先处理。
  P6: 七轮评分总览
  P8: 三、P0 安全漏洞（本轮新增 4 项）
  P9: 以下 4 项为本轮审查发现的高危安全漏洞，必须在生产部署前修复。
  P10: 四、P1 功能 Bug（本轮新增 8 项）
  P11: 五、P2 架构改进建议（本轮 10 项）
  P16: 6.1 Pipeline 类型系统（10/10）— shared/pipelineTypes.ts
  P22: 6.2 知识图谱类型体系（9.5/10）— shared/kgOrchestratorTypes.ts
  P27: 6.3 算法微服务架构（8.0/10）— services/algorithm-service/src/server.ts
  P32: 6.4 数据库 Schema 设计（9.0/10）— drizzle/schema.ts
  P37: 6.5 TypeScript SDK 工程质量（8.5/10）— sdk/typescript/src/index.ts
  P43: 7.2 最高优先级修复清单（P0 × 20，按危险程度排序）
  P45: 第一优先级：本周内完成（所有 P0）
  P46: 第二优先级：本月内完成（P1 Bug）
  P47: 第三优先级：规划迭代完成（P2 架构）
  P49: 9.1 本轮审查评分详情
  P54: xlsx 0.18.5 — SheetJS CE 版本，有已知的 ReDoS 漏洞，如需处理不可信 Excel 文件须升级至 Pro 版或替换
  P57: 静态代码分析：通过原始文件内容逐行分析，识别安全漏洞、逻辑错误和架构问题
  --- TABLE ISSUES ---
  T: P0 安全漏洞 | 4 项（本轮新增）
  T: P1 功能 Bug | 8 项（本轮新增）
  T: P2 架构改进 | 10 项（本轮新增）
  T: 轮次 | 审查范围 | 文件数 | 评分 | P0 | P1 | P2
  T: 25 | .env.local.template | 环境变量模板 | ~80 行 | ⚠️ 含安全风险
  T: # | 级别 | 文件 | 漏洞描述 | 风险影响 | 修复方案
  T: P0-R7-01 | P0 | .env.local.template | SKIP_AUTH=true 默认启用，注释说明"本地开发时设为 true"，无 NODE_ENV 保护。若此环境变量泄漏或错误部署到生产，将绕过所有认证 | 任意用户可无需认证访问全平台 API，等同于无认证状态 | 禁止在模板中默认设为 true；生产部署检查脚本强制校验 SKIP_AUTH=false
  T: P0-R7-02 | P0 | server/business/services/telemetry.service.ts | getHistory() 使用字符串拼接构建 ClickHouse SQL：deviceCode 和时间参数直接嵌入查询字符串，无任何转义 | SQL 注入攻击，可读取或删除历史遥测数据 | 使用 ClickHouse 参数化查询：query({ query: "...WHERE device_code = {dc:String}", query_params: { dc } })
  T: P0-R7-03 | P0 | server/services/topology.service.ts（resetToDefault） | resetToDefault 使用 gte(field, 0) 条件删除全表数据，但此操作注册于 publicProcedure（无需认证），任何用户可触发 | 未授权全量删除拓扑节点、边和布局，破坏系统可视化状态 | 改为 protectedProcedure + 管理员角色校验；或添加 REQUIRE_CONFIRM 二次确认机制
  T: P0-R7-04 | P0 | services/algorithm-service/src/server.ts | gRPC 服务使用 ServerCredentials.createInsecure()，节点间通信明文传输，且无 mTLS 身份验证 | 内网攻击者可截获算法执行数据、伪造 gRPC 请求篡改执行历史 | 生产环境启用 TLS：loadTLSCredentials()；开发环境保留 Insecure 并用 NODE_ENV 保护
  T: # | 级别 | 文件 | 问题描述 | 影响范围 | 修复方案
  T: P1-R7-01 | P1 | server/types/mqtt.d.ts | MQTT 类型声明过于简化：subscribe 仅支持 string | string[]，缺少 QoS 选项；on() 仅声明 3 个事件，实际 mqtt 库有 reconnect、close、offline 等关键事件 | TypeScript 无法捕获连接断开/重连状态，断线处理盲区 | 完整声明 mqtt 库接口，或直接使用 mqtt 包的内置类型 import type { MqttClient } from "mqtt"
  T: P1-R7-02 | P1 | server/business/routes/monitoring.routes.ts | clickhouseDashboard.recentQueries 查询 auditLogs 表时使用 publicProcedure，审计日志（含敏感操作）对所有用户可见；且查询无时间范围限制 | 审计日志泄露，可能暴露其他用户操作记录 | 改为 protectedProcedure；添加用户过滤和时间范围限制
  T: P1-R7-03 | P1 | server/jobs/healthCheck.job.ts | startPeriodicHealthCheck() 在调用处无容错：若 checkAllServicesAndUpdateTopology 抛出未处理异常，setInterval 内的错误只 log.error 不重置定时器，但 for...of 循环单个服务失败会中断后续检查 | 一个服务检查失败导致后续服务状态停止更新 | 每个 checkServiceHealth 调用独立 try-catch；使用 Promise.allSettled 并发检查
  T: P1-R7-04 | P1 | services/device-service/src/server.ts | createDevice / createSensor 时 nodeId 生成使用 Date.now() + Math.random()，高并发下存在 ID 碰撞概率；同时 inserted 返回值未被使用（数据库层未 returning()） | 批量创建设备时可能出现 ID 冲突或使用错误 ID 返回 | 使用 nanoid/uuid v4 生成唯一 ID；确保 insert().returning() 正确获取自增 ID
  T: P1-R7-05 | P1 | services/algorithm-service/src/server.ts | executeAlgorithm 异步写执行历史：getDb().then(db => db.insert(...).catch(...))，但外层 callback(null, {...}) 已经返回；若写数据库失败仅打印日志，执行历史不完整 | 执行历史丢失无法追溯，影响基准测试统计准确性 | 改为先写历史记录再返回，或使用 fire-and-forget 时在失败时发送告警
  T: P1-R7-06 | P1 | sdk/typescript/src/index.ts | SDK 中 HttpClient.request 的重试逻辑缺陷：对 AbortError（超时）也会尝试重试，但 AbortSignal 在超时后不会自动重置，后续 fetch 会立即因 signal.aborted 抛出错误 | 网络超时场景下重试无效，3次重试会立即失败 | 每次重试时创建新的 AbortController：const controller = new AbortController()
  T: P1-R7-07 | P1 | server/services/topology.service.ts | getTopology / getTopologySnapshot 每次请求都调用 initializeDefaultTopology()，该函数执行 SELECT + 可能 INSERT，在高并发下造成竞态条件和不必要的数据库负载 | 每个前端轮询请求都触发初始化检查，性能损耗明显 | 用应用级 flag 标记已初始化；或使用 Redis 分布式锁保证单次初始化
  T: P1-R7-08 | P1 | drizzle/schema.ts | diagnosisRules.conditionExpr 字段类型为 text，但作为规则引擎表达式执行时无任何格式验证；anomalyDetections 中 threshold/currentValue 使用 int（×100存储浮点），但写入时无强制转换 | 无效的规则表达式写入后可能导致规则引擎崩溃；浮点值未×100直接写入导致精度错误 | 在 tRPC 层添加 conditionExpr 语法验证；threshold 系列字段改为 double 类型
  T: P2-R7-01 | P2 | shared/_core/errors.ts | HttpError 兼容层文件被标记 @deprecated，但仍有多处调用方引用 BadRequestError/NotFoundError 工厂函数。应进行全局迁移至 NexusError，同时删除此兼容层 | 消除双重错误体系，统一异常处理流程 | 中（需全局搜索替换）
  T: P2-R7-02 | P2 | shared/apiSpec.ts | RateLimitPresets 的限流配置（STANDARD=100/分钟）是全局共享配置，但不同端点（如批量操作 vs 读取）在实际路由层并未按此规范设置 rateLimiter，导致限流规范成为"纸面标准" | 实际限流生效，防止 API 滥用 | 中（需逐路由应用）
  T: P2-R7-03 | P2 | shared/pipelineTypes.ts | validateEditorState 和 topologicalSort 是纯函数，但位于共享类型文件中。随着函数增多将导致 types.ts 文件臃肿；建议拆分为 shared/pipelineUtils.ts | 职责分离，加快 TypeScript 编译速度 | 低（文件拆分）
  T: P2-R7-04 | P2 | services/device-service/src/server.ts | DeviceGroup 相关操作（createDeviceGroup/addDevicesToGroup）为空实现，直接返回占位数据。若前端依赖此功能会出现静默失败，建议实现或明确返回 UNIMPLEMENTED 错误码 | 防止前端依赖未实现功能，增强 API 诚实性 | 中（需实现功能）
  T: P2-R7-05 | P2 | server/jobs/healthCheck.job.ts | SYSTEM_SERVICES 数组硬编码了所有监控服务，无法在运行时动态增减。添加 addCustomService/removeCustomService 函数但非持久化（重启丢失）。建议持久化到 MySQL 的 topo_nodes 表 | 支持热插拔服务监控，无需重启配置新服务 | 中（需数据库支持）
  T: P2-R7-06 | P2 | server/services/topologyDiscovery.service.ts | TCP 探测使用 HTTP fetch 模拟：连接被拒时依据 ECONNREFUSED 判断端口是否存在，但其他非 HTTP 服务（如 MySQL 3306）的 fetch 会报 ECONNREFUSED 反而被判定为"不存在"，逻辑相反 | 修复误判，提高拓扑发现准确率 | 低（修复逻辑）
  T: P2-R7-07 | P2 | sdk/typescript/src/index.ts | SDK 缺少 WebSocket/流式数据支持（平台有大量实时传感器流），缺少 MQTT 订阅接口；当前仅提供 REST 风格 API，无法满足实时监控场景 | 支持实时数据接入场景，扩大 SDK 适用范围 | 高（架构扩展）
  T: P2-R7-08 | P2 | drizzle/schema.ts | schema.ts 超过 1500 行，包含 30+ 表定义，建议按领域拆分为 schema/users.ts、schema/knowledge.ts、schema/device.ts、schema/pipeline.ts 等，通过 index.ts 汇总导出 | 提升代码可维护性，加快 TypeScript 类型检查速度 | 低（文件拆分）
  T: P2-R7-09 | P2 | package.json | 生产依赖包含多个重量级工业协议库（node-opcua、modbus-serial、mqtt），但这些库只在特定微服务中使用。建议将工业协议相关依赖移至 services/device-service/package.json，减少主应用包体积 | 减少主应用启动时间，降低 Docker 镜像体积约 30% | 中（调整依赖结构）
  T: P2-R7-10 | P2 | server/routers.ts | appRouter 注册了 45+ 个子路由，全部在主文件同步 import。建议对非核心路由（如 grokDiagnosticRouter、fusionDiagnosisRouter）使用动态导入懒加载，减少冷启动时间 | 改善服务启动性能，减少内存峰值 | 低（动态导入改造）
  T: 问题类型 | 前六轮 | 第七轮（本轮） | 累计合计
  T: P0 安全漏洞（高危） | 16 | 4 | 20
  T: P1 功能 Bug（中危） | 24 | 8 | 32
  T: P2 架构改进（低风险） | 33 | 10 | 43
  T: # | 来源 | 文件 | 漏洞描述（摘要） | 状态
  T: 1 | R7 | auth.service.ts（R4） | validateToken() 永远返回 valid:true，任意 Token 绕过认证 | 🔴 未修复 - 最高危
  T: 2 | R7 | context.ts（R5） | SKIP_AUTH=true 硬编码 admin，无 NODE_ENV 限制 | 🔴 未修复
  T: 3 | R7 | .env.local.template（本轮） | SKIP_AUTH=true 默认启用，无生产防护 | 🔴 未修复
  T: 4 | R7 | sdk.ts（R5） | JWT 密钥硬编码 "nexus-secret" | 🔴 未修复
  T: 5 | R7 | system.routes.ts（R4） | 管理接口无认证保护 | 🔴 未修复
  T: 6 | R7 | telemetry.service.ts（本轮） | ClickHouse 查询 SQL 拼接注入 | 🔴 未修复
  T: 7 | R7 | topology.service.ts（本轮） | resetToDefault publicProcedure 可无认证删库 | 🔴 未修复
  T: 8 | R7 | algorithm-service server.ts（本轮） | gRPC 明文传输，无 TLS | 🔴 未修复
  T: 9 | R7 | mysql.connector.ts（R2） | SQL 拼接字符串注入风险 | 🔴 未修复
  T: 10 | R7 | grpcClients.ts（R1） | gRPC 通信无 TLS 加密 | 🔴 未修复
  T: 11 | R7 | kafka.client.ts（R1） | Kafka 连接无 SASL 认证 | 🔴 未修复
  T: 12 | R7 | docker.router.ts（R3） | Docker API 无权限验证 | 🔴 未修复
  T: 13 | R7 | plugin.router.ts（R3） | 插件路径无验证，路径穿越风险 | 🔴 未修复
  T: 14 | R7 | pipeline.router.ts（R3） | Pipeline 执行无沙箱隔离 | 🔴 未修复
  T: 15 | R7 | securityHeaders.ts（R2） | CSP 策略过于宽松 | 🟡 低优先级修复
  T: 16 | R7 | vaultIntegration.ts（R2） | Vault Token 硬编码 | 🔴 未修复
  T: 17 | R7 | auditLog.ts（R2） | 审计日志未加密存储 | 🟡 可接受风险
  T: 18 | R7 | redis.connector.ts（R2） | 单 Key 错误导致级联失败 | 🔴 未修复
  T: 19 | R7 | clickhouse.connector.ts（R2） | 批量写入无事务保证 | 🔴 未修复
  T: 20 | R7 | accessLayer.router.ts（R3） | 设备命令下发无审批流 | 🔴 未修复
  T: 1 | 修复认证绕过漏洞 | auth.service.ts 实现真实 JWT 验证；context.ts 添加 NODE_ENV 保护；删除硬编码密钥 | 安全团队 | 8h
  T: 2 | 修复 SQL 注入 | telemetry.service.ts 改为参数化查询；mysql.connector.ts 使用 ? 占位符 | 后端开发 | 4h
  T: 3 | 修复危险接口权限 | topology.service.ts resetToDefault 改 protectedProcedure；system.routes.ts 添加认证 | 后端开发 | 4h
  T: 4 | 修复 gRPC TLS | algorithm-service 和 grpcClients 添加 TLS credentials，生产环境使用证书 | 基础设施 | 6h
  T: 6 | 修复路径穿越风险 | plugin.router.ts 添加路径验证和白名单，禁止 ../ | 安全团队 | 2h
  T: 2 | 修复监控路由权限 | monitoring.routes.ts recentQueries 改为 protectedProcedure | 后端开发 | 1h
  T: 5 | SDK 重试逻辑修复 | sdk/index.ts 每次重试创建新 AbortController | SDK 团队 | 2h
  T: 7 | Schema 字段类型修复 | diagnosisRules conditionExpr 添加验证；threshold 改 double 类型 | 数据库团队 | 4h
  T: 1 | Schema 文件拆分 | 按领域拆分 schema.ts 为 6 个文件，通过 index.ts 汇总 | 数据库团队 | S1
  T: 2 | 依赖包优化 | 工业协议库移至 device-service 独立依赖 | DevOps | S1
  T: 3 | 路由懒加载 | appRouter 非核心路由改为动态导入 | 后端开发 | S2
  T: 4 | SDK 实时扩展 | 添加 WebSocket/MQTT 实时数据接口 | SDK 团队 | S2
  T: 5 | 限流规范落地 | 按 RateLimitPresets 规范为各路由配置限流中间件 | 后端开发 | S2
  T: 7 | TCP 探测修复 | topologyDiscovery 改用 net.createConnection 实现真实 TCP 探测 | 后端开发 | S3

================================================================================
## 前端审查报告.docx
段落数: 84, 表格行数: 40
================================================================================
  P4: 本次审查覆盖客户端前端层共 25 个文件，涵盖 Pipeline 可视化编辑器、KG 知识图谱编排器、数据库设计器三大交互模块，以及 Schema Registry 数据层、Qdrant / Ollama 服务层和工具库。审查共发现问题 18 项，其中 P1 级高优先级 5 项（含 1 项安全漏洞）、P2 级中优先级 8 项、P3 级低优先级 5 项。
  P6: 2  P1 高优先级问题（5 项）
  P8: P1-C1  Sidebar 在 JSX 中调用 getState() 破坏响应性
  P9: P1-C2  ERDiagram Schema 异步加载时位置初始化为空
  P10: P1-C3  [安全漏洞] 前端直接暴露 Qdrant 管理 API
  P11: P1-C4  t-SNE / UMAP 主线程同步执行冻结 UI
  P12: P1-C5  AbortSignal.timeout() 旧浏览器不支持
  P13: 3  P2 中优先级问题（8 项）
  P15: P2-C1  ThemeContext 主题持久化永远不生效
  P19: P2-C2  PipelineAPIPanel 展示错误的 REST API 路径
  P23: P2-C3  KGConfigPanel Unicode 转义字符串渲染为乱码
  P27: P2-C4  SqlEditor SQL 执行为纯 Mock，无后端调用
  P31: P2-C5  VisualDesigner 保存 / 执行按钮未实现
  P35: P2-C6  ERDiagram columns / fields 混用导致连线坐标偏移
  P39: P2-C7  K-Means++ 向量全零时产生重复聚类中心
  P43: P2-C8  PipelineCanvas / KGCanvas fitToView 逻辑重复
  P69: data/registry.ts + data/domains.ts + data/fields/*.ts 形成清晰的三层架构，64 张表 × 11 个域完整注册。VisualDesigner、ERDiagram、SqlEditor 三个组件统一通过 useTableSchema() Hook 消费，彻底消除 Mock 数据硬编码，是前端数据治理的优秀实践。
  P75: ddl-generator.ts 支持单表 / 按域 / 全库三种导出模式，包含 DROP TABLE IF EXISTS、外键约束、SET FOREIGN_KEY_CHECKS 开关，字段格式化正确处理 CURRENT_TIMESTAMP 特殊语法，除外键名重复问题（P3-C3）外质量达到生产可用标准。
  P79: P0-7 延伸确认（server/index.ts 未挂载 tRPC 路由）：
  P80: KGConfigPanel.tsx 已直接调用 trpc.accessLayer.listConnectors.useQuery() 和 trpc.accessLayer.listEndpoints.useQuery()，说明客户端 tRPC 配置存在且正常工作。但由于 server/index.ts 未挂载 tRPC 路由（P0-7），这两个查询在生产环境中必然 404 失败，KG 节点数据源绑定功能完全不可用。P0-7 的修复是解锁 KG 配置面板完整功能的前提条件。
  P81: P1-8 延伸确认（前端 tRPC 调用鉴权缺失）：
  P82: DataSourceBindingSection 直接消费 trpc.accessLayer.*，而 accessLayer.router.ts 中 procedure 若大量使用 publicProcedure（参见已确认的 P0-6 问题模式），则 KG 节点数据源绑定会绕过鉴权，任何未登录用户均可查看所有连接器配置。建议待 accessLayer.router.ts 审查后一并处理。
  --- TABLE ISSUES ---
  T: 25
审查文件数 | 5
P1 高优先级 | 8
P2 中优先级 | 5
P3 低优先级
  T: 文件：client/src/components/layout/Sidebar.tsx，第 78-80 行
问题描述
在 JSX 渲染逻辑中直接调用 useAppStore.getState() 而非使用 React Hook，导致子菜单激活态无法响应状态变更（不会触发重渲染）。用户在子页面刷新或从外部跳转后，侧边栏高亮不同步。
修复方案
在组件顶部解构 currentSubPage，JSX 中改用该响应式变量替代 getState() 调用。
  T: 文件：client/src/components/designer/ERDiagram.tsx，useState 初始化器
问题描述
useState lazy initializer 仅在挂载时执行一次。若 useTableSchema() 通过 tRPC query 异步加载，初始化时 schema.allTables 为空数组，所有表位置丢失，ER 图画布一片空白。
修复方案
改用 useEffect + setPositions 依赖 schema.allTables.length 变化时补充缺失位置，确保异步数据到达后位置能正确初始化。
  T: 文件：client/src/services/qdrant.ts，全文
问题描述
生产环境通过 nginx 代理 /qdrant 将 Qdrant 全部管理 API（含 DELETE /collections、PUT /collections）暴露到公网，任何用户无需鉴权即可删库。同时 VITE_OLLAMA_URL 硬编码到前端 bundle，Ollama 服务地址泄露。initializeDefaultKnowledge() 在前端执行 createCollection + addKnowledgePoints，完全绕过后端权限控制。
修复方案
将所有 Qdrant 操作移至后端 tRPC
  T: 文件：client/src/services/dimensionReduction.ts，tsneReduce 函数
问题描述
t-SNE 默认 500 次迭代运行在主线程，对于 500+ 向量点会造成数秒 UI 完全冻结，页面失去响应。UMAP 同理。
修复方案
使用 Web Worker 执行降维计算，通过 postMessage 回传进度，主线程显示 loading 状态。
  T: 文件：client/src/services/ollama.ts，checkOllamaStatus 函数
问题描述
AbortSignal.timeout() 仅在 Chrome 103+ / Firefox 100+ 可用（2022年引入）。工业现场旧版浏览器可能抛错，导致健康检查崩溃而非超时返回 false，影响 Ollama 状态显示。
修复方案
改用手动 AbortController + setTimeout 组合实现超时，兼容性覆盖所有主流浏览器。
  T: P1-C1 | Sidebar.tsx | P1 | 状态管理 | getState() 在 JSX 中使用，子菜单高亮不同步
  T: P1-C2 | ERDiagram.tsx | P1 | 异步加载 | Schema 异步时位置初始化为空，画布空白
  T: P1-C3 | qdrant.ts | P1 | 安全漏洞 | 前端暴露 Qdrant 管理 API，任意用户可删库
  T: P1-C4 | dimensionReduction.ts | P1 | 性能 | t-SNE 主线程同步执行，冻结 UI 数秒
  T: P1-C5 | ollama.ts | P1 | 兼容性 | AbortSignal.timeout 旧浏览器不支持
  T: P2-C1 | ThemeContext.tsx | P2 | 逻辑错误 | switchable=false 导致主题持久化永不生效
  T: P2-C2 | PipelineAPIPanel.tsx | P2 | 文档错误 | 展示不存在的 REST 路径，实际为 tRPC
  T: P2-C3 | KGConfigPanel.tsx | P2 | 渲染错误 | Unicode 字面量在 TSX 中渲染为乱码
  T: P2-C4 | SqlEditor.tsx | P2 | 功能缺失 | SQL 执行是 Mock，无后端调用
  T: P2-C5 | VisualDesigner.tsx | P2 | 功能缺失 | 保存/执行按钮未实现，仅弹 toast
  T: P2-C6 | ERDiagram.tsx | P2 | 数据一致性 | columns / fields 混用，连线坐标偏移
  T: P2-C7 | dimensionReduction.ts | P2 | 算法缺陷 | 全零向量时 K-Means++ 产生重复中心
  T: P2-C8 | PipelineCanvas/KGCanvas | P2 | 代码质量 | fitToView 逻辑重复未提取为 Hook
  T: 报告说明
本报告基于对 25 个前端源文件的静态分析生成，问题定级标准：P1=影响核心功能/安全，P2=影响功能完整性/体验，P3=数据质量/规范问题。所有问题均附有文件和行号定位，修复方案仅供参考。

================================================================================
## 认知引擎代码审查报告.docx
段落数: 225, 表格行数: 81
================================================================================
  P6: 综合评分
  P17: ✓ 亮点   TAS 综合评分（McNemar + DS 融合 + Monte Carlo 三维）架构科学，各组件耦合清晰。
  P18: 主要缺陷集中于以下三类，均为可快速修复的接口问题，不涉及核心算法：
  P19: ✗ 缺陷   【P0-严重】DimensionProcessor.process() 签名在 4 个处理器中不一致，无法通过 CognitionUnit 统一调用。
  P20: ✗ 缺陷   【P0-严重】shadow-evaluator.ts 引用了 TASCalculator 类，但 tas-calculator.ts 仅导出函数，类不存在。
  P21: ✗ 缺陷   【P1-中等】fusion-processor.ts 调用了 dsFusionEngine.fuse()，而引擎只有 fuseWithReliability()。
  P22: 1.2 综合评分
  P33: ✗ 缺陷   reasoning-processor.ts 第 149 行使用了 stimulus.type === "drift_detected"，但 StimulusType 中定义的是 "drift_alert"，存在拼写不一致。
  P43: 评分: 9/10  ★★★★★★★★★☆
  P50: 评分: 8/10  ★★★★★★★★☆☆
  P55: ✗ 缺陷   combineTwoByStrategy() 中 Murphy 策略退化为 dempster，注释说明此为设计决策（两两融合时Murphy退化）。但 fuseWithReliability() 逐步融合时也走 combineTwoByStrategy()，导致实际上无法在逐步融合中使用真正的 Murphy 策略。建议在 fuseWithReliability() 中检测 murphy 策略时改用批量 murphyCombination()。
  P63: 评分: 9/10  ★★★★★★★★★☆
  P67: 4.3.1 P0 — ShadowEvaluator 引用不存在的 TASCalculator 类
  P68: ✗ 缺陷   shadow-evaluator.ts 第 12 行：import { TASCalculator, type TASConfig } from "../engines/tas-calculator"，但 tas-calculator.ts 中未定义 TASCalculator 类，只有独立函数 mcNemarTest()、monteCarloRobustness()、calculateTAS()。构建必然失败。
  P80: 评分: 7/10  ★★★★★★★☆☆☆
  P84: ✗ 缺陷   identifyHighEntropyDimensions() 对嵌套对象中的单标量字段（单个数值）使用 Math.log2(1 + Math.abs(subValue)) 作为熵，这在数学上没有意义——单个确定值的熵应为 0，而不是其对数。
  P90: 评分: 8/10  ★★★★★★★★☆☆
  P95: 5.1 接口一致性 — P0 严重问题
  P102: ✗ 缺陷   四个处理器均未正确实现 DimensionProcessor 接口。CognitionUnit.executeDimensions() 无法以多态方式调用 processor.process(stimulus, context)，实际代码会在运行时报错或需要类型断言绕过。
  P117: 评分: 7/10  ★★★★★★★☆☆☆
  P125: 评分: 7/10  ★★★★★★★☆☆☆
  P129: ✗ 缺陷   第 149、157 行使用 stimulus.type === "drift_detected" 和 stimulus.type === "performance_degraded"，但 StimulusType 中定义的是 "drift_alert"，没有 "drift_detected" 和 "performance_degraded"。两个 if 分支永远不会触发。
  P133: 评分: 7/10  ★★★★★★★☆☆☆
  P137: 5.4.1 P1 — 调用不存在的方法 fuse()
  P138: ✗ 缺陷   process() 第 79 行：const dsFusionResult = this.dsFusionEngine.fuse(evidences)，但 DSFusionEngine 中没有 fuse() 方法，只有 fuseWithReliability(inputs: DSEvidenceInput[])。构建/运行时必然报错。
  P146: 评分: 6/10  ★★★★★★☆☆☆☆
  P148: ✗ 缺陷   DecisionProcessor.process() 签名为 (stimulus, degradationMode, perception?, reasoning?, fusion?)，不符合 DimensionProcessor 接口。
  P150: ✗ 缺陷   "retrain" 动作的 trigger 检查 fusion.data.dsFusionResult.decision === "degraded" || "faulty"，但 FusionProcessor 使用的 frameOfDiscernment 为 ["normal","degraded","faulty","unknown"]。然而 "deploy" 动作的 trigger 检查 stimulus.type === "training_completed"，此类型在 StimulusType 枚举中不存在，该触发器永远不会激活。
  P155: 评分: 7/10  ★★★★★★★☆☆☆
  P159: 6.1.1 P0 — TASCalculator 引用不存在
  P160: ✗ 缺陷   第 12 行 import { TASCalculator } from "../engines/tas-calculator"，该类不存在，构建失败。见 4.3.1 的修复方案。
  P162: ⚠ 警告   方法名为"DS 融合评分"但实现是手工计算置信度比值和改善率，并未调用 DSFusionEngine，实质上是一个简单的启发式评分。不影响功能，但命名具有误导性，建议重命名为 computeChallengersAdvantageScore()。
  P170: 评分: 7/10  ★★★★★★★☆☆☆
  P175: ✗ 缺陷   completeGate3() 中存在状态赋值逻辑问题：gate3Passed 分支先设 "gate3_passed" 后立即设 "promoted"；gate3Failed 分支先设 "gate3_failed" 后立即设 "rejected"。中间状态从未对外可观测，且若在两次赋值之间有并发读取，可能读到不一致的中间态。
  P184: 评分: 8/10  ★★★★★★★★☆☆
  P209: 8.1 P0 — 阻塞构建/运行（必须立即修复）
  P210: 8.2 P1 — 影响正确性（应在下个迭代修复）
  P211: 8.3 P2 — 代码质量改进（可规划优化）
  P214: xilian-platform 认知引擎的设计理念先进，理论基础扎实，架构层次清晰。DS 融合引擎、TAS 评分体系、Champion-Challenger 三阶段门控等核心组件的设计均体现出高水平的工业 AI 工程能力。
  P215: 主要风险集中于接口一致性：四个维度处理器与 CognitionUnit 的 DimensionProcessor 接口不匹配，以及 ShadowEvaluator 引用不存在的 TASCalculator 类，这三个 P0 问题会导致构建失败，需要立即修复。修复成本较低，预计 1-2 人天即可完成。
  P219: Sprint 1（当前迭代）：修复 P0-1/2/3，统一 DimensionProcessor 接口和 TASCalculator 类定义，确保构建通过。
  P220: Sprint 2：修复 P1 系列（枚举值对齐、状态机竞态、percentile 计算），完善错误路径处理。
  P222: Sprint 4：完成 P2 优化（代码可读性、魔数提取、接口命名），并对 CanaryController 进行端到端集成测试。
  --- TABLE ISSUES ---
  T: 整体工程规范性 | 7 | 工厂函数统一，模块边界清晰，但存在跨文件接口不一致问题
  T: FusionProcessor | fuse() 调用本身会抛出（方法不存在），外层 catch 可捕获，但会误报为处理失败 | ✗ 有缺陷
  T: ShadowEvaluator | TASCalculator 实例化失败会导致构造函数抛出，无降级 | ✗ 有缺陷
  T: 单元测试 | DSFusionEngine 三策略数学正确性（特别是极端冲突场景） | P0
  T: 单元测试 | CognitionStateMachine 所有合法/非法转换 | P0
  T: 单元测试 | TAS 计算：mcNemar(显著/不显著) × dsFusion × monteCarlo 的矩阵测试 | P0
  T: 集成测试 | CognitionUnit 完整流程（含超时、降级模式切换） | P1
  T: 集成测试 | Champion-Challenger 三阶段门控（Gate1失败/Gate2失败/完整成功） | P1
  T: 集成测试 | CanaryController 四阶段推进 + 自动回滚触发 | P1
  T: 属性测试 | DSFusionEngine：fusedMass 各值之和恒为 1.0（含 theta） | P1
  T: 编号 | 文件 | 问题 | 修复方向
  T: P0-1 | shadow-evaluator.ts | import TASCalculator 但类不存在 | 在 tas-calculator.ts 新增 TASCalculator 类包装现有函数
  T: P0-2 | all 4 processors | process() 签名不符合 DimensionProcessor 接口 | 将 degradationMode 等参数移入 DimensionContext
  T: P0-3 | fusion-processor.ts | 调用不存在的 dsFusionEngine.fuse() | 改为 fuseWithReliability()
  T: 编号 | 文件 | 问题 | 修复方向
  T: P1-1 | reasoning-processor.ts | "drift_detected"/"performance_degraded" 枚举值不存在 | 对齐 StimulusType 枚举定义
  T: P1-2 | decision-processor.ts | "training_completed" trigger 枚举值不存在，deploy 动作永不触发 | 对齐 StimulusType 枚举定义
  T: P1-3 | champion-challenger.ts | completeGate3 双重状态赋值竞态风险 | 合并为单次状态赋值
  T: P1-4 | champion-challenger.ts | executeGates 异常后 challenge 停留中间状态 | 在 catch 块设置 rejected 状态
  T: P1-5 | shadow-evaluator.ts | percentile P99 计算偏低 | 改用 Math.floor(p * n)
  T: P1-6 | tas-calculator.ts | Murphy 策略在逐步融合时退化为 Dempster | 批量 Murphy 路径单独处理
  T: 编号 | 文件 | 问题 | 修复方向
  T: P2-1 | stimulus-preprocessor.ts | 单标量字段熵估计不合理 | 单个确定值熵=0，无基线时跳过
  T: P2-2 | perception-processor.ts | 无基线时 entropy=0.5 常量失去区分度 | 无基线时返回空数组
  T: P2-3 | fusion-processor.ts | 证据质量函数中魔数过多 | 提取为命名常量或配置
  T: P2-4 | shadow-evaluator.ts | computeDSFusionScore 命名具有误导性 | 重命名为 computeChallengersAdvantageScore
  T: P2-5 | meta-learner.ts | 综合置信度计算公式无注释说明 | 添加数学推导注释
  T: P2-6 | champion-challenger.ts | challenges Map 无清理机制 | 添加定期清理终结记录的逻辑
  T: P2-7 | canary-controller.ts | onObservationComplete 使用闭包 session 可能读取过时状态 | 从 Map 重新获取 session
  T: P2-8 | types/index.ts | DSEvidenceSourceConfig 不变量未在注释中说明 | 添加类型注释

================================================================================
## 配置、文档.docx
段落数: 62, 表格行数: 88
================================================================================
  P5: 本次（批次 17，全项目终结批次）审查覆盖 20 个文件，横跨四个层面：工程配置（package.json、tsconfig.json、vite.config.ts、drizzle.config.ts）、基础设施（docker-compose.yml、.env.local.template）、项目文档（AUDIT_FINDINGS.md、DEPLOYMENT.md），以及 12 个前端页面（数据治理三件套、系统监控五件套、微服务仪表板、基础设置两件套、网关管理）。共发现问题 17 项：P1 级 4 项（均为安全/生产高危）、P2 级 8 项、P3 级 5 项，同时记录 5 项架构亮点。
  P15: drizzle.config.ts 极简正确：DATABASE_URL 缺失时立即抛出错误（fail-fast 设计），防止 drizzle-kit 以静默方式使用错误数据库。schema 路径和 out 目录配置标准，MySQL 方言正确。无需修复。
  P23: AUDIT_FINDINGS.md 是团队在本次外部审查之前的内部算法模块自查记录，覆盖了融合诊断引擎的 5 个问题。文档质量良好，问题描述精准，有代码行号定位。标注了"待审查"的三个模块（融合诊断路由、高级知识蒸馏、工况归一化），说明内部审查在批次划分前已启动但未完成。
  P25: DEPLOYMENT.md 是本批次文档质量最高的文件：结构完整（系统要求→快速开始→架构图→端口映射→环境变量→常用命令→故障排查→生产建议），ASCII 架构图直观，分层启动策略切实可用。故障排查章节覆盖了三个最常见问题，非常实用。
  P52: 11.1  总体质量评分
  P53: 11.2  未关闭 P1 问题（上线阻断项）
  P54: 以下 P1 级问题必须在生产上线前全部解决：
  P58: 立即（1 周内）：消除所有安全 P1 阻断项。包括 SKIP_AUTH 默认值改为 false、删除 qdrant.ts 前端直连、将用户角色移出 localStorage、Compose 密码改为强制变量、补全 DEPLOYMENT.md 安全检查清单。
  --- TABLE ISSUES ---
  T: 20
审查文件数 | 4
P1 高优先级 | 8
P2 中优先级 | 5
P3 低优先级
  T: [P2-PKG1] P2 — 核心依赖使用 ^ 语义版本，CI/CD 中 pnpm install 可能安装破坏性更新版本
📁 位置: package.json
问题: drizzle-orm(^0.44.5)、@trpc/server(^11.6.0)、kafkajs(^2.2.4)、express(^4.21.2) 等核心依赖均使用 ^ 前缀。在没有 .npmrc save-exact=true 的情况下，执行 pnpm install 时会安装最新次要版本，可能引入非向后兼容变更。drizzle-orm 历史上在次要版本中多次改动查询 API。
修复: 在 .npmrc 中添加 save
  T: [P3-PKG1] P3 — @saehrimnir/druidjs 是小众非主流库，无 TypeScript 类型声明，维护活跃度低
📁 位置: package.json -> @saehrimnir/druidjs: ^0.7.3
问题: 批次 16 已分析 dimensionReduction.ts 大量使用 druid.Matrix、druid.TSNE 等 API。该库在 npm 上最后一次更新超过 12 个月，TypeScript 类型声明缺失，在生产工业 AI 场景中存在长期维护风险。
修复: 评估替换为 ml-matrix + tfjs 或 @tensorflow/tfjs 的
  T: [P3-TS1] P3 — noUncheckedIndexedAccess 和 exactOptionalPropertyTypes 均设为 false，掩盖数组越界和可选属性类型错误
📁 位置: tsconfig.json -> noUncheckedIndexedAccess: false, exactOptionalPropertyTypes: false
问题: noUncheckedIndexedAccess=false 意味着 arr[0] 的类型为 T 而非 T | undefined，在大量数据处理代码（特别是 Schema Registry 字段访问、Kafka 消息解析）
  T: [P3-V1] P3 — allowedHosts 只包含 localhost/127.0.0.1，容器化开发或局域网调试时 Vite 开发服务器拒绝连接
📁 位置: vite.config.ts -> server.allowedHosts
问题: 当开发者在 Docker 内运行或通过局域网 IP 访问开发服务器时，Vite 会返回 403 Forbidden。工业平台常见场景：在测试机（不同 IP）上通过浏览器访问运行在开发机上的 Vite 开发服务器进行 UI 调试。
修复: 在 .env.local.template 中添加 VITE_ALLOWED_HOSTS= 配置项，vite.
  T: [P1-DC1] P1 — SKIP_AUTH 在 docker-compose.yml 中默认值为 true，生产环境镜像构建时若未覆盖将以无认证模式启动
📁 位置: docker-compose.yml -> app.environment.SKIP_AUTH=${SKIP_AUTH:-true}
问题: 这与批次 13-14 发现的 P1-D1 一致：SKIP_AUTH 的 fallback 默认值为 true，意味着在生产环境 .env 中若没有显式设置 SKIP_AUTH=false，所有 API 端点将无需认证即可访问。docker-compose.yml 作为基础设施即代码文件，
  T: [P1-DC2] P1 — 多个服务的密码使用弱默认值且以明文环境变量传递，Vault 集成仅作可选 profile
📁 位置: docker-compose.yml -> MySQL(portai123), ClickHouse(portai123), MinIO(portai123456), Neo4j(portai123)
问题: MySQL、ClickHouse、MinIO、Neo4j 的默认密码弱且一致。这些密码通过 environment 节点明文写入 Compose 文件，会被 git 历史记录。Vault 虽然在 docker-compose.yml 中有完整配置（securi
  T: [P2-DC1] P2 — Kafka 和 ClickHouse 未配置内存/CPU 资源限制，单服务资源耗尽可导致宿主机 OOM
📁 位置: docker-compose.yml -> kafka, clickhouse 服务缺少 deploy.resources.limits
问题: Kafka 在高写入负载下可能消耗大量内存；ClickHouse 注释中也提到"默认需要较多内存"。docker-compose.yml 中 MySQL（innodb-buffer-pool-size=256M）和 Redis（maxmemory 256mb）做了配置限制，但 Kafka 和 ClickHou
  T: [P2-DC2] P2 — Kafka 开放了宿主机 9092 端口，在生产网络下任何内网主机均可无认证连接 Kafka Broker
📁 位置: docker-compose.yml -> kafka.ports: 9092:9092
问题: Kafka 默认无认证，暴露 9092 端口意味着内网主机可以任意向 Kafka 写入消息或消费消息。在工业内网（OT 网络与 IT 网络混合）场景中，设备或其他系统可能意外或恶意地向平台消息队列注入数据，影响诊断结果和告警准确性。
修复: 生产环境中将 Kafka 端口绑定限制为 127.0.0.1：ports: "127.0.0.1:9092:90
  T: [P2-ENV1] P2 — 模板缺少 VITE_OLLAMA_URL 配置项，与批次 16 P2-OL1 发现一致
📁 位置: .env.local.template
问题: 批次 16 审查 services/ollama.ts 时已发现：生产环境中 VITE_OLLAMA_URL 为空时依赖隐式 nginx 路径假设。.env.local.template 中没有 VITE_OLLAMA_URL 配置项，开发者无法通过模板了解该配置的存在和用途。
修复: 在 .env.local.template 的 Ollama 配置段添加：VITE_OLLAMA_URL=http://localho
  T: [P3-ENV1] P3 — MinIO 前端相关 VITE_ 变量缺失，模板不完整
📁 位置: .env.local.template
问题: 模板中只有服务端 MinIO 配置，如果前端直接访问 MinIO（用于预签名 URL 文件上传/下载），则需要 VITE_MINIO_ENDPOINT。前端相关的配置项缺失。
修复: 梳理哪些 VITE_ 前缀的环境变量被前端代码使用（通过 import.meta.env.VITE_*），统一补充到模板，并标注每项的用途和默认值。
  T: [P3-DOC1] P3 — AUDIT_FINDINGS.md 中三个模块标注为"待审查"已超过当前审查批次完成时间，应更新状态
📁 位置: AUDIT_FINDINGS.md -> 二、三、四 均为"待审查"
问题: 融合诊断路由（批次 12-14 已审查）、高级知识蒸馏、工况归一化的内部审查状态未更新，与本次外部审查发现不一致，可能引起新加入团队成员的混淆。
修复: 将 AUDIT_FINDINGS.md 更新为"已由外部审查覆盖（批次 N）"，并链接到对应的审查报告；或将该文件归档为历史文档，在主 README 中指向本次完整审查报告系列。
  T: [P1-DOC1] P1 — DEPLOYMENT.md 的"生产部署"章节被截断且缺少安全加固检查清单
📁 位置: DEPLOYMENT.md -> 生产部署 章节（文件被截断）
问题: 生产部署章节只展示了"使用外部数据库、配置 SSL、设置强密码"三条开头，后续内容被截断。更严重的是，文档中没有"生产前安全检查清单"（如确认 SKIP_AUTH=false、修改所有默认密码、限制 Kafka 端口、启用 HTTPS）。这对于工业内网部署是严重缺失，可能导致运维人员跳过安全加固直接上线。
修复: 补全"生产部署"章节，增加"安全检查清单（上线前必须确认）"表格，逐项列出：SKIP_AUTH
  T: [P2-DOC1] P2 — DEPLOYMENT.md 架构图与端口表中混用 NebulaGraph 和 Neo4j，与 docker-compose.yml 不一致
📁 位置: DEPLOYMENT.md -> 服务架构图 / 端口映射表 vs docker-compose.yml
问题: DEPLOYMENT.md 的 ASCII 架构图中写的是 NebulaGraph（metad:9559、storaged:9779、graphd:9669/19669），端口映射表也有 NebulaGraph 9669/19669；但 docker-compose.yml 中是 Neo4j（neo4j
  T: [P2-DG1] P2 — 三个页面的所有表单状态使用 any 类型，丢失类型安全
📁 位置: CleanManager.tsx / SliceManager.tsx / ConfigManager.tsx -> useState<any>({})
问题: 表单状态类型为 any 意味着：1) 字段名拼写错误不会报 TypeScript 错误；2) ConfigManager 的 handleCreate 中 JSON.parse(form.segments) 没有类型保护，若输入非法 JSON 会直接抛出未捕获异常；3) 表单字段缺失不会被编译器检测到，只会在运行时被 tRPC server
  T: [P3-DG1] P3 — 三个页面的删除操作无确认对话框，误点"删除"按钮直接执行不可逆操作
📁 位置: CleanManager.tsx(deleteRule.mutate), SliceManager.tsx(deleteSlice.mutate), ConfigManager.tsx(deleteCR/deleteNT/deleteMPT)
问题: 清洗规则、数据切片、编码规则均为业务核心配置，删除后无法通过 UI 恢复。点击删除按钮直接调用 Mutation，没有确认步骤，在快速操作或误触时风险极高。
修复: 添加删除确认 Dialog（可复用项目中已有的 AlertDialog 组
  T: [P3-MON1] P3 — PerformanceOverview 中 StatCard 只展示 4 个，grid-cols-6 有 2 个空位，6 个模块中只有 Outbox 有具体指标
📁 位置: PerformanceOverview.tsx -> StatCard grid（cols-6 但只渲染 4 个）
问题: 页面布局是 grid-cols-6 但实际只有 4 个 StatCard，有 2 个空位，视觉上不均衡。更重要的是，用户无法在总览页一眼看到 Saga 死信队列数、去重命中率、采样调整次数等关键健康指标，必须点击进子页面才能查看。
修复: 将所有 6 个模块的最重要指标（
  T: [P2-MON1] P2 — retryAllFailed Mutation 没有数量限制，可能一次性重发数千条失败消息，导致 Kafka 消息风暴
📁 位置: OutboxManager.tsx -> retryMutation = trpc.outbox.retryAllFailed.useMutation()
问题: 当 Outbox 中有大量失败事件时（如下游服务宕机导致积压），一次性重试所有失败事件可能导致：1）Kafka 瞬间写入速率峰值冲垮下游消费者；2）若失败原因未解决，重试事件迅速再次失败并再次积压，形成"重试风暴"。当前 UI 按钮没有显示失败事件数量。
修复: 在"重试失
  T: [P2-MON2] P2 — MicroserviceDashboard 的 Prometheus 指标 Tab 将原始指标文本直接渲染为 pre 标签，无搜索/过滤，在高指标密度场景下不可用
📁 位置: MicroserviceDashboard.tsx -> PrometheusMetricsTab -> <pre>{metrics}</pre>
问题: Prometheus 指标文本格式（text exposition format）包含数百行 counter/gauge/histogram 数据，直接以 pre 标签展示时用户无法快速定位关心的指标。在生产环境中（OpenTelemet
  T: [P2-SET1] P2 — nodeId 由前端生成（mech-${code.replace(...).toLowerCase()}），存在不唯一和特殊字符处理不一致风险
📁 位置: MechanismManager.tsx -> const nodeId = `mech-${code.replace(/[^a-zA-Z0-9]/g, "").toLowerCase()}`
问题: 前端基于编码生成 nodeId，过滤了非字母数字字符。如果两个编码在过滤后结果相同（如 Mgj-001 和 Mgjx001 均变为 mgjx001），会产生 nodeId 冲突。数据库中 nodeId 若有唯一约
  T: [P2-SET2] P2 — PartsLibrary 在 L4 和 L5 Tab 切换时，createNode Mutation 的 onSuccess 调用 refetch() 时 refetch 依赖的是切换时的 currentLevel 快照，可能刷新错误层级数据
📁 位置: PartsLibrary.tsx -> const refetch = () => { if (currentLevel===4) refetchL4(); else refetchL5(); }
问题: 在 Mutation 的 onSuccess 回调中调用 refetch()，而 refetch 是基于 c
  T: [P1-GW1] P1 — GatewayManagement 使用全 Mock 数据，无法反映真实网关状态，运维人员可能误信展示数据做出错误判断
📁 位置: GatewayManagement.tsx -> const mockDashboard = {...}, mockRoutes, mockServices 等
问题: 页面当前展示的所有数据均为静态 Mock（mockDashboard.counts.routes=12、mockMetrics.totalRequests=1,284,567 等），不反映真实 Kong 网关状态。如果运维人员通过此页面监控网关健康状态，他们看到的是"一
  T: [P2-GW1] P2 — API Key 生成逻辑在前端 Math.random() 实现，不符合密码学安全要求
📁 位置: GatewayManagement.tsx -> const generateApiKey = () => Math.random().toString(36).substring(2,15)+...
问题: Math.random() 是伪随机数生成器，不具备密码学安全性（CSPRNG），生成的 API Key 理论上可预测。API Key 用于认证消费者身份，应使用 crypto.randomBytes() 或 Web Crypto API 生成。此外，即使 Ke
  T: [P3-GW1] P3 — GatewayManagement 内部多处使用内联 onClick 函数，缺少 useCallback 优化，频繁重渲染场景下有不必要的函数重建
📁 位置: GatewayManagement.tsx -> 多处 onClick={() => setXxx(item.id)} 内联函数
问题: 约 700 行的页面组件包含多个 Tab 和大量列表，每次状态更新（如 filteredRoutes 变化）都会重建所有内联 onClick 函数。这导致子组件 memo 失效并引发不必要的重渲染。
修复: 提取频繁调用的 handler 为 useCallback，特别是
  T: 编号 | 优先级 | 问题描述 | 位置 | 修复方向
  T: P1-DC1 | P1 严重 | SKIP_AUTH 默认值 true，生产镜像可无认证启动 | docker-compose.yml | 默认值改 false 并写入上线检查清单
  T: P1-DC2 | P1 严重 | 多服务弱密码明文写入 Compose 文件，Vault 为可选 | docker-compose.yml | 密码改为强制设置变量，生产使用 Vault 注入
  T: P1-DOC1 | P1 严重 | DEPLOYMENT.md 缺少生产安全检查清单，生产建议章节截断 | DEPLOYMENT.md | 补全章节，增加上线前安全核查表格
  T: P1-GW1 | P1 严重 | 网关管理页面全 Mock 数据，运维人员无法判断真实网关状态 | GatewayManagement.tsx | 添加"演示数据"醒目横幅，接入真实 Kong API
  T: P2-PKG1 | P2 中等 | 核心依赖使用 ^ 语义版本，CI 可能安装破坏性更新 | package.json | save-exact=true 或 --frozen-lockfile
  T: P2-DC1 | P2 中等 | Kafka/ClickHouse 无资源限制，可导致宿主机 OOM | docker-compose.yml | 添加 deploy.resources.limits
  T: P2-DC2 | P2 中等 | Kafka 9092 端口暴露，内网主机可无认证连接 | docker-compose.yml | 绑定到 127.0.0.1 或启用 SASL 认证
  T: P2-ENV1 | P2 中等 | 模板缺少 VITE_OLLAMA_URL 配置项 | .env.local.template | 补充 VITE_OLLAMA_URL 并加注释
  T: P2-DOC1 | P2 中等 | DEPLOYMENT.md 混用 NebulaGraph 和 Neo4j，文档与实现不一致 | DEPLOYMENT.md | 统一为 Neo4j，移除 NebulaGraph 引用
  T: P2-DG1 | P2 中等 | 数据治理三件套表单状态为 any 类型，丢失类型安全 | CleanManager/SliceManager/ConfigManager.tsx | 定义表单接口类型，JSON 解析加 try/catch
  T: P2-MON1 | P2 中等 | retryAllFailed 无数量限制，可能触发 Kafka 消息风暴 | OutboxManager.tsx | 增加 batchSize 参数和确认对话框
  T: P2-MON2 | P2 中等 | Prometheus 指标 Tab 原始文本渲染，无搜索过滤 | MicroserviceDashboard.tsx | 添加指标名称搜索框或分类展示
  T: P2-SET1 | P2 中等 | nodeId 前端生成，存在碰撞风险 | MechanismManager.tsx | nodeId 由后端生成（UUID v7/nanoid）
  T: P2-SET2 | P2 中等 | Tab 切换时 refetch 闭包可能刷新错误层级数据 | PartsLibrary.tsx | 在 mutate 调用时捕获 levelAtSubmit 快照
  T: P2-GW1 | P2 中等 | API Key 生成使用 Math.random()，不符合密码学安全 | GatewayManagement.tsx | 移到后端，使用 crypto.randomBytes(32)
  T: P3-TS1 | P3 低 | noUncheckedIndexedAccess=false 掩盖数组越界错误 | tsconfig.json | 阶段性开启，修复类型错误
  T: 安全性 | ⭐ 5/10 | P1 安全问题贯穿全项目：SKIP_AUTH 默认 true、明文密码、Qdrant 前端直连、用户角色明文 localStorage
  T: 性能 | ⭐ 7/10 | tRPC 查询缓存、Zustand 细粒度订阅、ClickHouse 优化；风险在 t-SNE 主线程阻塞和拖拽节流缺失
  T: # | 批次 | 问题 | 修复要求
  T: 5 | 批次 16（P2→P1） | t-SNE 主线程阻塞 2-4 秒 | 迁移到 Web Worker 后再上线
  T: 📊 全项目审查统计
本批次：20 个文件  |  17 项问题（P1: 4  P2: 8  P3: 5）  |  5 个架构亮点
累计全项目：175+ 个文件  |  17 个批次  |  150+ 个问题  |  未关闭 P1 安全阻断项：8 个
审查完成日期：2026-02-20  |  审查范围：后端服务 → tRPC 路由 → 算法引擎 → 前端核心 → 组件 → 服务 → 配置 → 文档 → 页面

================================================================================
## 算法库底层平台设计_v3_深化版.docx
段落数: 475, 表格行数: 124
================================================================================
  P305: // 推荐评分逻辑:
  P473: 10.1 P0 验收标准（深化）
  --- TABLE ISSUES ---
  T: failure_probability | 故障概率评估 | 多维特征 | 故障概率 + 风险等级 | 所有设备
  T: P0 | 4 张数据库表 + algorithm.registry.ts + algorithm.router.ts | 中 | 无
  T: P0 | algorithm.service.ts（CRUD + 桥接层） | 中 | P0 表
  T: P1 | 内置信号处理算法（FFT/小波/包络/滤波） | 大 | P0 服务
  T: P1 | 智能推荐引擎（recommend + autoConfig） | 中 | P0 + 设备注册中心
  T: P2 | 内置统计分析算法 | 中 | P0 服务
  T: P2 | 内置预测性维护算法 | 中 | P0 服务
  T: P2 | 算法组合编排 | 中 | P0 服务
  T: P3 | 基准测试 | 小 | P0 服务
  T: P3 | 内置优化算法 | 小 | P0 服务
  T: P0: 骨架 | Week 1-2 | 4 表 + Router + Service + Bridge | 能跑通一个算法 E2E
  T: P1: 信号+推荐 | Week 3-5 | 8 个信号处理 + recommend API | 设备选算法自动推荐
  T: P2: 统计+预测+组合 | Week 6-8 | 统计/预测算法 + Composition 编排 | 轴承诊断方案跑通

================================================================================
## 西联平台-全栈架构代码审查报告.docx
段落数: 58, 表格行数: 85
================================================================================
  P5: 1.1 整体评分
  P13: 设计正确：全部外部依赖（Airflow/Kafka Connect/Elasticsearch/Flink/Grok）均通过 Feature Flag 隔离，防止未部署服务阻断启动。requireFeature() 守卫函数和 lazyImportIf() 延迟导入机制设计完善。
  P32: 整体评价：Z-Score 滑动窗口异常检测实现正确，但存在多个阻断性问题。
  P51: 9.1 P0 —— 立即修复（当前系统无法工作）
  P52: 9.2 P1 —— 高优先级（影响功能完整性）
  P53: 9.3 P2 —— 中优先级（影响可靠性）
  --- TABLE ISSUES ---
  T: 审查结论 | 架构设计优秀，存在3个致命数据断点待修复
  T: ⚠️ 发现问题
【中】adminProcedure 缺少 requireUser 中间件 —— 仅校验 role 但未通过 requireUser 确保 user 不为 null，虽然逻辑等价但语义不完整
【低】auditLog 仅挂载在 protectedProcedure 和 adminProcedure，publicProcedure 的敏感查询无审计记录
  T: ⚠️ 发现问题
【低】emit() 在 listeners 遍历期间允许同步修改，存在 ConcurrentModificationException 风险（小概率）
【低】registry.clear() 不触发各子项的 unregister 事件，监听器无法收到粒度化清除通知
  T: ⚠️ 发现问题
【中】featureFlags.grok 的初始化逻辑存在二义性：process.env.XAI_API_KEY !== undefined 在 API Key 为空字符串时也返回 false，但 FEATURE_GROK_ENABLED=true 可能与空 API Key 同时存在
【低】requireFeature() 的错误消息中 feature.replace() 正则仅处理大写字母开头，无法正确生成 kafkaConnect → KAFKA_CONNECT 格式
  T: ⚠️ 发现问题
【中】CognitionUnit 设计为"一次性"但无防重复执行机制 —— executed 标志存在，但并发调用的 race condition 未处理（Promise.race 内部）
【低】crossValidate() 中"无异常但融合高冲突"分支重复递增 totalChecks 但逻辑上应为单次检查
【低】unitCounter 为模块级全局变量，服务重启后从0开始，ID可能重复（建议加时间戳盐）
  T: ⚠️ 发现问题
【中】murphyCombination() 在多个证据源时将平均证据与自身进行 (n-1) 次组合，但每次组合的"另一方"应为原始平均值，实现中用 result（已组合结果）作为一方存在误差
【低】frameOfDiscernment 默认值硬编码（bearing_damage/gear_wear 等），应从配置或数据库加载
  T: ⚠️ 发现问题
【高】三个 Provider 接口（KGHistoryProvider/OCTransferProvider/ShadowEvalProvider）仅定义了接口，无任何内置实现 —— MetaLearner 当前处于"空壳"状态，query() 总是返回空结果
【中】computeOverallConfidence() 使用 confidence² 作为权重，导致高置信度预测被过度放大（建议改为线性权重）
  T: ⚠️ 发现问题
【高】Worker线程池 fallback 逻辑存在潜在双重执行：executeInWorker() Worker失败时 fallback 到主线程，但 Worker 内部若已部分执行（副作用），会导致重复
【中】缓存键生成 hashObject() 使用 djb2 hash，存在碰撞风险 —— 不同输入可能生成相同缓存键（建议改为 SHA-256 或 content hash）
【低】getExecutionHistory() 每次调用都 spread 整个数组（[...this.executionHistory]），高频调用下内存压力大
  T: ⚠️ 发现问题
【中】amplitudeSpectrum() 使用 nextPow2 零填充后返回的 frequencies/amplitudes，旧版调用（grokDiagnosticAgent.service.ts）仅传入 signal 不传 sampleRate，导致 grok agent 中 DSP 调用会报错
【低】rmsVelocity() 假设输入为加速度信号（m/s²），但调用方传入的可能是原始 ADC 值（未单位换算），应加参数文档或单位校验
  T: 🔴 致命问题：Topic命名混乱
kafkaStream.processor.ts 订阅 KAFKA_TOPICS.TELEMETRY = "xilian.telemetry"（@deprecated）
真实数据应发布到 TELEMETRY_FEATURE = "telemetry.feature" 和 TELEMETRY_RAW = "telemetry.raw"
两个Topic不同，消费者永远收不到真实数据 —— 修复方法：将订阅改为 KAFKA_TOPICS.TELEMETRY_FEATURE
类似问题：ANOMALIES → ANOMALY_RESULTS，AGGREGATIONS →
  T: Topic类别 | 生产Topic | 废弃Topic | 风险
  T: 🔴 致命问题
订阅 KAFKA_TOPICS.TELEMETRY（废弃）而非 TELEMETRY_FEATURE —— 永远收不到数据
异常检测结果写入 anomalyDetections（MySQL），同时也有 ClickHouse 的 anomaly_detections 表 —— 双写冲突，前端查哪个？
聚合结果写入 event_store（MySQL），而非 ClickHouse 时序库 —— 无法支持高频时序查询
窗口数据保存在内存 Map，服务重启全部丢失，无持久化恢复机制
  T: ⚠️ 设计问题
【中】aggregateVersionCounter 单调递增但无隔离，多个 sensor 共享同一计数器，aggregateVersion 语义混乱
【中】queryAggregations() 的 deviceCode 过滤在内存层执行（filter），而非 SQL WHERE，存在大量数据时的性能问题
  T: 🔴 致命问题
insertSensorReadings() 向 sensor_readings 表写入，但该 ClickHouse 表未初始化（无建表脚本）
insertTelemetryData() 向 telemetry_data 表写入，同上问题
queryAggregatedData() 查询 sensor_readings_1m/1h/1d，这些物化视图也未初始化
结论：ClickHouse 层的读写调用均会报"Table does not exist"错误
  T: ⚠️ 设计问题
【中】clickhouse.client.ts 中的字段名为 device_id（snake_case），而系统其他地方用 deviceCode —— 插入时需要转换，存在遗漏风险
【低】getClickHouseClient() 不是懒加载，模块 import 时立即创建连接，若 ClickHouse 未启动会在启动时报错（但被 try-catch 隐藏）
  T: ⚠️ 发现问题
【高】dataflowManager 引用了 flinkProcessor（anomalyDetector/metricsAggregator/kgBuilder）和 kafkaCluster，这两个模块在本次审查中未提供，无法评估其实现质量
【中】DataflowManager.start() 顺序启动4个处理器，若第2个失败，第1个已启动但不会被回滚
【低】getMetrics() 返回 { ...this.metrics } 浅拷贝，若 metrics 中有嵌套对象，调用方可能修改内部状态
  T: 🔴 安全风险
【高】sessions Map 无大小限制 —— 并发大量请求时内存泄漏（建议最大会话数 + LRU淘汰）
【高】executeToolCall() 中 getDb() 返回的是 Promise<DB> 但代码当作同步使用（const db = getDb()），所有工具调用会静默失败并返回"数据库未连接"
  T: ⚠️ 设计问题
【中】parseDiagnosticOutput() 依赖正则从自然语言提取结构化字段，LLM输出格式不稳定，建议改为 JSON mode（structured output）
【中】get_sensor_history Tool 从 assetSensors.lastReadingAt 过滤，但 lastReadingAt 是传感器的最后读数时间而非历史数据时间戳，过滤逻辑有误
【低】getQueryEmbedding() Ollama不可用时返回随机向量并写入日志警告，生产环境应完全禁用而非降级到随机向量
【低】DIAGNOSTIC_TOOLS 工具定义为模块级常量，所有 
  T: ⚠️ 发现问题
【高】ensureAccessLayerTables() 与 drizzle/schema.ts 中的 Drizzle 表定义并行存在 —— 两个建表来源可能产生字段不一致，应以 drizzle migrate 为权威源
【中】createEndpointsBatch() 中每个 endpoint 的 endpointId 使用 Date.now() + random，高并发批量插入时可能重复
【中】batchHealthCheck() 对所有 Connector 串行健康检查，若有慢连接可能阻塞数分钟
【低】seedDemoData() 中应力通道元数据硬编码，若设备编号变更
  T: ⚠️ 发现问题
【高】所有 Gateway 操作使用 publicProcedure（无鉴权），Kong Admin API 为敏感操作（创建路由/删除服务），应改为 adminProcedure
【中】kongRequest() 无重试机制，Kong Admin API 瞬时不可用时直接返回错误
【低】getDashboard() 并行6个请求，若其中一个超时全部等待，建议加 Promise.allSettled + 超时
  T: ⚠️ 发现问题
【高】pluginStorage 使用内存 Map，服务重启后插件数据全部丢失；应持久化到 Redis 或数据库
【中】installPlugin() 依赖检查仅验证 depId 是否在 Map 中，不检查版本兼容性（dependencies 中的版本字符串被忽略）
【中】PluginEngine 继承 EventEmitter 但 setMaxListeners 未设置，大量插件注册时会触发 "MaxListenersExceededWarning"
【低】builtinPlugins 自动安装后状态为 installed 而非 enabled，外部调用 executePlu
  T: 编号 | 文件 | 问题 | 修复方案 | 工时
  T: P0-1 | kafkaStream.processor.ts | 订阅废弃Topic，收不到数据 | 改订阅 KAFKA_TOPICS.TELEMETRY_FEATURE | 0.5天
  T: P0-2 | clickhouse.client.ts | ClickHouse表未初始化 | 执行ClickHouse建表脚本 | 1天
  T: P0-3 | grokDiagnosticAgent.service.ts | getDb()同步调用，工具全部失败 | 改为 await getDb() | 0.5天
  T: P0-4 | kafkaStream.processor.ts | 聚合数据写MySQL而非ClickHouse | 聚合结果写入ClickHouse | 2天
  T: P0-5 | telemetry.db.service.ts | 振动聚合数据写MySQL | 迁移到ClickHouse时序表 | 2天
  T: 编号 | 文件 | 问题 | 修复方案 | 工时
  T: P1-1 | gateway.service.ts | Admin操作无鉴权 | 改为adminProcedure | 0.5天
  T: P1-2 | meta-learner.ts | 三个Provider无实现 | 实现KGHistoryProvider | 3天
  T: P1-3 | plugin.engine.ts | pluginStorage无持久化 | 对接Redis | 1天
  T: P1-4 | grokDiagnosticAgent.service.ts | sessions无大小限制 | 加LRU限制（max 1000） | 0.5天
  T: P1-5 | kafka-topics.const.ts | 生产/废弃Topic混用 | 废弃Topic全部移除或只保留映射 | 1天
  T: 编号 | 问题 | 影响 | 建议方案
  T: P2-1 | anomalyDetections双写（MySQL+ClickHouse） | 前端查询结果不一致 | 确定权威数据源，删除另一个
  T: P2-2 | 缓存键碰撞风险（djb2 hash） | 不同输入返回相同缓存结果 | 改用 SHA-256 或 content hash
  T: P2-3 | murphyCombination逻辑误差 | 多证据融合结果偏差 | 修正平均证据的组合方式
  T: P2-4 | parseDiagnosticOutput正则提取不稳定 | AI诊断结构化结果错误 | 改为structured output（JSON mode）
  T: P2-5 | access-layer两个建表来源 | 表结构可能不一致 | ensureAccessLayerTables移除，完全依赖drizzle migrate
  T: ❌ 当前不可用
算法层无真实数据输入（Kafka Topic错误）
ClickHouse表未初始化（查询全部报错）
Grok Agent工具调用静默失败
MetaLearner空壳（无Provider实现） | ✅ P0修复后可用
边缘数据接入管道（已开发filename.parser）
认知推理系统（待数据输入后激活）
算法引擎（待ClickHouse表初始化）
AI Grok诊断（待getDb修复）

================================================================================
## 西联平台系统性修复路径与行动方案.docx
段落数: 195, 表格行数: 60
================================================================================
  P5: 整体架构评分矩阵
  P6: 基于10分制对系统各模块进⾏的综合评分， 反映了当前系统的成熟度与健康状况。
  P8: 系统当前积累的技术债务严重程度分级如下：
  P11: 第⼆章： P0级问题详细清单与修复路径
  P12: 安全漏洞（严重级）
  P13: 以下20项漏洞被定义为P0级， 直接威胁系统安全或导致核⼼功能失效。
  P14: P0级修复优先级矩阵
  P31: 第四章： P1级功能缺陷修复计划
  P33: MQTT类型声明不⾜（mqtt.d.ts） ： 缺少 QoS 选项和关键事件（offline, reconnect） ， 导致连接稳定性难以维护。
  P37: SDK重试逻辑缺陷（sdk/typescript） ：中⽌。
  P43: 健康检查Job容错缺陷（healthCheck.job.ts） ： 单个服务检查失败抛出异常， 会导致后续所有服务状态停⽌更新。
  P52: 第五章： P2级架构债务清理
  P89: □ 🚨 数据管道三⼤断点修复（P0-15/16/17）
  P90: 🚨 移除 SKIP_AUTH 硬编码 admin 权限（P0-1）
  P91: 🚨 修复 SQL 注⼊漏洞（P0-10）
  P97: 🔒 修复所有⾝份验证绕过漏洞（P0-1/2/3/4）
  P98: 🔒API 路由权限整改（P0-5/6/7/8/9）
  P99: 🔒 启⽤ gRPC TLS 加密（P0-12）
  P100: 🔒 配置 Kafka SASL 认证（P0-13）
  P102: 🔒 插件路径穿越防护（P0-8）
  P105: ⚙ 修复 30+ P1 级功能缺陷
  P106: ⚙ DimensionProcessor 接⼝统⼀（P1-11）
  P107: ⚙ ⾼并发 ID ⽣成策略（P1-2）
  P108: ⚙ 监控指标准确性修复（P1-9）
  P109: ⚙ SDK 重试逻辑修复（P1-4）
  P110: ⚙ Schema 验证机制（P1-6）
  P113: 🏗 Schema 管理统⼀（P2）
  P114: 🏗 配置体系合并（P2）
  P115: 🏗 基础设施层重构（P2）
  P116: 🏗AI 能⼒解耦与完善（P2）
  P117: 🏗 插件引擎设计（P2）
  P118: 🏗 代码库领域拆分（P2）
  P150: 每⽇站会： P0级问题进度同步， 阻碍点快速清除。
  P151: 每周评审： P1/P2级问题优先级调整， ⻛险重新评估。
  P154: P0级验收标准
  P163: 安全测试： 运⾏ OWASP Top 10 扫描⼯具， 确保⽆⾼危漏洞。
  P189: （此处应提供关键漏洞的标准测试⽤例， 详⻅QA测试⽤例库）
  --- TABLE ISSUES ---
  T: 认知引擎 | 7.4/10 | 良好。 状态机设计精妙， 但存在接⼝签名不⼀致的架构问题。
  T: API路由层 | 7.2/10 | 及格。 功能实现完整， 但核⼼管理接⼝缺乏鉴权， 存在⾼危漏洞。
  T: AI集成能⼒ | 4.5/10 | 薄弱。 向量库耦合严重， 缺乏Embedding抽象， RAG链路未打通。
  T: ID | 涉及⽂件 | 问题描述 | ⻛险等级 | 修复⽅案 | 负责团队
  T: ID | 涉及⽂件 | 问题描述 | ⻛险等级 | 修复⽅案 | 负责团队
  T: 9 | topology.service.ts | resetToDefault存在越权数据删除漏洞 | ⾼ | 增加⼆次确认及管理员权限校验 | 后端开发
  T: 数据管道断点（三⼤致命问题） | 数据管道断点（三⼤致命问题） | 数据管道断点（三⼤致命问题） | 数据管道断点（三⼤致命问题） | 数据管道断点（三⼤致命问题） | 数据管道断点（三⼤致命问题）
  T: ID | 涉及⽂件 | 问题描述 | ⻛险等级 | 修复⽅案 | 负责团队
  T: 修复顺序 | 漏洞ID | 依赖关系 | 修复时间窗⼝ | ⻛险维度
  T: 3 | 10, 5, 9 | 修复顺序2 | 48⼩时内 | 安全⻛险（数据破坏）
  T: 5 | 6, 7, 8 | 修复顺序2 | 1周内 | 安全⻛险（API滥⽤）
  T: 6 | 12, 13 | 修复顺序1 | 1周内 | 安全⻛险（通信窃听）
  T: 安全团队 | 平台安全加固 | P0 安全漏洞（1-14） ， 凭据管理
  T: 数据团队 | 数据管道恢复 | 数据管道断点（15-17） + 数据质量（P1-1~6）
  T: 后端团队 | 业务逻辑修复 | API 路由整改+ 基础设施重构 + 功能缺陷
  T: 算法团队 | 算法集成 | 认知引擎接⼝统⼀+ DS 融合修复

================================================================================
## 核心层代码审查报告（第五轮）.docx
段落数: 107, 表格行数: 65
================================================================================
  P3: 五轮累计：168 个文件 | 15 项 P0 安全漏洞 | 21 项 P1 Bug | 28 项 P2 架构改进
  P6: 核心层整体工程质量为本平台五轮审查最高分，呈现出工业级平台的设计水准。亮点包括：BaseRegistry 泛型注册中心（10/10）、统一错误体系（9/10）、算法注册中心（48 个内置工业算法）、模块注册中心（28 个模块 Manifest 精准标注完整度）。发现 2 项关键安全问题：SKIP_AUTH 环境变量机制过于宽泛（可允许任意非空字符串触发管理员会话）、config.ts 与 env.ts 两套配置体系并行导致管理混乱。
  P7: 2. 文件清单与评分
  P8: 3. P0 安全漏洞
  P9: S0-1：SKIP_AUTH 机制过于宽泛（context.ts / sdk.ts）
  P10: 严重级别：HIGH（生产环境若泄露环境变量可获得管理员权限）
  P11: 问题描述
  P12: context.ts 中，当 process.env.SKIP_AUTH === 'true' 时，直接返回硬编码的 LOCAL_DEV_USER（role: 'admin'）。sdk.ts 中，authenticateRequest() 在 SKIP_AUTH 为 true 时从 DB 返回或新建一个 role='admin' 的 local-dev-user。
  P15: containerized 部署中若 docker-compose.yml 硬编码 SKIP_AUTH=true 并被推送到生产，整个平台完全无防护
  P16: 与第四轮发现的 auth.service.ts 永远验证成功漏洞叠加，双重无防护
  P24: 4. P1 功能 Bug
  P25: P1-1：config.ts 与 env.ts 两套并行配置体系
  P31: P1-2：cookies.ts 的 domain 逻辑被注释
  P36: P1-3：llm.ts 的 LLM 模型和 thinking budget 硬编码
  P42: 5. P2 架构改进
  P44: index.ts 中部分动态 import 在 .catch() 中只打印日志不阻断启动（如 DataFlowTracer），而 initPerformanceServices 内部每个服务独立 try/catch 吞掉错误继续启动。理论上某些关键服务（如 Outbox Publisher、Saga Orchestrator）失败应阻断启动。建议区分「关键服务」（启动失败应报错）和「可选服务」（启动失败仅记录警告）。
  P45: A2-2：voiceTranscription.ts 缺少 tRPC Router 注册
  P51: A2-5：featureFlags.ts 与 moduleFeatureFlags.ts 初始化时机问题
  P52: 两个文件在模块 import 时立即读取 process.env，这意味着 dotenv/config 必须在这两个文件被 import 之前执行。index.ts 第一行是 import 'dotenv/config'，顺序正确，但如果其他文件（如测试或工具脚本）直接 import 这两个 flag 文件，环境变量可能尚未加载。建议改为惰性初始化（提供 initialize() 方法）或添加注释说明必须先加载 dotenv。
  P56: 6.1 BaseRegistry 泛型注册中心（10/10）— 架构教科书
  P65: 6.2 ModuleRegistry 平台完整度看板（10/10）— 透明度工程
  P73: 6.3 统一错误体系（10/10）— 错误处理典范
  P81: 6.4 算法注册中心 48 个工业算法（10/10）— 领域知识沉淀
  P88: 6.5 StubTracker 桩追踪器（10/10）— 技术债透明化
  P95: 7. 文件评分汇总
  P97: 第一优先级：本周内（安全漏洞）
  P102: 16 项 P0 安全漏洞（其中最高危：auth.service.ts 永远验证成功 + 本轮 SKIP_AUTH 机制）
  P103: 25 项 P1 功能 Bug（包括 ClickHouse GET 请求 SQL 注入、ConfigCenter rollback 静默失败等）
  P104: 34 项 P2 架构改进（包括 connectors 层质量不均、schema-registry SQL 注入、K8s 服务发现硬编码等）
  P106: 最高优先级待修复项：auth.service.ts（P0，全平台最高危，第四轮发现，任意非空 token 获得管理员权限）在五轮审查结束后仍是首要安全任务。
  --- TABLE ISSUES ---
  T: P0 安全漏洞 | 1 项（SKIP_AUTH 机制过于宽泛）
  T: P1 功能 Bug | 3 项
  T: P2 架构改进 | 6 项
  T: 文件 | 评分 | 核心亮点 / 主要问题
  T: 7/10（2 个） | context.ts（SKIP_AUTH 风险）, cookies.ts（domain 逻辑注释）
  T: S0-1 | context.ts / sdk.ts | 生产环境强制禁用 SKIP_AUTH；LOCAL_DEV_USER 角色降为 'user' | 0.5d
  T: P1-1 | config.ts + env.ts | 合并两套配置体系，env.ts 改为 config.ts 的别名层 | 1d
  T: P1-2 | cookies.ts | 恢复 domain 逻辑；sameSite=none 必须要求 secure=true | 0.5d
  T: P1-3 | llm.ts | model/max_tokens/thinking_budget 支持 InvokeParams 覆盖和环境变量配置 | 0.5d
  T: 轮次 | 层级 | 文件数 | 评分 | P0 | P1 | P2