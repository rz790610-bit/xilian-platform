# ============================================================
# 西联智能平台 - 全栈一键部署 Docker Compose
# 包含: MySQL + Redis + ClickHouse + MinIO + Qdrant + Kafka + Neo4j
#        + Elasticsearch + Kafka Connect (bigdata profile)
#        + Prometheus + Grafana + Jaeger (monitoring)
# ============================================================
# 启动方式:
#   最小启动:      docker-compose up -d mysql redis
#   核心服务:      docker-compose up -d mysql redis clickhouse kafka qdrant neo4j
#   全部启动:      docker-compose up -d
#   大数据服务:    docker-compose --profile bigdata up -d
#   全部+大数据:   docker-compose --profile full up -d
#   停止全部:      docker-compose down
#   查看日志:      docker-compose logs -f <service>
#   查看状态:      docker-compose ps
# ============================================================

services:
  # ==================== Application ====================
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: xilian-nexus
    restart: unless-stopped
    ports:
      - "${PORT:-3000}:3000"
    environment:
      - NODE_ENV=production
      - PORT=3000
      - SKIP_AUTH=${SKIP_AUTH:-true}
      - DATABASE_URL=mysql://portai:portai123@mysql:3306/portai_nexus
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - KAFKA_BROKERS=kafka:29092
      - KAFKA_CLIENT_ID=xilian-platform
      - OLLAMA_HOST=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.1:70b}
      - QDRANT_HOST=http://qdrant:6333
      - CLICKHOUSE_HOST=clickhouse
      - CLICKHOUSE_PORT=8123
      - CLICKHOUSE_DATABASE=portai_timeseries
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_PORT=9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-portai}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-portai123456}
      - NEO4J_HOST=neo4j
      - NEO4J_HTTP_PORT=7474
      - NEO4J_BOLT_PORT=7687
      - NEO4J_USER=${NEO4J_USER:-neo4j}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-portai123}
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - KAFKA_CONNECT_HOST=kafka-connect
      - KAFKA_CONNECT_PORT=8083
      - FEATURE_ELASTICSEARCH_ENABLED=${FEATURE_ELASTICSEARCH_ENABLED:-false}
      - FEATURE_KAFKA_CONNECT_ENABLED=${FEATURE_KAFKA_CONNECT_ENABLED:-false}
      - FEATURE_AIRFLOW_ENABLED=${FEATURE_AIRFLOW_ENABLED:-false}
      - FEATURE_FLINK_ENABLED=${FEATURE_FLINK_ENABLED:-false}
      - VAULT_ADDR=http://vault:8200
      - VAULT_TOKEN=${VAULT_DEV_TOKEN:-xilian-dev-root-token}
      - LOG_LEVEL=${LOG_LEVEL:-info}
    depends_on:
      mysql:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/rest/_health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ==================== Core: MySQL 8.0 ====================
  # 关系型数据库 — 存储所有核心业务数据
  mysql:
    image: mysql:8.0
    container_name: xilian-mysql
    restart: unless-stopped
    ports:
      - "${MYSQL_PORT:-3306}:3306"
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-root123}
      MYSQL_DATABASE: portai_nexus
      MYSQL_USER: portai
      MYSQL_PASSWORD: portai123
      TZ: Asia/Shanghai
    command: >
      --default-authentication-plugin=mysql_native_password
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci
      --max-connections=500
      --innodb-buffer-pool-size=256M
      --slow-query-log=1
      --long-query-time=2
    volumes:
      - mysql_data:/var/lib/mysql
      - ./docker/mysql/init:/docker-entrypoint-initdb.d
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-proot123"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  # ==================== Core: Redis 7 ====================
  # 缓存层 — 设备状态缓存、会话管理、事件去重、分布式锁
  redis:
    image: redis:7-alpine
    container_name: xilian-redis
    restart: unless-stopped
    ports:
      - "${REDIS_PORT:-6379}:6379"
    command: >
      redis-server
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
    volumes:
      - redis_data:/data
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ==================== TSDB: ClickHouse ====================
  # 时序数据库 — 存储高频传感器数据和聚合指标
  clickhouse:
    image: clickhouse/clickhouse-server:24.3
    container_name: xilian-clickhouse
    restart: unless-stopped
    ports:
      - "${CLICKHOUSE_HTTP_PORT:-8123}:8123"
      - "${CLICKHOUSE_NATIVE_PORT:-9000}:9000"
    environment:
      CLICKHOUSE_DB: portai_timeseries
      CLICKHOUSE_USER: portai
      CLICKHOUSE_PASSWORD: portai123
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - clickhouse_logs:/var/log/clickhouse-server
      - ./docker/clickhouse/init:/docker-entrypoint-initdb.d
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  # ==================== Object Store: MinIO ====================
  # 对象存储 — 波形文件、频谱图、模型文件等大文件
  minio:
    image: minio/minio:latest
    container_name: xilian-minio
    restart: unless-stopped
    ports:
      - "${MINIO_API_PORT:-9010}:9000"
      - "${MINIO_CONSOLE_PORT:-9011}:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-portai}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-portai123456}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

  # MinIO 初始化：自动创建默认存储桶
  minio-init:
    image: minio/mc:latest
    container_name: xilian-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set portai http://minio:9000 portai portai123456;
      mc mb --ignore-existing portai/waveforms;
      mc mb --ignore-existing portai/spectrums;
      mc mb --ignore-existing portai/models;
      mc mb --ignore-existing portai/reports;
      mc mb --ignore-existing portai/backups;
      echo '✅ MinIO buckets created successfully';
      exit 0;
      "
    networks:
      - portai-network

  # ==================== Vector DB: Qdrant ====================
  # 向量数据库 — 相似故障检索和语义搜索 (RAG)
  qdrant:
    image: qdrant/qdrant:v1.9.0
    container_name: xilian-qdrant
    restart: unless-stopped
    ports:
      - "${QDRANT_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      QDRANT__SERVICE__GRPC_PORT: 6334
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:6333/readyz"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

  # ==================== Message Queue: Kafka (KRaft) ====================
  # 消息队列 — 事件总线、数据流处理（KRaft 模式，无需 Zookeeper）
  kafka:
    image: apache/kafka:3.7.0
    container_name: xilian-kafka
    restart: unless-stopped
    ports:
      - "${KAFKA_PORT:-9092}:9092"
    environment:
      # KRaft 模式（无需 Zookeeper）
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_LISTENERS: PLAINTEXT://:29092,CONTROLLER://:9093,EXTERNAL://:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_MESSAGE_MAX_BYTES: 10485760
      CLUSTER_ID: portai-kafka-cluster-001
    volumes:
      - kafka_data:/tmp/kafka-logs
    networks:
      - portai-network
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:29092 --list || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ==================== Search & Analytics: Elasticsearch ====================
  # 全文搜索 + 日志分析 — 告警事件检索、审计日志、诊断结果全文搜索
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.0
    container_name: xilian-elasticsearch
    restart: unless-stopped
    profiles: ["bigdata", "full"]
    ports:
      - "${ELASTICSEARCH_PORT:-9200}:9200"
      - "${ELASTICSEARCH_TRANSPORT_PORT:-9300}:9300"
    environment:
      - discovery.type=single-node
      - cluster.name=xilian-cluster
      - node.name=xilian-es-01
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
      - xpack.ilm.enabled=true
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - bootstrap.memory_lock=true
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - portai-network
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9200/_cluster/health || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s

  # Elasticsearch 初始化：索引模板 + ILM 策略
  elasticsearch-init:
    image: curlimages/curl:8.5.0
    container_name: xilian-elasticsearch-init
    profiles: ["bigdata", "full"]
    depends_on:
      elasticsearch:
        condition: service_healthy
    volumes:
      - ./docker/elasticsearch/init-es.sh:/init-es.sh:ro
    entrypoint: ["/bin/sh", "/init-es.sh"]
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
    networks:
      - portai-network

  # ==================== Data Integration: Kafka Connect ====================
  # 数据集成 — CDC (Debezium) + ES Sink + JDBC Sink
  kafka-connect:
    build:
      context: ./docker/kafka-connect
      dockerfile: Dockerfile
    container_name: xilian-kafka-connect
    restart: unless-stopped
    profiles: ["bigdata", "full"]
    ports:
      - "${KAFKA_CONNECT_PORT:-8083}:8083"
    environment:
      KAFKA_CONNECT_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_CONNECT_GROUP_ID: xilian-connect-cluster
      KAFKA_CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      KAFKA_CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      KAFKA_CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      KAFKA_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      KAFKA_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      KAFKA_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      KAFKA_CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KAFKA_CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KAFKA_CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      KAFKA_CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      KAFKA_CONNECT_REST_PORT: 8083
      KAFKA_CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      KAFKA_CONNECT_PLUGIN_PATH: /opt/kafka/connect-plugins
    command: >
      /opt/kafka/bin/connect-distributed.sh
      /opt/kafka/config/connect-distributed.properties
    volumes:
      - ./docker/kafka-connect/connect-distributed.properties:/opt/kafka/config/connect-distributed.properties:ro
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - portai-network
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8083/ || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s

  # Kafka Connect 初始化：自动注册连接器
  kafka-connect-init:
    image: curlimages/curl:8.5.0
    container_name: xilian-kafka-connect-init
    profiles: ["bigdata", "full"]
    depends_on:
      kafka-connect:
        condition: service_healthy
      mysql:
        condition: service_healthy
    volumes:
      - ./docker/kafka-connect/init-connectors.sh:/init-connectors.sh:ro
    entrypoint: ["/bin/sh", "/init-connectors.sh"]
    environment:
      - KAFKA_CONNECT_URL=http://kafka-connect:8083
    networks:
      - portai-network

  # ==================== Graph DB: Neo4j ====================
  # 图数据库 — 知识图谱和设备关系拓扑（替代 NebulaGraph，原生支持 ARM64）
  neo4j:
    image: neo4j:5.18-community
    container_name: xilian-neo4j
    restart: unless-stopped
    ports:
      - "${NEO4J_HTTP_PORT:-7474}:7474"
      - "${NEO4J_BOLT_PORT:-7687}:7687"
    environment:
      NEO4J_AUTH: ${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:-portai123}
      NEO4J_PLUGINS: '["apoc"]'
      NEO4J_server_memory_heap_initial__size: 256m
      NEO4J_server_memory_heap_max__size: 512m
      NEO4J_server_memory_pagecache_size: 256m
      TZ: Asia/Shanghai
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    networks:
      - portai-network
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:7474 || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 30s

  # ==================== LLM Engine: Ollama ====================
  # 大模型推理引擎
  ollama:
    image: ollama/ollama:latest
    container_name: xilian-ollama
    profiles: ["llm", "full"]
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - portai-network
    # Uncomment for NVIDIA GPU support:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # ==================== Monitoring: Prometheus ====================
  prometheus:
    image: prom/prometheus:v2.51.0
    container_name: xilian-prometheus
    restart: unless-stopped
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus/rules:/etc/prometheus/rules
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:9090/-/healthy"]
      interval: 15s
      timeout: 5s
      retries: 3

  # ==================== Monitoring: Alertmanager ====================
  alertmanager:
    image: prom/alertmanager:v0.27.0
    container_name: xilian-alertmanager
    restart: unless-stopped
    ports:
      - "${ALERTMANAGER_PORT:-9093}:9093"
    volumes:
      - ./docker/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    networks:
      - portai-network

  # ==================== Monitoring: Grafana ====================
  grafana:
    image: grafana/grafana:10.4.0
    container_name: xilian-grafana
    restart: unless-stopped
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clickhouse-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./docker/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - prometheus
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:3000/api/health"]
      interval: 15s
      timeout: 5s
      retries: 3

  # ==================== Monitoring: Node Exporter ====================
  node-exporter:
    image: prom/node-exporter:v1.7.0
    container_name: xilian-node-exporter
    restart: unless-stopped
    ports:
      - "${NODE_EXPORTER_PORT:-9100}:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - portai-network

  # ==================== Monitoring: cAdvisor ====================
  # 安全说明: cAdvisor 需要 privileged 模式访问宿主机容器指标
  # 仅限开发/测试环境使用；生产 K8s 环境应使用 DaemonSet + PodSecurityStandard 豁免
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    container_name: xilian-cadvisor
    restart: unless-stopped
    ports:
      - "${CADVISOR_PORT:-8080}:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    devices:
      - /dev/kmsg
    networks:
      - portai-network

  # ==================== Tracing: Jaeger ====================
  jaeger:
    image: jaegertracing/all-in-one:1.53
    container_name: xilian-jaeger
    restart: unless-stopped
    ports:
      - "${JAEGER_UI_PORT:-16686}:16686"     # Jaeger UI
      - "${JAEGER_OTLP_PORT:-4318}:4318"     # OTLP HTTP
      - "${JAEGER_OTLP_GRPC_PORT:-4317}:4317" # OTLP gRPC
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=false
      - BADGER_DIRECTORY_VALUE=/badger/data
      - BADGER_DIRECTORY_KEY=/badger/key
    volumes:
      - jaeger_data:/badger
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:14269/"]
      interval: 15s
      timeout: 5s
      retries: 3

  # ==================== Exporter: MySQL ====================
  mysql-exporter:
    image: prom/mysqld-exporter:v0.15.1
    container_name: xilian-mysql-exporter
    restart: unless-stopped
    ports:
      - "${MYSQL_EXPORTER_PORT:-9104}:9104"
    environment:
      - DATA_SOURCE_NAME=${MYSQL_EXPORTER_DSN:-root:xilian2024@(mysql:3306)/}
    depends_on:
      - mysql
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:9104/metrics"]
      interval: 30s
      timeout: 5s
      retries: 3

  # ==================== Exporter: Redis ====================
  redis-exporter:
    image: oliver006/redis_exporter:v1.58.0
    container_name: xilian-redis-exporter
    restart: unless-stopped
    ports:
      - "${REDIS_EXPORTER_PORT:-9121}:9121"
    environment:
      - REDIS_ADDR=redis://redis:6379
    depends_on:
      - redis
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:9121/metrics"]
      interval: 30s
      timeout: 5s
      retries: 3

  # ==================== Exporter: Kafka ====================
  kafka-exporter:
    image: danielqsj/kafka-exporter:v1.7.0
    container_name: xilian-kafka-exporter
    restart: unless-stopped
    ports:
      - "${KAFKA_EXPORTER_PORT:-9308}:9308"
    command:
      - '--kafka.server=kafka:9092'
      - '--topic.filter=.*'
      - '--group.filter=.*'
    depends_on:
      - kafka
    networks:
      - portai-network

  # ==================== HashiCorp Vault (Secrets Management) ====================
  vault:
    image: hashicorp/vault:1.15
    container_name: xilian-vault
    restart: unless-stopped
    cap_add:
      - IPC_LOCK
    ports:
      - "${VAULT_PORT:-8200}:8200"
    environment:
      VAULT_DEV_ROOT_TOKEN_ID: ${VAULT_DEV_TOKEN:-xilian-dev-root-token}
      VAULT_DEV_LISTEN_ADDRESS: 0.0.0.0:8200
      VAULT_ADDR: http://127.0.0.1:8200
      VAULT_TOKEN: ${VAULT_DEV_TOKEN:-xilian-dev-root-token}
    volumes:
      - vault_data:/vault/data
      - vault_logs:/vault/logs
      - ./docker/vault/vault-config.hcl:/vault/config/vault-config.hcl:ro
      - ./docker/vault/init-vault.sh:/vault/config/init-vault.sh:ro
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "vault", "status"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 10s

  vault-init:
    image: hashicorp/vault:1.15
    container_name: xilian-vault-init
    entrypoint: /bin/sh
    command: ["-c", "sleep 5 && /vault/config/init-vault.sh"]
    environment:
      VAULT_ADDR: http://vault:8200
      VAULT_TOKEN: ${VAULT_DEV_TOKEN:-xilian-dev-root-token}
      DB_PASSWORD: ${DB_PASSWORD:-xilian_secret}
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-root_password}
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      ES_PASSWORD: ${ES_PASSWORD:-changeme}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minioadmin}
      JWT_SECRET: ${JWT_SECRET:-}
    volumes:
      - ./docker/vault/init-vault.sh:/vault/config/init-vault.sh:ro
    depends_on:
      vault:
        condition: service_healthy
    networks:
      - portai-network

  # ==================== Apache Flink (Streaming) ====================
  # Profile: bigdata — 仅在 docker compose --profile bigdata up 时启动
  flink-jobmanager:
    image: flink:1.18.1-scala_2.12-java11
    container_name: xilian-flink-jobmanager
    restart: unless-stopped
    profiles: ["bigdata"]
    ports:
      - "${FLINK_JOBMANAGER_PORT:-8081}:8081"   # Web UI
      - "${FLINK_RPC_PORT:-6123}:6123"           # RPC
    environment:
      - FLINK_PROPERTIES=
          jobmanager.rpc.address: flink-jobmanager
          jobmanager.rpc.port: 6123
          jobmanager.memory.process.size: 1600m
          state.backend: rocksdb
          state.checkpoints.dir: file:///opt/flink/checkpoints
          state.savepoints.dir: file:///opt/flink/savepoints
          execution.checkpointing.interval: 60000
          execution.checkpointing.mode: EXACTLY_ONCE
          rest.flamegraph.enabled: true
          metrics.reporters: prom
          metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
          metrics.reporter.prom.port: 9249
    volumes:
      - flink_checkpoints:/opt/flink/checkpoints
      - flink_savepoints:/opt/flink/savepoints
      - ./docker/flink/jobs:/opt/flink/usrlib
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:8081/overview"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  flink-taskmanager:
    image: flink:1.18.1-scala_2.12-java11
    container_name: xilian-flink-taskmanager
    restart: unless-stopped
    profiles: ["bigdata"]
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    environment:
      - FLINK_PROPERTIES=
          jobmanager.rpc.address: flink-jobmanager
          taskmanager.numberOfTaskSlots: 4
          taskmanager.memory.process.size: 2048m
          taskmanager.memory.managed.fraction: 0.4
          metrics.reporters: prom
          metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
          metrics.reporter.prom.port: 9249
    volumes:
      - flink_checkpoints:/opt/flink/checkpoints
      - ./docker/flink/jobs:/opt/flink/usrlib
    networks:
      - portai-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2560M
        reservations:
          cpus: '0.5'
          memory: 1024M

  # ==================== Apache Airflow (Orchestration) ====================
  # Profile: bigdata — 仅在 docker compose --profile bigdata up 时启动
  airflow-postgres:
    image: postgres:16-alpine
    container_name: xilian-airflow-postgres
    restart: unless-stopped
    profiles: ["bigdata"]
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD:-airflow_secret}
      POSTGRES_DB: airflow
    volumes:
      - airflow_postgres_data:/var/lib/postgresql/data
    networks:
      - portai-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-init:
    image: apache/airflow:2.8.3-python3.11
    container_name: xilian-airflow-init
    profiles: ["bigdata"]
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create \
          --username admin \
          --password $${AIRFLOW_ADMIN_PASSWORD:-admin} \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@xilian.local
        echo "Airflow initialized successfully"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${AIRFLOW_DB_PASSWORD:-airflow_secret}@airflow-postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=}
      AIRFLOW_ADMIN_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD:-admin}
    depends_on:
      airflow-postgres:
        condition: service_healthy
    networks:
      - portai-network

  airflow-webserver:
    image: apache/airflow:2.8.3-python3.11
    container_name: xilian-airflow-webserver
    restart: unless-stopped
    profiles: ["bigdata"]
    command: webserver
    ports:
      - "${AIRFLOW_WEBSERVER_PORT:-8080}:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${AIRFLOW_DB_PASSWORD:-airflow_secret}@airflow-postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=}
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__WEBSERVER__RBAC: "true"
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
      AIRFLOW__METRICS__STATSD_ON: "true"
      AIRFLOW__METRICS__STATSD_HOST: statsd-exporter
      AIRFLOW__METRICS__STATSD_PORT: "9125"
      AIRFLOW__METRICS__STATSD_PREFIX: airflow
    volumes:
      - ./docker/airflow/dags:/opt/airflow/dags
      - ./docker/airflow/plugins:/opt/airflow/plugins
      - airflow_logs:/opt/airflow/logs
    depends_on:
      airflow-postgres:
        condition: service_healthy
    networks:
      - portai-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  airflow-scheduler:
    image: apache/airflow:2.8.3-python3.11
    container_name: xilian-airflow-scheduler
    restart: unless-stopped
    profiles: ["bigdata"]
    command: scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:${AIRFLOW_DB_PASSWORD:-airflow_secret}@airflow-postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=}
    volumes:
      - ./docker/airflow/dags:/opt/airflow/dags
      - ./docker/airflow/plugins:/opt/airflow/plugins
      - airflow_logs:/opt/airflow/logs
    depends_on:
      airflow-postgres:
        condition: service_healthy
    networks:
      - portai-network
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

# ==================== Networks ====================
networks:
  portai-network:
    driver: bridge
    name: portai-network

# ==================== Volumes ====================
volumes:
  mysql_data:
    name: portai-mysql-data
  redis_data:
    name: portai-redis-data
  clickhouse_data:
    name: portai-clickhouse-data
  clickhouse_logs:
    name: portai-clickhouse-logs
  minio_data:
    name: portai-minio-data
  qdrant_data:
    name: portai-qdrant-data
  kafka_data:
    name: portai-kafka-data
  neo4j_data:
    name: portai-neo4j-data
  neo4j_logs:
    name: portai-neo4j-logs
  ollama_data:
    name: portai-ollama-data
  prometheus_data:
    name: portai-prometheus-data
  grafana_data:
    name: portai-grafana-data
  jaeger_data:
    name: xilian-jaeger-data
  elasticsearch_data:
    name: xilian-elasticsearch-data
  flink_checkpoints:
    name: xilian-flink-checkpoints
  flink_savepoints:
    name: xilian-flink-savepoints
  airflow_postgres_data:
    name: xilian-airflow-postgres-data
  airflow_logs:
    name: xilian-airflow-logs
  vault_data:
    name: xilian-vault-data
  vault_logs:
    name: xilian-vault-logs
