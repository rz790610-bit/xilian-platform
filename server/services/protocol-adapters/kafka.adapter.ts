/**
 * Apache Kafka æ¶ˆæ¯é˜Ÿåˆ—åè®®é€‚é…å™¨ - ç”Ÿäº§çº§å®ç°
 * 
 * åŸºäº kafkajs åº“
 * æ”¯æŒ SASL (PLAIN/SCRAM/GSSAPI/OAUTHBEARER)ã€SSL/TLSã€æ¶ˆè´¹è€…ç»„ç®¡ç†
 * é«˜çº§ç‰¹æ€§ï¼šåˆ†åŒºç­–ç•¥ã€å‹ç¼©ã€å¹‚ç­‰ç”Ÿäº§è€…ã€äº‹åŠ¡ã€Schema Registry
 * èµ„æºå‘ç°ï¼šåˆ—å‡º Topicã€åˆ†åŒºã€æ¶ˆè´¹è€…ç»„ã€åç§»é‡ä¿¡æ¯
 */

import { Kafka, logLevel, SASLOptions, CompressionTypes } from 'kafkajs';
import * as net from 'net';
import { BaseAdapter, normalizeError, AdapterError, AdapterErrorCode } from './base';
import type { ConnectionTestResult, DiscoveredEndpoint, ProtocolConfigSchema, HealthCheckResult } from '../../../shared/accessLayerTypes';

export class KafkaAdapter extends BaseAdapter {
  readonly protocolType = 'kafka' as const;
  protected defaultTimeoutMs = 20000;

  readonly configSchema: ProtocolConfigSchema = {
    protocolType: 'kafka',
    label: 'Apache Kafka',
    icon: 'ğŸ“¨',
    description: 'äº‹ä»¶æµ/æ—¥å¿—èšåˆ',
    category: 'messaging',
    connectionFields: [
      { key: 'brokers', label: 'Broker åˆ—è¡¨', type: 'string', required: true, placeholder: 'broker1:9092,broker2:9092,broker3:9092', description: 'é€—å·åˆ†éš”çš„ Kafka Broker åœ°å€åˆ—è¡¨' },
      { key: 'clientId', label: 'å®¢æˆ·ç«¯ ID', type: 'string', required: false, defaultValue: 'xilian-platform', description: 'åœ¨ Broker æ—¥å¿—ä¸­æ ‡è¯†æ­¤å®¢æˆ·ç«¯' },
      { key: 'connectionTimeout', label: 'è¿æ¥è¶…æ—¶(ms)', type: 'number', required: false, defaultValue: 10000 },
      { key: 'requestTimeout', label: 'è¯·æ±‚è¶…æ—¶(ms)', type: 'number', required: false, defaultValue: 30000 },
    ],
    authFields: [
      { key: 'saslMechanism', label: 'SASL è®¤è¯æœºåˆ¶', type: 'select', required: false, defaultValue: 'none', options: [
        { label: 'æ— è®¤è¯', value: 'none' },
        { label: 'PLAIN', value: 'plain' },
        { label: 'SCRAM-SHA-256', value: 'scram-sha-256' },
        { label: 'SCRAM-SHA-512', value: 'scram-sha-512' },
        { label: 'AWS IAM', value: 'aws' },
        { label: 'OAUTHBEARER', value: 'oauthbearer' },
      ]},
      { key: 'username', label: 'ç”¨æˆ·å', type: 'string', required: false, description: 'SASL PLAIN/SCRAM ç”¨æˆ·å' },
      { key: 'password', label: 'å¯†ç ', type: 'password', required: false, description: 'SASL PLAIN/SCRAM å¯†ç ' },
      { key: 'awsAccessKeyId', label: 'AWS Access Key ID', type: 'string', required: false, group: 'AWS IAM' },
      { key: 'awsSecretAccessKey', label: 'AWS Secret Access Key', type: 'password', required: false, group: 'AWS IAM' },
      { key: 'awsSessionToken', label: 'AWS Session Token', type: 'password', required: false, group: 'AWS IAM' },
      { key: 'awsRegion', label: 'AWS Region', type: 'string', required: false, group: 'AWS IAM' },
      { key: 'ssl', label: 'å¯ç”¨ SSL/TLS', type: 'boolean', required: false, defaultValue: false },
      { key: 'sslCa', label: 'CA è¯ä¹¦ (PEM)', type: 'textarea', required: false, group: 'SSL' },
      { key: 'sslCert', label: 'å®¢æˆ·ç«¯è¯ä¹¦ (PEM)', type: 'textarea', required: false, group: 'SSL' },
      { key: 'sslKey', label: 'å®¢æˆ·ç«¯ç§é’¥ (PEM)', type: 'textarea', required: false, group: 'SSL' },
      { key: 'sslRejectUnauthorized', label: 'éªŒè¯æœåŠ¡å™¨è¯ä¹¦', type: 'boolean', required: false, defaultValue: true, group: 'SSL' },
    ],
    advancedFields: [
      // ç”Ÿäº§è€…é…ç½®
      { key: 'producerAcks', label: 'ç”Ÿäº§è€…ç¡®è®¤', type: 'select', required: false, defaultValue: '-1', options: [
        { label: '0 (ä¸ç­‰å¾…ç¡®è®¤)', value: '0' },
        { label: '1 (Leader ç¡®è®¤)', value: '1' },
        { label: '-1 / all (æ‰€æœ‰ ISR ç¡®è®¤)', value: '-1' },
      ], description: 'æ¶ˆæ¯å†™å…¥ç¡®è®¤çº§åˆ«' },
      { key: 'producerIdempotent', label: 'å¹‚ç­‰ç”Ÿäº§è€…', type: 'boolean', required: false, defaultValue: false, description: 'å¯ç”¨ Exactly-Once è¯­ä¹‰ï¼ˆéœ€è¦ acks=-1ï¼‰' },
      { key: 'producerCompression', label: 'å‹ç¼©ç®—æ³•', type: 'select', required: false, defaultValue: 'none', options: [
        { label: 'æ— å‹ç¼©', value: 'none' },
        { label: 'GZIP', value: 'gzip' },
        { label: 'Snappy', value: 'snappy' },
        { label: 'LZ4', value: 'lz4' },
        { label: 'ZSTD', value: 'zstd' },
      ]},
      { key: 'producerBatchSize', label: 'æ‰¹é‡å¤§å°(å­—èŠ‚)', type: 'number', required: false, defaultValue: 16384, description: 'ç”Ÿäº§è€…æ‰¹é‡å‘é€çš„å­—èŠ‚é˜ˆå€¼' },
      { key: 'producerLingerMs', label: 'å‘é€å»¶è¿Ÿ(ms)', type: 'number', required: false, defaultValue: 0, description: 'ç­‰å¾…æ›´å¤šæ¶ˆæ¯ä»¥æ‰¹é‡å‘é€çš„å»¶è¿Ÿ' },
      { key: 'transactionalId', label: 'äº‹åŠ¡ ID', type: 'string', required: false, description: 'å¯ç”¨äº‹åŠ¡æ€§ç”Ÿäº§è€…çš„å”¯ä¸€æ ‡è¯†' },
      // æ¶ˆè´¹è€…é…ç½®
      { key: 'consumerGroupId', label: 'æ¶ˆè´¹è€…ç»„ ID', type: 'string', required: false, placeholder: 'xilian-consumer-group', description: 'æ¶ˆè´¹è€…ç»„æ ‡è¯†' },
      { key: 'consumerAutoOffsetReset', label: 'åç§»é‡é‡ç½®ç­–ç•¥', type: 'select', required: false, defaultValue: 'latest', options: [
        { label: 'æœ€æ–° (latest)', value: 'latest' },
        { label: 'æœ€æ—© (earliest)', value: 'earliest' },
      ]},
      { key: 'consumerMaxBytes', label: 'æœ€å¤§æ‹‰å–å­—èŠ‚', type: 'number', required: false, defaultValue: 1048576, description: 'å•æ¬¡ fetch çš„æœ€å¤§å­—èŠ‚æ•°ï¼ˆé»˜è®¤ 1MBï¼‰' },
      { key: 'consumerMaxWaitTimeMs', label: 'æœ€å¤§ç­‰å¾…æ—¶é—´(ms)', type: 'number', required: false, defaultValue: 5000 },
      { key: 'consumerSessionTimeout', label: 'ä¼šè¯è¶…æ—¶(ms)', type: 'number', required: false, defaultValue: 30000 },
      { key: 'consumerHeartbeatInterval', label: 'å¿ƒè·³é—´éš”(ms)', type: 'number', required: false, defaultValue: 3000 },
      { key: 'consumerAutoCommit', label: 'è‡ªåŠ¨æäº¤åç§»é‡', type: 'boolean', required: false, defaultValue: true },
      { key: 'consumerAutoCommitInterval', label: 'è‡ªåŠ¨æäº¤é—´éš”(ms)', type: 'number', required: false, defaultValue: 5000 },
      // é‡è¯•ä¸è¿æ¥
      { key: 'retries', label: 'é‡è¯•æ¬¡æ•°', type: 'number', required: false, defaultValue: 5 },
      { key: 'initialRetryTime', label: 'åˆå§‹é‡è¯•å»¶è¿Ÿ(ms)', type: 'number', required: false, defaultValue: 300 },
      { key: 'maxRetryTime', label: 'æœ€å¤§é‡è¯•å»¶è¿Ÿ(ms)', type: 'number', required: false, defaultValue: 30000 },
      { key: 'logLevel', label: 'æ—¥å¿—çº§åˆ«', type: 'select', required: false, defaultValue: 'WARN', options: [
        { label: 'NOTHING', value: 'NOTHING' },
        { label: 'ERROR', value: 'ERROR' },
        { label: 'WARN', value: 'WARN' },
        { label: 'INFO', value: 'INFO' },
        { label: 'DEBUG', value: 'DEBUG' },
      ]},
      // Schema Registry
      { key: 'schemaRegistryUrl', label: 'Schema Registry URL', type: 'string', required: false, placeholder: 'http://schema-registry:8081', description: 'Confluent Schema Registry åœ°å€' },
      { key: 'schemaRegistryAuth', label: 'Schema Registry è®¤è¯', type: 'string', required: false, description: 'Basic Auth (user:password)' },
    ],
  };

  private createKafka(params: Record<string, unknown>, auth?: Record<string, unknown>): Kafka {
    const brokersStr = (params.brokers as string) || 'localhost:9092';
    const brokers = brokersStr.split(',').map(b => b.trim()).filter(Boolean);

    const kafkaConfig: any = {
      clientId: (params.clientId as string) || 'xilian-platform',
      brokers,
      connectionTimeout: (params.connectionTimeout as number) || 10000,
      requestTimeout: (params.requestTimeout as number) || 30000,
      retry: {
        retries: (params.retries as number) ?? 5,
        initialRetryTime: (params.initialRetryTime as number) || 300,
        maxRetryTime: (params.maxRetryTime as number) || 30000,
      },
      logLevel: this.getLogLevel((params.logLevel as string) || 'WARN'),
    };

    // SASL è®¤è¯
    const saslMechanism = (auth?.saslMechanism as string) || 'none';
    if (saslMechanism !== 'none') {
      if (saslMechanism === 'plain' || saslMechanism === 'scram-sha-256' || saslMechanism === 'scram-sha-512') {
        kafkaConfig.sasl = {
          mechanism: saslMechanism,
          username: (auth?.username as string) || '',
          password: (auth?.password as string) || '',
        };
      } else if (saslMechanism === 'aws') {
        kafkaConfig.sasl = {
          mechanism: 'aws',
          authorizationIdentity: (auth?.awsAccessKeyId as string) || '',
          accessKeyId: (auth?.awsAccessKeyId as string) || '',
          secretAccessKey: (auth?.awsSecretAccessKey as string) || '',
          sessionToken: (auth?.awsSessionToken as string) || undefined,
        };
      }
    }

    // SSL
    if (auth?.ssl) {
      kafkaConfig.ssl = {
        rejectUnauthorized: auth.sslRejectUnauthorized !== false,
      };
      if (auth.sslCa) kafkaConfig.ssl.ca = [auth.sslCa as string];
      if (auth.sslCert) kafkaConfig.ssl.cert = auth.sslCert as string;
      if (auth.sslKey) kafkaConfig.ssl.key = auth.sslKey as string;
    }

    return new Kafka(kafkaConfig);
  }

  private getLogLevel(level: string): logLevel {
    const map: Record<string, logLevel> = {
      'NOTHING': logLevel.NOTHING,
      'ERROR': logLevel.ERROR,
      'WARN': logLevel.WARN,
      'INFO': logLevel.INFO,
      'DEBUG': logLevel.DEBUG,
    };
    return map[level] || logLevel.WARN;
  }

  /**
   * TCP é¢„æ£€ï¼šåœ¨è°ƒç”¨ KafkaJS ä¹‹å‰å…ˆç”¨åŸç”Ÿ TCP æ£€æŸ¥ Broker æ˜¯å¦å¯è¾¾
   * è¿™æ ·å¯ä»¥å½»åº•é¿å… KafkaJS å†…éƒ¨é‡è¯•äº§ç”Ÿçš„ ERROR æ—¥å¿—
   */
  private tcpProbe(host: string, port: number, timeoutMs: number = 5000): Promise<boolean> {
    return new Promise((resolve) => {
      const socket = new net.Socket();
      const timer = setTimeout(() => {
        socket.destroy();
        resolve(false);
      }, timeoutMs);
      socket.connect(port, host, () => {
        clearTimeout(timer);
        socket.destroy();
        resolve(true);
      });
      socket.on('error', () => {
        clearTimeout(timer);
        socket.destroy();
        resolve(false);
      });
    });
  }

  protected async doTestConnection(
    params: Record<string, unknown>,
    auth?: Record<string, unknown>
  ): Promise<ConnectionTestResult> {
    const brokersStr = (params.brokers as string) || '';
    if (!brokersStr) {
      return { success: false, latencyMs: 0, message: 'Broker åˆ—è¡¨ä¸èƒ½ä¸ºç©º' };
    }

    const brokers = brokersStr.split(',').map(b => b.trim()).filter(Boolean);

    // ç¬¬ 1 æ­¥ï¼šTCP é¢„æ£€ â€” å¿«é€Ÿåˆ¤æ–­ Broker æ˜¯å¦å¯è¾¾ï¼ˆä¸è§¦å‘ KafkaJSï¼‰
    const probeTimeout = Math.min((params.connectionTimeout as number) || 8000, 5000);
    const probeResults = await Promise.all(
      brokers.map(async (broker) => {
        const [host, portStr] = broker.split(':');
        const port = parseInt(portStr) || 9092;
        const reachable = await this.tcpProbe(host, port, probeTimeout);
        return { broker, host, port, reachable };
      })
    );

    const reachableBrokers = probeResults.filter(r => r.reachable);
    if (reachableBrokers.length === 0) {
      const unreachable = probeResults.map(r => r.broker).join(', ');
      return {
        success: false,
        latencyMs: 0,
        message: `Kafka Broker ä¸å¯è¾¾: ${unreachable} (ç½‘ç»œä¸é€šæˆ–æœåŠ¡æœªå¯åŠ¨)`,
        details: { brokers: brokersStr, probeResults },
      };
    }

    // ç¬¬ 2 æ­¥ï¼šTCP å¯è¾¾åæ‰ç”¨ KafkaJS è·å–é›†ç¾¤è¯¦æƒ…
    const testParams = {
      ...params,
      brokers: reachableBrokers.map(r => r.broker).join(','),
      connectionTimeout: Math.min((params.connectionTimeout as number) || 10000, 8000),
      retries: 0,              // TCP å·²é¢„æ£€ï¼Œä¸éœ€è¦é‡è¯•
      initialRetryTime: 100,
      maxRetryTime: 1000,
      logLevel: 'NOTHING',     // æŠ‘åˆ¶ KafkaJS å†…éƒ¨æ—¥å¿—
    };
    const kafka = this.createKafka(testParams, auth);
    const admin = kafka.admin();

    try {
      await admin.connect();

      // è·å–é›†ç¾¤ä¿¡æ¯
      const cluster = await admin.describeCluster();
      const topics = await admin.listTopics();
      const groups = await admin.listGroups();

      const details: Record<string, unknown> = {
        clusterId: cluster.clusterId,
        controller: cluster.controller,
        brokers: cluster.brokers.map(b => ({ nodeId: b.nodeId, host: b.host, port: b.port })),
        brokerCount: cluster.brokers.length,
        topicCount: topics.length,
        topics: topics.slice(0, 50),
        consumerGroupCount: groups.groups.length,
        consumerGroups: groups.groups.slice(0, 20).map(g => ({ groupId: g.groupId, protocolType: g.protocolType })),
        tcpProbeResults: probeResults,
      };

      return {
        success: true,
        latencyMs: 0,
        message: `Kafka é›†ç¾¤è¿æ¥æˆåŠŸ (${cluster.brokers.length} ä¸ª Broker, ${topics.length} ä¸ª Topic)`,
        serverVersion: `Kafka Cluster ${cluster.clusterId}`,
        details,
      };
    } catch (err) {
      return {
        success: false,
        latencyMs: 0,
        message: `Kafka è¿æ¥å¤±è´¥: ${(err as Error).message}`,
        details: { brokers: brokersStr, error: (err as Error).message, tcpProbeResults: probeResults },
      };
    } finally {
      try { await admin.disconnect(); } catch { /* ignore */ }
    }
  }

  protected async doDiscoverResources(
    params: Record<string, unknown>,
    auth?: Record<string, unknown>
  ): Promise<DiscoveredEndpoint[]> {
    const kafka = this.createKafka(params, auth);
    const admin = kafka.admin();
    const endpoints: DiscoveredEndpoint[] = [];

    try {
      await admin.connect();

      // è·å–æ‰€æœ‰ Topic çš„å…ƒæ•°æ®
      const topics = await admin.listTopics();
      const metadata = await admin.fetchTopicMetadata({ topics });

      for (const topicMeta of metadata.topics) {
        // è·å– Topic åç§»é‡
        let offsets: any[] = [];
        try {
          offsets = await admin.fetchTopicOffsets(topicMeta.name);
        } catch { /* ignore */ }

        const totalMessages = offsets.reduce((sum, p) => {
          return sum + (parseInt(p.high) - parseInt(p.low));
        }, 0);

        endpoints.push({
          resourcePath: topicMeta.name,
          resourceType: 'topic',
          name: `Topic: ${topicMeta.name}`,
          dataFormat: 'json',
          schemaInfo: {
            partitions: topicMeta.partitions.length,
            replicationFactor: topicMeta.partitions[0]?.replicas?.length || 0,
          },
          metadata: {
            partitionDetails: topicMeta.partitions.map(p => ({
              partitionId: p.partitionId,
              leader: p.leader,
              replicas: p.replicas,
              isr: p.isr,
            })),
            offsets,
            estimatedMessages: totalMessages,
            isInternal: topicMeta.name.startsWith('__'),
          },
        });
      }

      // è·å–æ¶ˆè´¹è€…ç»„ä¿¡æ¯
      const groups = await admin.listGroups();
      for (const group of groups.groups.slice(0, 50)) {
        try {
          const groupDesc = await admin.describeGroups([group.groupId]);
          const desc = groupDesc.groups[0];
          endpoints.push({
            resourcePath: `group:${group.groupId}`,
            resourceType: 'collection',
            name: `æ¶ˆè´¹è€…ç»„: ${group.groupId}`,
            dataFormat: 'json',
            metadata: {
              entityType: 'consumerGroup',
              state: desc?.state,
              protocol: desc?.protocol,
              protocolType: desc?.protocolType,
              members: desc?.members?.length || 0,
            },
          });
        } catch { /* ignore */ }
      }

      return endpoints;
    } finally {
      try { await admin.disconnect(); } catch { /* ignore */ }
    }
  }

  protected async doHealthCheck(
    params: Record<string, unknown>,
    auth?: Record<string, unknown>
  ): Promise<Omit<HealthCheckResult, 'latencyMs' | 'checkedAt'>> {
    const brokersStr = (params.brokers as string) || '';
    if (!brokersStr) {
      return { status: 'unhealthy', message: 'Broker åˆ—è¡¨æœªé…ç½®' };
    }

    const brokers = brokersStr.split(',').map(b => b.trim()).filter(Boolean);

    // TCP é¢„æ£€ â€” å¿«é€Ÿåˆ¤æ–­ Broker æ˜¯å¦å¯è¾¾ï¼ˆä¸è§¦å‘ KafkaJSï¼‰
    const probeTimeout = Math.min((params.connectionTimeout as number) || 8000, 5000);
    const probeResults = await Promise.all(
      brokers.map(async (broker) => {
        const [host, portStr] = broker.split(':');
        const port = parseInt(portStr) || 9092;
        return { broker, reachable: await this.tcpProbe(host, port, probeTimeout) };
      })
    );

    const reachable = probeResults.filter(r => r.reachable);
    if (reachable.length === 0) {
      return {
        status: 'unhealthy',
        message: `Kafka Broker ä¸å¯è¾¾: ${brokers.join(', ')} (ç½‘ç»œä¸é€šæˆ–æœåŠ¡æœªå¯åŠ¨)`,
      };
    }

    // TCP å¯è¾¾åæ‰ç”¨ KafkaJS è·å–é›†ç¾¤çŠ¶æ€
    const healthParams = {
      ...params,
      brokers: reachable.map(r => r.broker).join(','),
      connectionTimeout: Math.min((params.connectionTimeout as number) || 10000, 8000),
      retries: 0,
      initialRetryTime: 100,
      maxRetryTime: 1000,
      logLevel: 'NOTHING',
    };
    const kafka = this.createKafka(healthParams, auth);
    const admin = kafka.admin();

    try {
      await admin.connect();
      const cluster = await admin.describeCluster();
      const topics = await admin.listTopics();

      return {
        status: 'healthy',
        message: `Kafka é›†ç¾¤æ­£å¸¸ - ${cluster.brokers.length} Broker, ${topics.length} Topic`,
        metrics: {
          clusterId: cluster.clusterId,
          brokerCount: cluster.brokers.length,
          topicCount: topics.length,
          controller: cluster.controller,
        },
      };
    } catch (err) {
      return {
        status: 'unhealthy',
        message: `Kafka å¥åº·æ£€æŸ¥å¤±è´¥: ${(err as Error).message}`,
      };
    } finally {
      try { await admin.disconnect(); } catch { /* ignore */ }
    }
  }
}
